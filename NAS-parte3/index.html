<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="icon" href="/assets/images/favicon.ico">

<title>Neural Architecture Search (Part 3) | GranaData</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Neural Architecture Search (Part 3) | GranaData</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Neural Architecture Search (Part 3)" />
<meta name="author" content="jose" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We have seen in Part 1 and Part 2 that neural architecture search can be used to find hyperparameters and to design recurrent networks. Is there any more use to it? The answer is obviously yes (otherwise there would be no point on this post). Let’s recap, NAS is just a method where you learn something that is non-learnable by backpropagation using trial and error, aka, reinforcement learning. The key for using it in more complex settings is to have a well defined space of parameters. In the first part that space was simply the real range for each hyperparameter. In the second part the search space included every recurrent network. Since that is a pretty broad space we narrowed it down by only working with some specific recurrent networks that could be generated by a tree-like structure. In this third part the search space is going to be the set of all convolutional neural networks. And to reduce that space to something manageable we are only going to consider networks generated by residual connections. Residual connections are typically just an addition operation like this" />
<meta property="og:description" content="We have seen in Part 1 and Part 2 that neural architecture search can be used to find hyperparameters and to design recurrent networks. Is there any more use to it? The answer is obviously yes (otherwise there would be no point on this post). Let’s recap, NAS is just a method where you learn something that is non-learnable by backpropagation using trial and error, aka, reinforcement learning. The key for using it in more complex settings is to have a well defined space of parameters. In the first part that space was simply the real range for each hyperparameter. In the second part the search space included every recurrent network. Since that is a pretty broad space we narrowed it down by only working with some specific recurrent networks that could be generated by a tree-like structure. In this third part the search space is going to be the set of all convolutional neural networks. And to reduce that space to something manageable we are only going to consider networks generated by residual connections. Residual connections are typically just an addition operation like this" />
<link rel="canonical" href="https://granadata.art/NAS-parte3/" />
<meta property="og:url" content="https://granadata.art/NAS-parte3/" />
<meta property="og:site_name" content="GranaData" />
<meta property="og:image" content="https://granadata.art/assets/images/NAS/nas3-image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-22T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://granadata.art/assets/images/NAS/nas3-image.png" />
<meta property="twitter:title" content="Neural Architecture Search (Part 3)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"jose"},"dateModified":"2022-08-22T00:00:00+00:00","datePublished":"2022-08-22T00:00:00+00:00","description":"We have seen in Part 1 and Part 2 that neural architecture search can be used to find hyperparameters and to design recurrent networks. Is there any more use to it? The answer is obviously yes (otherwise there would be no point on this post). Let’s recap, NAS is just a method where you learn something that is non-learnable by backpropagation using trial and error, aka, reinforcement learning. The key for using it in more complex settings is to have a well defined space of parameters. In the first part that space was simply the real range for each hyperparameter. In the second part the search space included every recurrent network. Since that is a pretty broad space we narrowed it down by only working with some specific recurrent networks that could be generated by a tree-like structure. In this third part the search space is going to be the set of all convolutional neural networks. And to reduce that space to something manageable we are only going to consider networks generated by residual connections. Residual connections are typically just an addition operation like this","headline":"Neural Architecture Search (Part 3)","image":"https://granadata.art/assets/images/NAS/nas3-image.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://granadata.art/NAS-parte3/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://granadata.art/assets/images/logo.png"},"name":"jose"},"url":"https://granadata.art/NAS-parte3/"}</script>
<!-- End Jekyll SEO tag -->



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(','\\)'] ],
      displayMath: [ ['$$', '$$'], ['\\[','\\]'] ],
      processEscapes: true,
      processEnvironments: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/h-hg/docsify-pseudocode/dist/docsify-pseudocode.min.js"></script>


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

<!-- Twitter cards -->
<meta name="twitter:site"    content="@GranaDataJose">
<meta name="twitter:creator" content="@GranaDataJose">
<meta name="twitter:title"   content="Neural Architecture Search (Part 3)">


<meta name="twitter:description" content="Data science from Granada to the world.">



<meta name="twitter:card"  content="summary_large_image">
<meta name="twitter:image" content="https://granadata.art/assets/images/NAS/nas3-image.png">

<!-- end of Twitter cards -->

</head>


<!-- change your GA id in _config.yml -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-46278016-1', 'auto');
ga('send', 'pageview');
</script>



<body class="layout-post">
	<!-- defer loading of font and font awesome -->
    <noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>

    
	


<!-- Begin Menu Navigation
================================================== -->

<nav class="navbar navbar-expand-sm navbar-dark bg-dark shadow fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="GranaData">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav mr-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Blog</a>
                </li>

                
                <li class="nav-item">
                
                <a class="nav-link" href="/about">About</a>
                </li>
                
            </ul>
            <ul class="navbar-nav ml-auto">

                <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">GranaData</h1>
    <p class="lead">
        Data science from Granada to the world.
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container inside-container">
    
    <div class="row">

        <!-- Post Share -->
        
        <!-- Post -->
        
        
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-right mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="/assets/images/JoseAvatar.png" alt="Jose">
                        
                    </div>
                    <div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left">
                        <a target="_blank" class="link-dark" href="https://jerry-master.github.io/Home-Page/">Jose</a>
                        <a target="_blank" href="https://twitter.com/GranaDataJose" class="btn follow">Follow</a>
                        
                        <a target="_blank" href="/read_time" class="btn readtime">4 min read</a>
                        
                        <span class="author-description">Data scientist from Alcalá la Real. Studied at BarcelonaTech, worked as a researcher at the UGR and UPC, was a machine learning engineer at El Ranchito and Nemeda and now work in Koh Young Research Spain. Always wanting to explain my knowledge to the world.</span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Neural Architecture Search (Part 3)</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                <!-- End Toc -->
                <p>We have seen in <a href="/NAS" target="_blank">Part 1</a> and <a href="/NAS-parte2" target="_blank">Part 2</a> that neural architecture search can be used to find hyperparameters and to design recurrent networks. Is there any more use to it? The answer is obviously yes (otherwise there would be no point on this post). Let’s recap, NAS is just a method where you learn something that is non-learnable by backpropagation using trial and error, aka, reinforcement learning. The key for using it in more complex settings is to have a well defined space of parameters. In the first part that space was simply the real range for each hyperparameter. In the second part the search space included every recurrent network. Since that is a pretty broad space we narrowed it down by only working with some specific recurrent networks that could be generated by a tree-like structure. In this third part the search space is going to be the set of all convolutional neural networks. And to reduce that space to something manageable we are only going to consider networks generated by residual connections. Residual connections are typically just an addition operation like this</p>

<p class="text-center"><img class="" src="/assets/images/NAS/residual-def.png" alt="simple" /></p>

<p>The good property of residual connections is that they prevent vanishing gradients for very deep networks. Their discovery allowed to increase the number of layers. ResNet achieved state of the art results when it was first created, and big transformer-based architecture also use residual connections. Normally the residual connection is between one layer and the next one. Can you think of a way of generalising this? Is there a ways of creating a bigger set of possible architectures? Similar to what we did for recurrent networks, we could break that restriction of just connecting to the next layer. This way we could end up with something like the architecture presented in the original NAS paper</p>

<p class="text-center"><img class="" src="/assets/images/NAS/orig-res.png" alt="simple" /></p>

<p>Again, the real question is how to encode that architecture, because using a picture is not quite computer-friendly. Think of the way of representing a graph, you use the adjacency matrix, or the adjacency lists. Here it is similar, for each layer you just need to know the inputs. Or in other words, the architecture is saved as adjacency lists of incoming vertices. That’s simple enough for the controller to generate. For each layer the controller has an array of previous layers and simply selects one or more indices from there representing the incoming residual connections. The rest of hyperparameters are generated afterwards. See the diagram</p>

<p class="text-center"><img class="" src="/assets/images/NAS/res-diagram.png" alt="simple" /></p>

<p>What the anchor point box is doing is just that, sampling indices representing the incoming layers. That box may implemented as a feed forward network with several classes. It could seem that the number of classes varies and so we cannot fix the last layer to have a constant size, and that is true, but for a given layer it is always the same. Therefore, fixing the total number of layers also fixes the number of output classes for each anchor point and so we can train them. Another reasonable way to implement the anchor box is as a multinomial variable. That way you only need to learn the probabilities, maybe conditioned to something. Although that poses another problem since another way of applying backpropagation needs to be designed, probably by using some reparameterisation trick. The original implementation was done through attention. They gave some hidden state to each anchor box and used Bahdanau attention mechanism to select the most similar layers (according to that hidden state that is learnable) to use as residual connections. This solves the problem of variable sized input, you can use the same attention mechanism for every anchor point. Thus, reducing the number of total parameters while making all the connections related to each other.</p>

<p>But simply selecting residual connections at random has one issue, have you seen it? There could be layers without connections, or layers that are not connected to anything. The solution is simple, for those layers that are not connected, you simply connect them to the last layer. And for those that doesn’t receive any input, the input image is used as the input, and not any other layer. This way, many complex architectures can emerge, not just linear ones. And there you have it, you can use reinforcement learning to explore the search space of convolutional neural networks.</p>

<p>Here I have presented you three ways of using neural architecture search to better design neural network architectures. If you are feeling short of ideas to create networks, maybe try designing a broad search space and traing a NAS controller on a toy dataset. And don’t feel that it is an outdated technique, Google is still using it, see <a href="https://ai.googleblog.com/2022/08/building-efficient-multiple-visual.html">this article</a>. The main disadvantage of this tecnique is that it requires a lot of computational power, but as everything else, you can use it with reduced capabilities on smaller datasets to try on your own. Maybe you find the next transformer this way, who knows?</p>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2022-08-22">22 Aug 2022</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="/categories#Deep-Learning">Deep Learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#Happy-Ideas">Happy Ideas</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#Reinforcement-Learning">Reinforcement Learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#Supervised-Learning">Supervised Learning</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="/gan_optimality_proof/"> &laquo; GAN optimality proof</a>
            
            
            <a class="next d-block col-md-6 text-lg-right" href="/stable_diffusion_tutorial/">Stable Diffusion Tutorial (Deprecated) &raquo; </a>
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->
        
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span> Never miss a <b>story</b> from us, subscribe to this blog</span>
        <form
          class="wj-contact-form validate"
          action="https://gmail.us4.list-manage.com/subscribe/post?u=6bc2f64e5d9109bd06bf859c5&amp;id=68bbfe755b&amp;f_id=005c05e9f0"
          method="post"
          id="mc-embedded-subscribe-form"
          name="mc-embedded-subscribe-form"
          target="_blank"
        >
            <div class="mc-field-group center-inputs">
                <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
                <input type="submit" value="Subscribe" name="subscribe" class="button">
            </div>
        </form>
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="/categories#Stories">Stories (5)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Philosophy">Philosophy (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Deep-Learning">Deep Learning (7)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Reinforcement-Learning">Reinforcement Learning (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Supervised-Learning">Supervised Learning (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Happy-Ideas">Happy Ideas (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#GAN">GAN (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Theory">Theory (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#unsupervised-learning">unsupervised learning (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Tutorial">Tutorial (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Servers">Servers (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Raspberry-Pi">Raspberry Pi (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Hidden-Markov-Model">Hidden Markov Model (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Django">Django (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Docker">Docker (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#AWS">AWS (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#AI">AI (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Unsupervised-Learning">Unsupervised Learning (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Python">Python (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Psychology">Psychology (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Economy">Economy (1)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== 
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2025 GranaData 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>-->
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>


<script src="/assets/js/lazyload.js"></script>


<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//demowebsite.disqus.com/count.js"></script>


</body>
</html>
