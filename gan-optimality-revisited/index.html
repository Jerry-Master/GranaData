<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="icon" href="/assets/images/favicon.ico">

<title>GAN optimality proof revisited | GranaData</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>GAN optimality proof revisited | GranaData</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="GAN optimality proof revisited" />
<meta name="author" content="jose" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Three years have passed since I published two posts related to the original formulation of the Generative Adversarial Networks (GANs). Three very crazy years in which the state of the art (SOTA) of generative models has surpassed any kind of expectation. Specially in video models that have now passed Tik-Tok quality by of large, they are still below TV quality and, of course, below cinema quality, but it is a lot more than what I predicted back in the days. The new SOTA models are all based on a different formulation from GANs, they use diffusion models. I am not going to write about diffusion models since that topic is already well covered in this post together with a continuation for video diffusion models. And for those of you who want to have deeper mathematical intuition there is this wonderful paper with hundreds of formulas and derivations that gets as deep as you can get into the foundations of diffusion models. However, this post is again about GANs, more concretely, about the optimality proof." />
<meta property="og:description" content="Three years have passed since I published two posts related to the original formulation of the Generative Adversarial Networks (GANs). Three very crazy years in which the state of the art (SOTA) of generative models has surpassed any kind of expectation. Specially in video models that have now passed Tik-Tok quality by of large, they are still below TV quality and, of course, below cinema quality, but it is a lot more than what I predicted back in the days. The new SOTA models are all based on a different formulation from GANs, they use diffusion models. I am not going to write about diffusion models since that topic is already well covered in this post together with a continuation for video diffusion models. And for those of you who want to have deeper mathematical intuition there is this wonderful paper with hundreds of formulas and derivations that gets as deep as you can get into the foundations of diffusion models. However, this post is again about GANs, more concretely, about the optimality proof." />
<link rel="canonical" href="https://granadata.art/gan-optimality-revisited/" />
<meta property="og:url" content="https://granadata.art/gan-optimality-revisited/" />
<meta property="og:site_name" content="GranaData" />
<meta property="og:image" content="https://granadata.art/assets/images/GAN/gan-cat3.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-20T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://granadata.art/assets/images/GAN/gan-cat3.png" />
<meta property="twitter:title" content="GAN optimality proof revisited" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"jose"},"dateModified":"2025-07-20T00:00:00+00:00","datePublished":"2025-07-20T00:00:00+00:00","description":"Three years have passed since I published two posts related to the original formulation of the Generative Adversarial Networks (GANs). Three very crazy years in which the state of the art (SOTA) of generative models has surpassed any kind of expectation. Specially in video models that have now passed Tik-Tok quality by of large, they are still below TV quality and, of course, below cinema quality, but it is a lot more than what I predicted back in the days. The new SOTA models are all based on a different formulation from GANs, they use diffusion models. I am not going to write about diffusion models since that topic is already well covered in this post together with a continuation for video diffusion models. And for those of you who want to have deeper mathematical intuition there is this wonderful paper with hundreds of formulas and derivations that gets as deep as you can get into the foundations of diffusion models. However, this post is again about GANs, more concretely, about the optimality proof.","headline":"GAN optimality proof revisited","image":"https://granadata.art/assets/images/GAN/gan-cat3.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://granadata.art/gan-optimality-revisited/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://granadata.art/assets/images/logo.png"},"name":"jose"},"url":"https://granadata.art/gan-optimality-revisited/"}</script>
<!-- End Jekyll SEO tag -->



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(','\\)'] ],
      displayMath: [ ['$$', '$$'], ['\\[','\\]'] ],
      processEscapes: true,
      processEnvironments: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/h-hg/docsify-pseudocode/dist/docsify-pseudocode.min.js"></script>


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

<!-- Twitter cards -->
<meta name="twitter:site"    content="@GranaDataJose">
<meta name="twitter:creator" content="@GranaDataJose">
<meta name="twitter:title"   content="GAN optimality proof revisited">


<meta name="twitter:description" content="Data science from Granada to the world.">



<meta name="twitter:card"  content="summary_large_image">
<meta name="twitter:image" content="https://granadata.art/assets/images/GAN/gan-cat3.png">

<!-- end of Twitter cards -->

</head>


<!-- change your GA id in _config.yml -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-46278016-1', 'auto');
ga('send', 'pageview');
</script>



<body class="layout-post">
	<!-- defer loading of font and font awesome -->
    <noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>

    
	


<!-- Begin Menu Navigation
================================================== -->

<nav class="navbar navbar-expand-sm navbar-dark bg-dark shadow fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="GranaData">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav mr-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Blog</a>
                </li>

                
                <li class="nav-item">
                
                <a class="nav-link" href="/about">About</a>
                </li>
                
            </ul>
            <ul class="navbar-nav ml-auto">

                <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">GranaData</h1>
    <p class="lead">
        Data science from Granada to the world.
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container inside-container">
    
    <div class="row">

        <!-- Post Share -->
        
        <!-- Post -->
        
        
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-right mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="/assets/images/JoseAvatar.png" alt="Jose">
                        
                    </div>
                    <div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left">
                        <a target="_blank" class="link-dark" href="https://jerry-master.github.io/Home-Page/">Jose</a>
                        <a target="_blank" href="https://twitter.com/GranaDataJose" class="btn follow">Follow</a>
                        
                        <a target="_blank" href="/read_time" class="btn readtime">4 min read</a>
                        
                        <span class="author-description">Data scientist from Alcalá la Real. Studied at BarcelonaTech, worked as a researcher at the UGR and UPC, was a machine learning engineer at El Ranchito and Nemeda and now work in Koh Young Research Spain. Always wanting to explain my knowledge to the world.</span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">GAN optimality proof revisited</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                <!-- End Toc -->
                <p>Three years have passed since I published two posts related to the original formulation of the Generative Adversarial Networks (GANs). Three very crazy years in which the state of the art (SOTA) of generative models has surpassed any kind of expectation. Specially in video models that have now passed Tik-Tok quality by of large, they are still below TV quality and, of course, below cinema quality, but it is a lot more than what I predicted back in the days. The new SOTA models are all based on a different formulation from GANs, they use diffusion models. I am not going to write about diffusion models since that topic is already well covered in this <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank">post</a> together with a continuation for <a href="https://lilianweng.github.io/posts/2024-04-12-diffusion-video/" target="_blank">video diffusion models</a>. And for those of you who want to have deeper mathematical intuition there is <a href="https://arxiv.org/abs/2208.11970" target="_blank">this wonderful paper</a> with hundreds of formulas and derivations that gets as deep as you can get into the foundations of diffusion models. However, this post is again about GANs, more concretely, about the optimality proof.</p>

<p>Three years ago I was not convinced by Goodfellow’s proof of this theorem:</p>

<div class="theorem"> The global minimum of $C(G) = \max_{D}V(G,D)$ is reached if and only if $p_{data}\equiv p_g$. Also, that minimum value is $-\log(4)$.</div>

<p>Just to recap, this theorem is saying that the solution of the min-max problem is reached when we have a perfect generator. And the step that bothered me was in an intermediate proposition:</p>

<div class="prop"> If $G$ is fixed, the optimal $D$ is 
<div>$$ D^*(\textbf{x}) = \frac{p_{data}(\textbf{x})}{p_{data}(\textbf{x}) + p_{g}(\textbf{x})} $$</div>
</div>

<p>This proposition computed the optimal discriminator given the generator. And in the limit it results in a stupid discriminator who always predicts 50-50 probability of being real and fake. I already showed <a href="/gan_optimality_proof#my-doubts" target="_blank">my doubts</a> with respect to one of the steps regarding the proof of this proposition. In fact, I wrote a <a href="https://datascience.stackexchange.com/questions/113390/minor-error-in-ian-goodfellows-gan-optimality-proof" target="_blank">post</a> back in 2022 asking if anyone knew of a way to overcome what seemed to be a minor error. Surprisingly, after three years somebody has taken the time to provide a more convincing proof of that tiny step and also highlighted an omission in the Goodfellow paper. So, without further ado, let’s see what I was missing and what Goodfellow was missing.</p>

<p>First, let’s remember what was the proof. It involves two steps: a change of variable and a maximization problem. The change of variable is a bit technical and you can give a look to an sketch of the details and the theorems involved in <a href="/gan_optimality_proof#prop" target="_blank">my previous post</a>. It reduced the problem to finding the maximum of this integral</p>

<div>$$ \int_{\textbf{x}} p_{data}(\textbf{x})\log(D(\textbf{x})) + p_g(\textbf{x})\log(1-D(\textbf{x})) d\textbf{x} $$</div>

<p>The original argument to maximize this integral read, quote: “For any $(a,b) \in \mathbb{R} - {(0,0)}$, the function $y \rightarrow a \log (y) + b \log (1-y)$ achieves its maximum in $[0,1]$ at $\frac{a}{a+b}$.” That argument for me was insufficiently explained, but it is correct (for the most part). The argument is basically constructing a function that at each point is the maximum; therefore, when integrating, the result is the maximum possible. I initially understood the argument as simply optimizing the integrand which seemed incorrect to me because $(a,b)$ here are not constant. But later on, Graham Pulford pointed out that this is calculus of variations. So here it goes what I think is a more rigorous rewrite of that step. We define the lagrangian to be</p>

<div>$$ \mathcal{L}(\textbf{x}, D, D') = p_{data}(\textbf{x})\log(D(\textbf{x})) + p_g(\textbf{x})\log(1-D(\textbf{x})) $$</div>

<p>Thus, the optimum (if it exists) must satisfy the Euler-Lagrange equations:</p>

<div>$$ \frac{\partial \mathcal{L}}{\partial D} - \frac{d}{d\textbf{x}}\frac{\partial \mathcal{L}}{\partial D'} = 0 $$</div>

<p>It looks like I am treating multivariable as single variable but I am just using this notation for simplicity: $\frac{d}{d\textbf{x}} = \sum_i \frac{\partial}{\partial x_i}$. The Euler-Lagrange equations is only one equation in fact because there is no derivative in the lagrangian. Therefore we just need to solve for $\frac{\partial \mathcal{L}}{\partial D} = 0$. With some patience that leads to the desired result:</p>

<div>$$ 
\begin{align*}
&amp;\frac{\partial \mathcal{L}}{\partial D} = \frac{p_{data}(\textbf{x})}{D} - \frac{p_g(\textbf{x})}{1-D} = 0\\
\Rightarrow&amp; \frac{p_{data}(\textbf{x})}{p_g(\textbf{x})} = \frac{D}{1-D} = \frac{1}{1-D}-1\\
\Rightarrow&amp; \frac{p_{data}(\textbf{x}) + p_g(\textbf{x})}{p_g(\textbf{x})} = \frac{1}{1-D}\\
\Rightarrow&amp; D = \frac{p_{data}(\textbf{x})}{p_{data}(\textbf{x}) + p_g(\textbf{x})}
\end{align*}
$$</div>

<p>But there is a catch. This argument only works for sufficiently smooth lagrangians which is why intuitive arguments should be made rigorous. Taking the optimum at each point and integrating only returns the functional optimum if the functions are not pathologic, which in this context means twice differentiable. To solve the Euler-Lagrange you only need the integrand to be once differentiable, but to prove it is a maximum you need to look into the second derivative and see it is negative, which is, in fact, the case:</p>

<div>
$$ \frac{\partial^2 \mathcal{L}}{\partial D^2} = -\left(\frac{p_{data}(\textbf{x})}{D} + \frac{p_g(\textbf{x})}{1-D}\right) &lt; 0 $$
</div>

<p>And what happens if you do not have a sufficiently smooth integrand? In that case you no longer have such a simple way to find the optimal discriminator. But that surely only happens in the mind of the perturbed mathematicians, right? Wrong. That assumption can be broken very easily. <a href="https://ieeexplore.ieee.org/document/9641798" target="_blank">Graham Pulford</a> showed that having $\dim(\textbf{z}) &lt; \dim(\textbf{x})$ is enough to break the smoothness of the lagrangian. This comes from the first step of the proof: the change of variable. When reducing the dimensionality of a probability distribution function the behaviour can be pathological, breaking the smoothness. In that article I linked, he constructed several concrete examples showing such pathological behaviour. That same author <a href="https://ieeexplore.ieee.org/abstract/document/11030454" target="_blank">went a step further</a> and constructed a discriminator that was optimal but was nowhere equal to the $1/2$ that Goodfellow promised. Curiously enough, that discriminator produces $1$ almost everywhere and $0$ in a set of measure zero, which means it is also a stupid discriminator that is overconfident in saying that everything is fake. As expected, answering one question creates new ones. In this case I am left with the doubt of whether or not the optimal discriminator is always independent with respect to the generator. Maybe in another three years that is also solved?</p>

<p>Apart from the theoretical insights, all of these new results bring more light to the practical side of the GANs. Training a GAN was always an unstable process that required many tweaks to make it work. We now know the reason for that instability. The initial algorithm for training GANs was an iterative process that tuned discriminator and generator in alternate processes. The tuning method for the discriminator involved that formula that appeared in the proposition we mentioned before. That formula we now know has no guarantees of being always correct. Since the issue was with the smoothness, many tweaks for making the learning more stable did that in different ways. Either with regularization or controlling the rate of change. But, in the end, every method was just a patch to a fundamental flaw in the formulation. This may explain why everybody has moved to diffusion models. Their formulation is more robust and its training is better understood.</p>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2025-07-20">20 Jul 2025</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="/categories#Deep-Learning">Deep Learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#GAN">GAN</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#Theory">Theory</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#unsupervised-learning">unsupervised learning</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="/fast_slow/"> &laquo; Pensar rápido, pensar despacio</a>
            
            
            <a class="next d-block col-md-6 text-lg-right" href="/vibe-coding/">Vibe-coding, an informed guide. &raquo; </a>
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->
        
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span> Never miss a <b>story</b> from us, subscribe to this blog</span>
        <form
          class="wj-contact-form validate"
          action="https://gmail.us4.list-manage.com/subscribe/post?u=6bc2f64e5d9109bd06bf859c5&amp;id=68bbfe755b&amp;f_id=005c05e9f0"
          method="post"
          id="mc-embedded-subscribe-form"
          name="mc-embedded-subscribe-form"
          target="_blank"
        >
            <div class="mc-field-group center-inputs">
                <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
                <input type="submit" value="Subscribe" name="subscribe" class="button">
            </div>
        </form>
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="/categories#Stories">Stories (5)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Philosophy">Philosophy (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Deep-Learning">Deep Learning (7)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Reinforcement-Learning">Reinforcement Learning (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Supervised-Learning">Supervised Learning (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Happy-Ideas">Happy Ideas (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#GAN">GAN (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Theory">Theory (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#unsupervised-learning">unsupervised learning (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Tutorial">Tutorial (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Servers">Servers (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Raspberry-Pi">Raspberry Pi (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Hidden-Markov-Model">Hidden Markov Model (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Django">Django (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Docker">Docker (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#AWS">AWS (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#AI">AI (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Unsupervised-Learning">Unsupervised Learning (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Python">Python (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Psychology">Psychology (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Economy">Economy (1)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== 
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2025 GranaData 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>-->
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>


<script src="/assets/js/lazyload.js"></script>


<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//demowebsite.disqus.com/count.js"></script>


</body>
</html>
