---
layout: post
title:  "Vibe-coding, an informed guide."
author: jose
categories: [ AI ]
featured: false
hidden: false
comments: false
share: false
image: assets/images/vibe-coding.png
time-read: 8
---

At the beginning of this year 2025, Andrej Karpathy coined the term: ["vibe-coding"](https://x.com/karpathy/status/1886192184808149383){:target="_blank"}. And since then we have seen the AI discourse become even less serious over time. The typical FOMO discourse (Fear Of Missing Out) continues to say that if you are not using AI you are missing out, that AI will replace us all, that what previously took years now takes hours and so on. I am a big fan of AI and want it to succeed. My side is definitely with [e/acc](https://effectiveaccelerationism.substack.com/p/repost-effective-accelerationism){:target="_blank"}, but I also want to throw a bit of clarity to all this madness. AI is progress and cannot be stopped, but we must think critically and analyse exactly what is the point in which we found ourselves right now. Vibe-coding is being monetised which means the PRs (Public Relations) are everywhere trying to convince us to buy their products. [Replit](https://replit.com/){:target="_blank"} is one of those tools that sells itself as a way to build apps just with natural language. It is quite impressive and I have tested and created [one webpage](https://preparaomegrx.es/){:target="_blank"} entirely with it. Nevertheless, Replit is also responsible for [deleting another company's codebase](https://www.businessinsider.com/replit-ceo-apologizes-ai-coding-tool-delete-company-database-2025-7){:target="_blank"}. As I said I want to bring some light to the discourse and this post is going to be an informed analysis on how to do vibe-coding properly, what are its risks and when is it worth it. 

## What the data shows

Let's start with what we already know. AI is being here for a while and studies are starting to appear. The first study that got to the news was about [Copilot](https://www.microsoft.com/en-us/research/wp-content/uploads/2023/12/AI-and-Productivity-Report-First-Edition.pdf){:target="_blank"}. They compared the time it took developers to complete tasks with and without Copilot. They measured improvements in the range of 26% and 73%. This study was optimistic since it was coming from Microsoft and it was about one of its products which may raise some eyebrows. But even this optimistic value (73%) is not "what took months takes days". That is one of the premises that should be banned from the current discourse. Another independent study from Stanford found that improvement to be [14%](https://www.nber.org/system/files/working_papers/w31161/w31161.pdf){:target="_blank"}, with some productivity metrics going up to a 22.2% improvement, which is a more conservative value. That study started to show that the improvement depended highly on your level of expertise, which is something quite reasonable. Before somebody says that those articles are from 2023, those results have been replicated in [2024](https://fortegrp.com/insights/ai-coding-assistants){:target="_blank"} (~50%) and [2025](https://www.pwc.com/gx/en/news-room/press-releases/2025/ai-linked-to-a-fourfold-increase-in-productivity-growth.html){:target="_blank"} (~25%). This clearly means that there is some gain, but there is high variability. The next question is when does this gain happen so that we don't use AI when it is pointless.

One study advancing our knowledge in that matter was recently [published by METR](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/){:target="_blank"}. This study showed a 19% decrease in productivity in one very specific case: experienced developers of complex projects that knew perfectly all the details of their projects. It also found that developers wrongly believed they were being more productive, something I think is caused by the current discourse. We have been told for a few years that AI is making us more productive so we feel like that even when that is not the case. In the Copilot study developers estimated their gains properly, they felt more productive and were more productive. But here is the opposite. The subjective perception is challenged by the objective measurements. In a curious case of Stigler's Law, this was not the first study in that direction even though it was the first to reach the public discourse. The [2024 DORA report](https://services.google.com/fh/files/misc/2024_final_dora_report.pdf){:target="_blank"} reached the conclusion that AI is good for "code quality, documentation, and review processes". However, it was detrimental for "software delivery performance" (aka teams are slower). Those two reports are all I could find so far using Google and Perplexity that provide negative results, but they provide a clear enough picture into what are the correct and incorrect ways to use AI for software development.

## What can we do now?

In a few years I will come to this post and the advice I give is going to be obsolete, but in the current state of affairs what I will describe below are probably good practices when using AI. The main issue with our current systems is the long context. The METR study pointed to complex systems as a place where AI is falling behind, why? AI can perfectly follow orders when you tell it to write a function and has good enough reasoning capabilities for most of the day to day problems, what is happening? The answer is [long context rot](https://research.trychroma.com/context-rot){:target="_blank"}. Current systems are technically capable of managing millions of tokens which is something my professors at university thought impossible. That context is enough to read entire books and answer any question about any page of the book. Not even a human can do that. Therefore, again, what is happening? What is happening is that the performance is decaying as Lecun [predicted](https://youtu.be/ETZfkkv6V7Y?si=bukWQA2SdbgAGgUR&t=635){:target="_blank"}, he was initially wrong and correctly criticised but in the end, and for completely unrelated reasons, he seems to be right, once again. Complex systems require a massive amount of knowledge before introducing a new change. Otherwise another feature may stop working, just like in the [meme](https://www.reddit.com/r/ProgrammerHumor/comments/b61snk/that_famous_function/){:target="_blank"} but for real. Long context have been a problem for a while. Before long context models we had [RAG](https://datos.gob.es/en/blog/rag-retrieval-augmented-generation-key-unlocks-door-precision-language-models){:target="_blank"} systems that are still being developed today. With the arrival of the agentic models a new protocol was created: [MCP](https://www.anthropic.com/news/model-context-protocol){:target="_blank"}, which enabled the usage of tools and gave the model access to different contexts. This technology is still in development and will solve the complexity issue in the future but for now, we are stuck with the complexity issue.

Now that we know the main limitation of current systems, what could be a good workflow to improve productivity? We have a technology that is good in some places and bad in others. The correct approach is neither to discard nor to glorify it. The correct approach is to design a workflow to mitigate risks and increase productivity. This part is a bit technical, so if you just want to know when the usage of this technology is financially justified skip it and go to the next one. The workflow I propose is based on my current experience. I have tried many tools, almost all the LLMs that are in the market, I have worked in production systems, big codebases with no documentation, and also have my own personal projects. Apart from my experience, I also have the habit of discussing software development methodologies with my coworkers. The main concern of senior developers that I have met when using this technology is maintanibility and scalability. Another constraint is that a senior developer must know what every single line of code does, otherwise you just have a functional product that nobody can fix when it breaks. So, how can we reconcile the senior developer job with that of the machine?

The workflow in itself is quite simple, but the demon is in the details. The workflow that has worked most for me is the following. The first time you arrive at a new codebase you know nothing. It does not matter how many years of experience you have, you start from zero. Therefore, before you are productive you need to get familiar with the codebase. Here, AI is your best ally. Instead of looking at the code and thinking hard what is happening you can ask the bot. The quintessential example is regex. Suppose you find this regex:

{% highlight python %}
r'<a[^>]*index\s*=\s*["\']?(\d+)["\']?[^>]*>(?=.*?<p[^>]*index\s*=\s*["\']?\1["\']?[^>]*>)(.*?)</a>'
{% endhighlight %}

A simple question to ChatGPT tells you that it is looking for html a tags with a child p tag that has the same index. Some engineer may have needed to find duplicated html indices to fix some bug and did not have time to document the hacky solution. A few seconds of LLM inference and you can continue understanding the code. 

The next step after you have familiarized yourself with the codebase is to identify patterns. LLMs are matching patterns most of the time, it is their best quality. In day to day work there are many boring tasks that could be easily automatized. One example is generating boilerplate code. LLMs are faster than us at typing. When I learnt to type, to obtain the certificate you needed to type 300 characters per minute, or around 60 words per minute (wps). Some pages show the average is [40 wps](https://www.typingpal.com/en/typing-test#:~:text=What%20is%20the%20average%20typing,to%2070%20words%20per%20minute.){:target="_blank"} with developers probably in the 70-80 wps side. Evan Chen, which was famous for typing fast is around 150 wps. But that is peanuts compared to LLMs, ChatGPT API throughput is [55 tokens per seconds](https://community.openai.com/t/the-output-speed-of-gpt-4o-in-chatgpt-web-and-app-is-extremely-slow/1123614){:target="_blank"}, which is 495 wps. And that is one of the slowest LLMs out there. Groq systems are way faster and some even talk about the velocity in [pages per second](https://www.reddit.com/r/LocalLLaMA/comments/15nig1k/comment/ksuqaru/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button){:target="_blank"}. If you can tell the AI to code a repetitive task that is very long, you can save a lot of time simply because it is faster at typing than you. The difficult part is deciding what processes are suitable for this.

Once the repetitive patterns are identified the next step is to design skeleton code. Instead of coding everything, you first code the skeleton. Create some main function that calls several other functions and then let the AI implement those other functions. For this task any Cursor-like tool is definitely the best tool, like windsurf. The key is to reduce the complexity. You know the code and you know the solution, split it into simpler tasks and when you reach the level of complexity that current LLMs can handle, you just let them do it. 

Let's put an example. I recently had a project in which I had to work with OpenCV CUDA. Many of you may be familiar with OpenCV but I doubt that you know all the details of its CUDA version. For instance, `cv::Mat` is normally continuous but `cv::cuda::GpuMat` is almost never continuous. That piece of knowledge is hidden in a [note](https://docs.opencv.org/4.x/d0/d60/classcv_1_1cuda_1_1GpuMat.html){:target="_blank"} in the API reference. LLMs kind of helped to find that, for instance. However, documentation ends there. How to access images pixel values? How to pass `cv::cuda::GpuMat` to a kernel? That is even more hidden and LLMs are of no help finding that information because it does not exist. You need to go to the code to answer those questions. OpenCV code has a lot of templates and C++ template language is known for being notoriously difficult to interpret. But, after some back and forth with LLMs I finally understood that passing a `cv::cuda::GpuMat` to a kernel and accessing it was as simple as putting `cv::cuda::PtrStep<T>` in the kernel signature and doing `img(y, x)`. This is the first phase, using models to understand and familiarise with the code. Then, I designed a common structure for all the kernels, fixing grid dimensions, fixing the error checking macros and other parts that were going to be repetitive. For instance, after calling a kernel function I needed to add `CUDA_CHECK(cudaDeviceSynchronize()); CUDA_CHECK(cudaGetLastError());` for error checking. That is the same everywhere. Repetitive and boring, so if I can make the AI write it for me it is going to save me time. After I designed the skeleton for all the kernels and all the kernel wrappers I finally started using AI to code. With a simple system prompt I put some limits to what the code could be and the best practices that the bot should follow. Whenever the function names were descriptive enough and the task was simple enough the code was working on the first try. Another critical aspect to make all of this work is to do [test driven development](https://en.wikipedia.org/wiki/Test-driven_development){:target="_blank"}. That makes vibe-coding way more effective.

After you familiarised yourself with the code, you identified patterns and created the roadmap for the AI, the final step is to review the code. There are parts of the review process that can be automatised also. The DORA report that was one of the critical ones still pointed that AI was positive in this step of the development process. In CUDA code you normally check these things: memory coalescing, thread divergence, bank conflicts in shared memory, occupancy and race conditions. This is anecdotical evidence, but AI once detected a very obscure race condition I had in some code I did myself without AI help. Race conditions are the kind of needle in hay problems AI is good for. Memory coalescing is also a very LLM-friendly task because you just need to check that indices are ordered correctly. That is the kind of mechanical work that AI is good for and humans are not. Automating that leaves room to decide what the kernel should do, which is where my creativity is needed. Apart from that, before merging the code to the main branch, LLMs can perform the first review of your code before other developers get there. It can detect possible deadlocks, memory leakages or simply parts of the code that do not follow the style guidelines. Of course, you need a real human making the final review, but putting an AI in the middle can reduce the workload in the human reviewer.

## Is it worth it?

Finally let's put some numbers into the financial side of the equation. We have talked about the performance benefits which range from the optimistic 73% of Microsoft to the more realistic 14% of Stanford. Now let's talk about the costs. To know how much AI is costing we need to have an estimate of how many tokens per month does a developer consume. There is no conclusive data on this, mainly because the usage is not extended enough. I found this [blogpost](https://getdx.com/blog/ai-coding-tools-implementation-cost/#what-teams-actually-pay){:target="_blank"} that estimates the consumption to be 1 million tokens per month per developer. That seems like a lot but remember that codebases are big, and the context required to answer the question is what really consumes tokens, so it could be a reasonable estimate. And what is the cost per token of the latest models? As per 27th of July of 2025 the pricing in the OpenAI API is 8$ / 1M tokens for the output of the smartest model. That amounts to a total of 96$ per developer per year. I don't know if the price has VAT included, but we can round to 150$ to account for that and have a pessimistic estimate. Is that worth it? Well, unless you pay your developers less than 2000$ a year, then yes. Even with the 14% estimate you are better off using AI. However, you need to be careful to not use AI in unneeded places, or you could end up worse. 

## Conclusion

Software development, when done well, is hard. Current systems have come a long way and will continue to improve. But in the current state of affairs their usage needs to be carefully monitored. There are places where it boosts productivity modestly and there are cases when it can be detrimental. Until AI is good enough to replace us all, we need to be responsible and make decisions based on the data and not on marketing claims.
