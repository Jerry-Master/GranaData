
var documents = [{
    "id": 0,
    "url": "https://granadata.art/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://granadata.art/about",
    "title": "About",
    "body": "This blog was bornt from my need to explain everything that I have learnt during my degrees and to show to the world the results of my research in a more divulgative way. The name from the blog was inspired from the love I have for the place I belong: Granada, and the love I have for Data Science. As you may have noticed, the footer image is of La Alhambra, as it couldn't be otherwise. About me I can say that I always have many things in mind, but seldom have time to put them in practice. Below is a photo of me surrounded by olives, you cannot see it but behind the camera there was a rally. I hope you enjoy my blog.  Buy me a coffeeThank you for your support! Your donation helps me to maintain and improve this blog. Buy me a coffee"
    }, {
    "id": 2,
    "url": "https://granadata.art/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "https://granadata.art/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                                                         The Hack Life                              :               Believe or not today I got up at 4am to take a train Madrid -&gt; Barcelona to work for more than 24 hours straight for. . . :                                                                                                                                                                       Jose                                                5 min read ·                                 12 Nov 2023                                                                                                                                                                                                                                                                                                                                                                                                The buddhist pace                              :               This is a story I read years ago from Reddit that has guided me through most parts of my life and whenever I want to. . . :                                                                                                                                                                       Jose                                                3 min read ·                                 26 Jun 2022                                                                                                                            All Stories:                                                                                                     Vibe-coding, an informed guide.               :       At the beginning of this year 2025, Andrej Karpathy coined the term: “vibe-coding”. And since then we have seen the AI discourse become even less serious over time. The typical. . . :                                                                               Jose                        8 min read ·                 27 Jul 2025                                                                                                                                    GAN optimality proof revisited              :       Three years have passed since I published two posts related to the original formulation of the Generative Adversarial Networks (GANs). Three very crazy years in which the state of the. . . :                                                                               Jose                        4 min read ·                 20 Jul 2025                                                                                                                                    Pensar rápido, pensar despacio              :       Uno de enero, todo el mundo se propone cambiar algún hábito. Pocos los consiguen. En mi caso no me propuse nada pero surgió una idea que posteriormente se transformó en. . . :                                                                               Jose                        22 min read ·                 16 Jul 2024                                                                                                                                    How to build a python library              :       When I was halfway through my bachelor’s thesis I decided to package everything into a python package to facilitate easier usage for the next student. However, instead of just following. . . :                                                                               Jose                        4 min read ·                 24 Mar 2024                                                                                                                                    Crowdsourcing methods for cancer detection              :       I recently survived the reviewing process of a scientific article called: “Annotation protocol and crowdsourcing multiple instance learning classification of skin histological images: The CR-AI4SkIN dataset” and today I want. . . :                                                                               Jose                        11 min read ·                 09 Nov 2023                                                                                                                                    How to make a full audiobook with AI tools.               :       A few days ago I got access to DALLE·3 and thought, “what if I made a book for children with it?” It may seem a bold idea, but as you. . . :                                                                               Jose                        7 min read ·                 15 Oct 2023                                               &laquo; Prev       1        2        3      Next &raquo; "
    }, {
    "id": 4,
    "url": "https://granadata.art/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 5,
    "url": "https://granadata.art/page2/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 6,
    "url": "https://granadata.art/page3/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "https://granadata.art/2024/",
    "title": "2024",
    "body": "2025/08/13 - Prefacio: Sé que esto no es un libro, y que esta sección debería llamarse introducción y no prefacio. Pero como que este mi blog, pues decido yo mis propias reglas. Lo que vengo a contar a continuación, aunque no muy extenso, sí que es denso. Con el paso de los años voy notando que es imposible acordarse de todo lo que sucede, los detalles en los recuerdos se van erosionando dejando mayormente las emociones que sentimos en aquellos momentos. Una parte del motivo por el que escribo estos párrafos es para poder volver años más tarde, ver qué fue lo que pasó y comprobar si es así cómo lo recuerdo. Otro motivo es usarlo como terapia, y, para qué mentir, también lo hago para alimentar a la IA. Escribir un post así es complicado porque tiene que ser íntimo, pero sin violar la privacidad de nadie. Tiene que tener detalles y al mismo tiempo el hilo central debe quedar claro. Y además de ser descriptivo, también debe intentar hacer reflexionar al lector. Dudo haber conseguido el equilibrio perfecto, pero al menos lo he intentado. Sin duda este es el post al que más horas le he dedicado. Empecé a redactarlo en octubre de 2024 con la intención de terminarlo para la navidad de 2024 y publicarlo como un regalo para mí. Verano de 2025 y aún estoy redactando este prefacio y dándole los últimos retoques. Al final el regalo no será para mí, sino que será de cumpleaños para una persona muy especial para mí. Te invito a ti, lector o lectora, a adentrarte en un pequeño capítulo de mi vida, a ver desde dentro cómo viví aquel año 2024. 2024Siempre recordaré un post de un viejo mentor que decía algo como que en la vida hay momentos de reflexión y momentos de grindear. Enero de 2024 fue sin duda un periodo de mucha reflexión. Las decisiones que marcan el rumbo de nuestras vidas normalmente representan un pequeño porcentaje de nuestra vida. Son decisiones difíciles que determinan los próximos años y que no tienen marcha atrás. Una vez tomadas hay que llegar hasta el final. Un ejemplo de esto es el estudio de una carrera universitaria, no lo vas a dejar el último año así que una vez tomada la decisión la presión de seguir hasta el final es muy grande. Esta decisión yo la tomé en 2018 y determinó mi vida hasta el 2023. Llegado el 2023 me tocaba de nuevo enfrentarme a la vida aunque no estaba preparado. Algunas vidas no vienen con manual de instrucciones así que te toca escribirlo a ti. Muchas vidas sí que vienen con una guía. La sociedad tiene ya escritas las vidas de mucha gente. Desde quien va tachando la lista estándar de coche, casa, pareja e hijos, hasta quien sigue el camino estándar de carrera, máster, doctorado, postdoctorado, profesorado y cátedra. Pero si tu objetivo es inventar algo que se cree imposible entonces la sociedad no sabe qué decirte, y eso complica aún más las decisiones. En 2018 elegí estudiar ciencia de datos porque me acercaba a ese destino. Fue una decisión difícil porque aunque la carrera la tenía clara, el cómo pagarla no tanto. La falta de recursos siempre es un reto añadido. No basta con saber qué quieres sino que tienes que encontrar una manera de lograrlo dentro de tus posibilidades. 2023 no fue diferente. Había una opción clara pero faltaron los recursos una vez más. Por eso me tomé un periodo “sabático”. Para mí, un periodo “sabático” es aquel en el que solo tengo un trabajo a tiempo completo. Así tengo tiempo para pensar y buscar qué rumbo tomar mientras ahorro para poder ejecutar la próxima decisión. Mi plan al irme a El Ranchito era ese. Tomarme un año “sabático” hasta que llegase 2025 y después ya decidir. Sin embargo, al universo no le gusta que me lo tome con calma por lo que me obligó a tomar una decisión un poquito antes. Los guionistas se pusieron en huelga, la industria del cine entró en recesión y a mí me pusieron en ERTE trabajando al 75%. Así que llegando al final de 2023 tuve que buscar alternativas. Como no quería tomar una decisión dejé que el universo eligiera por mí ya que me había obligado a elegir. Jugué un uno reverse. ¿Cómo se hace eso? Muy sencillo, tomar una decisión no te involucra solo a ti. Si quieres ir a un laboratorio te tienen que aceptar como PhD, si quieres trabajar en una empresa tienes que pasar las entrevistas. Así que lo que yo hice fue diversificar mis resultados. ¿Qué pasa? Que puesto así aún no has obligado al universo a decidir aún. Si tienes tres opciones y te cogen en las tres tienes que elegir igual. Así que lo que hice fue crear un árbol de decisión. Si sucede A hago B, si sucede C hago D y así sucesivamente. De esta forma el universo ya sabe qué voy a hacer y el evento que finalmente acaba sucediendo se puede ver como la elección del universo. Si tengo dos opciones yo determino de antemano cual tiene preferencia, así que si el universo quiere que coja la segunda me tienen que rechazar en la primera. Más que una religión es una filosofía de vida para no amargarte y que las decisiones pesen menos sobre tu espalda. De esta forma ni te duelen los rechazos ni te martirizas por los errores. Estas eran las opciones que le di al universo. En primer lugar, el laboratorio de Mihaela: de los más prestigiosos del mundo y con puerta directa a Deep Mind. Claramente la primera opción. El proceso de selección involucraba hacer un artículo en dos semanas y defenderlo como si fuera una tesis frente a sus alumnos. Yo contaba con la recomendación de un viejo amigo suyo y uno de sus estudiantes. Como segunda opción estaba seguir dónde estaba, sin más. Para que eso sucediera en El Ranchito debían quitar el ERTE y subirme el sueldo. Improbable pero no imposible. Esta opción me permitía atrasar más una decisión difícil, así que yo esperaba que no sucediese ya que el universo acostumbra a no ponérmelo fácil. Las otras dos opciones eran dos empresas que simplemente llamaré Alpha y Beta. Ninguna de esas dos empresas me atraía especialmente. Alpha se dedica a optimización del entrenamiento de modelos del lenguaje masivos y Beta se estaba orientando para atacar el mercado de los modelos del lenguaje de consumo. Mi rama favorita de la inteligencia artificial es la visión artificial, no el procesamiento del lenguaje. Lo cual es una putada porque pagan más en procesamiento del lenguaje y se me da mejor. Es por todo esto que el árbol de decisión se limitaba a las condiciones laborales, si no me cogía Mihaela me vendería al mejor postor. ¿Qué sucedió? Pues nada de eso. El universo me jugó su propio uno reverse. Cuando todo apuntaba a que la decisión estaba tomada surgió una quinta opción. Antes de desvelarla, veamos qué pasó con las opciones iniciales. En la entrevista de Mihaela me acribillaron a preguntas de una forma agobiante y no estuve a la altura. En cierto modo esto habla bien de ella porque ignoró las referencias y evaluó solo mis capacidades, algo que se echa en falta en el sistema español. No me cogieron posiblemente porque mi perfil es muy aplicado –algo que solucionaría en 2025 con un máster en mates puras– y ellos son bastante más teóricos. Aunque podría haber sido mi vía de entrada a Deep Mind seguramente me hubiera quemado. Así que aunque me dolió no conseguirlo en el fondo siento que es mejor así (“No hay mal que por bien no venga”). De hecho creo que hice mal poniéndola como la primera opción. La segunda opción tampoco sucedió. El Ranchito no se recuperó hasta el verano de 2024 y el departamento de Machine Learning no era una prioridad para el grupo Pitch Black, por lo que no merecía la pena seguir mi carrera allí. Ese empleo fue un desvío divertido de mis objetivos, pero tanto yo como las de recursos humanos sabíamos que era temporal. Cuando yo ya había descartado estas dos opciones empezaba el proceso de las otras dos. En Alpha pasé las entrevistas fácilmente porque mis conocimientos se alinean perfectamente con lo que buscaban y en Beta me hicieron una oferta también porque buscaban talento joven para el equipo. Alpha era una empresa grande con la que poco se podía negociar. Beta era una empresa pequeña que tenía más margen de maniobra con las condiciones laborales. En conjunto, si no hubiera habido una quinta opción, habría escogido a Beta. ¿Cuál era la quinta opción? Pues bien, en medio de todo este caos y de tantos cambios me contactaron unos headhunters para que empezara el proceso de selección de Koh Young Research Spain (KYRS). Normalmente yo a los headhunters los ignoro porque son bastante poco profesionales. No se miran tu perfil y te llaman cuando no encajas, o bien tienen condiciones precarias y lo ocultan. Sin embargo, esta vez fue diferente. El empleo era de visión artificial, lo cual es justo mi perfil. Y las condiciones laborales no las ocultaban, eran transparentes desde el minuto cero. Así que les di una oportunidad. La pega era que tenían el proceso de selección más exigente y largo que he visto nunca, lo que me desmontó todos los planes. El headhunter tuvo que pedirme por favor en varias ocasiones que siguiera con el proceso. La empresa iba ganando puntos cuanto más sabía de ella, y la probabilidad de entrar se me presentó como nula desde un principio. Todo esto dio pie a que me tuviera que enfrentar a una decisión verdaderamente difícil, aunque no la más difícil del año. Al abandonar a Beta, que había apostado por mí de esa manera, yo corría el riesgo de dañar mi reputación. Por contraposición, si aceptaba la oferta de KYRS podía hacer carrera profesional dentro de Granada en el ámbito en el que me había especializado. Los trabajos de visión artificial son muy escasos y requieren de experiencia para conseguir entrar. A la larga, KYRS iba a ser mejor sello en el currículum y el hecho de que tuviera oficinas en Granada me daba la tranquilidad de que no me van a obligar a irme de aquí. Porque teletrabajar siempre viene con letra pequeña, cuando la empresa quiera te puede obligar a ir presencial, si ya estás presencial no te van a mover de ahí. Esto puso a Beta y a KYRS en una balanza que tardó en inclinarse. Finalmente, y no sin remordimientos, opté por entrar en KYRS, decidiendo así el rumbo de mi próximo lustro. Seis meses duró el periodo de reflexión. Octubre, noviembre, diciembre, enero, febrero y marzo. Ese fue el plazo desde que se inicia el ERTE hasta que me establezco en Granada. Bastante tiempo para pensar, aunque no el suficiente para decidir. Nunca se tiene suficiente tiempo para decidir, como me volvería a dar cuenta más tarde en agosto de ese mismo año. Aún así, la decisión estaba tomada: haría carrera en KYRS. Empezaría en el equipo de inspección óptica y eventualmente me movería al sector de software médico para finalmente ascender a manager y proponer mis propios proyectos de neurocirujía automatizada. No se sabe si sucederá pero por lo menos el plan está ahí, entre otros planes que se fueron detallando posteriormente. Desgraciadamente no todos los planes se cumplen. En ese enero de 2024 también se gestó un plan que se rompería en pedazos meses más tarde. Un 6 de enero conocería a una mujer estupenda (aún pienso que lo es), llamémosla Ana. Por casualidad, el día 5 Ana y yo nos tropezamos en la cabalgata de reyes y al día siguiente tuvimos nuestra primera cita. Hablamos largo y tendido tomando un café al que ella me invitó, y que aún le debo, para despedirnos sin saber cuándo volveríamos a vernos. Pocas veces me he sentido tan a gusto y cómodo conociendo a una persona. Ana me hacía sentir seguro y deseado. Pero duró poco. Yo volvía a Madrid y ella a su ciudad por lo que no quedaba otra que separarse. El resto del año mi tiempo libre lo dedicaba a planear escapadas para intentar volver a verla. Una de ellas fue un viaje que organicé con el único propósito de escaparme en el viaje de vuelta a su ciudad. Movilicé a todos mis amigos solo para tener la oportunidad de acercarme a ella. Fue divertido. Sobretodo cuando nos quedamos atascados en Mérida por la huelga de tractores. A la vuelta mi plan falló porque ella también se había ido de viaje así que me volví solito y desolado. Nada como un buen chuletón para ahogar las penas, me acerqué al casa Pepe y mano de santo. Llevaba ya algún tiempo con curiosidad de ver cómo se sentía comer en un lugar tan franquista. Lo cierto es que es un tanto incómodo, pero tomándoselo con humor se lo pasa uno bien. La gente que va allí son básicamente copias de Mauricio Colmenero. El choque cultural me ayudó a olvidarme de no haber podido ver a Ana ese fin de semana y las preciosas vistas de Despeñaperros hicieron más soportable las seis horas de vuelta a Madrid. De Madrid no echaría en falta gran cosa. Me faltó tiempo para mudarme. El único detalle que recuerdo con melancolía son las preparaciones de olimpiadas. Tal es la melancolía que al venir a Granada lo primero que hice fue escribir a Pascual para intentar montarlo en Granada. Pero en ese aspecto Granada no es Madrid y puede que no lo sea nunca, aún así yo aspiro a despertar el talento granadino igualmente. También echaré en falta los manolitos. Cada cierto tiempo llegabas a la oficina y al abrir el ordenador veías un mensaje: “manolitos en la cuarta”. Subías y algún alma caritativa había llevado mini-croissants para todos. Te alegraba el día. Yo llevé tortilla de patatas alguna vez y a la preparación les llevé manolitos también una vez. También le tenía aprecio a una tienda de empanadas argentinas que había cerca de donde vivía. Siempre que veo la marca en otro lugar los compro. Esos pequeños detalles sí los echaré en falta. Todo lo demás de Madrid, por mí puede quedarse en Madrid. Con esto no quiero decir que Granada sea perfecta. Mi primer piso fue un desastre. Olía siempre a porro. Una barbaridad. Y no te sentías seguro. Una noche cuando aún no me conocía la zona me desvié y casi acabo en el Almanjáyar. Lo pasé mal. Una pandilla delante, otra en la acera de enfrente, y todos andando hacia ti. Media vuelta y para casa. Seis meses más tarde me mudaría a la otra punta de Granada. El cambio se nota y se agradece. Mientras aún vivía en aquella zona me tocó adaptarme a las circunstancias, entre ellas, el trayecto al trabajo de 40 minutos. Lejos de verlo como un problema lo vi como una oportunidad. 23 libros me leí ese año gracias a esos 80 minutos de viaje diario. Me ayudó a construir un hábito de lectura que aún perdura. Por contra, mantener el hábito del gimnasio fue todo un reto. En Madrid era fácil porque estaba a cinco minutos. En Granada en cambio no hay apenas gimnasios. Tocó adaptarse. Me compré unas pesas, un banco y una barra. No hace falta más. Con eso y disciplina te mantienes en forma fácilmente. O te lesionas por falta de seguridad, por suerte, no sucedió ningún accidente que lamentar. Después llegó abril. Dicen que hacen falta dos primaveras para superar un duelo. En abril de 2024 se cumplían ya dos años del accidente de Marc Herault. La primera muerte siempre te deja marcado, las siguientes duelen, pero duelen diferente. La muerte de Marc aún la recuerdo como si fuera ayer. Esa cara del que te da la noticia es inolvidable. Cada vez que la vuelvo a ver no necesito que me digan nada, lo sé al instante. Ese tono de voz apagado es tan distinto de cualquier otra conversación, tan característico. No te queda lugar a duda que lo que te dice es verdad por más que te niegues a aceptarlo. Tu amigo se ha ido, y se ha ido para siempre. Dos años hacían ya de todo eso y se podría decir que, efectivamente, dos primaveras son suficientes. La semana santa de 2024 se sintió como otra cualquiera, el dolor ya no estaba. Solo perduraba el recuerdo alegre de la persona que ya no está. Los que nos quedamos aquí tenemos que seguir, se lo debemos a los que ya no están. Su partida no es excusa para achantarnos aunque igualmente es díficil seguir adelante. Como dice la letra de la canción de Xavibo que había grafiteada cerca de mi piso: “que seguir hacia adelante se sienta como traicionarte te juro que no es justo”. Ese mismo abril tuve la oportunidad de participar en los Data Days. Fui a dar un seminario de mi trabajo en El Ranchito y pude coincidir con otros ponentes muy interesantes. La asociación de estudiantes de algún modo había conseguido atraer a trabajadores de DeepMind, de la ONU, de hospitales, y a mí. Me sorprendió el poco éxito que tuvo para la calidad de los ponentes que había. Supongo que la fecha estuvo mal escogida. Mayo. En Granada este mes da comienzo con las cruces, una de sus principales fiestas y mi segunda excusa para intentar quedar con Ana. Igual debí haber escuchado al universo. Si algo no sale, es mejor dejarlo estar. Aquellas cruces intenté quedar con ella pero un malentendido nos llevó a no encontrarnos. Posteriormente, ese mismo mes sucedería la noche más surrealista de mi vida hasta el día de hoy. El CEO de Beta venía de viaje a Granada y quería conocerme en persona. Ya de por sí me sorprendía que no me odiara por como jugué con ellos. Pero no solo es que no me odiara sino que aquella noche me invitó a vaciar las reservas de vino de Los Poetas Andaluces. Aunque esta parte podría considerarse normal. Alguien rico que suelta pasta. Esperable. Lo surrealista vendría a consecuencia del vino. La noche fue avanzando y tornándose divertida por momentos. Ojalá no hubiera tenido que trabajar el día siguiente. Quien pudo haber sido mi jefe vino con una compañera suya. Soltera y 40 años. Te puedes imaginar el percal. Una noche para el recuerdo sin duda. El corpus en Granada no decepciona. Junio. Feria de Alcalá. Tercer intento de quedar con Ana. A la tercera va la vencida, dicen, pero esta vez no. Esta vez no fue un malentendido, fue cobardía. Los dos estuvimos a menos de cincuenta metros pero no nos juntamos. Ana es de discoteca y mis amigos son de botellón. No me atreví a lanzarme yo solo a los lobos (sus amigas). Un error del que aprendería. Más tarde, en la fiesta de la Abuela en Santana sí que abandonaría a mis amigos para ir a verla. Maravilloso reencuentro que poco duraría. Sus amigas no paraban de alejarla de mí. Lo cual, en retrospectiva, era una señal que tendría que haber escuchado. Esa noche también fue para el recuerdo. Nos tropezamos por la fiesta hasta en tres ocasiones. Bailamos juntos, me robó un beso, hablamos. Balance positivo. El final de nuevo volvió a ser surrealista. Una de sus amigas se puso a hablar de su adicción a la cocaína. No recuerdo si era la que se iba a casar por error o era otra. La cuestión es que a mí me daba igual porque Ana tendía sus brazos en mi pecho. Mi piel aún no se recupera de sus caricias, cómo las añoro. Me faltó tiempo para planear mi próximo intento para quedar con ella: la fiesta de los fuegos en agosto. Este plan también falló, pero antes de explicar por qué, hay que situarnos. En julio me mudé más cerca de mi trabajo. También en ese mes despiden a mi padre. Fue un mes de mucho estrés por diversos motivos. Tuve la suerte de poder invitar a mis amigos de Barcelona a pasar un fin de semana estupendo aquí, en Granada. Pero eso no compensó lo que sucedió después. El despido, el estrés, la falta de afecto por su familia y la medicación hicieron de la vida de mi padre un infierno esos días. El 11 agosto celebramos el cumpleaños de mi padre –que es el 13 de agosto– y la cara que mi padre tenía aquel entonces es otra de esas cara que nunca olvidaré. Sus labios sonreían pero sus ojos contaban una historia diferente. Una mirada perdida, cargada de dolor y sin fuerzas. Esos ojos te abrían las puertas de su alma, el alma de quien yo tanto quería, que se mostraba rendida a las circunstancias. Todos tenemos un límite. Un límite al sufrimiento que podemos aguantar, al estrés, al exceso de trabajo, a los abusos, a las injusticias… Y a sus 56 años él llegó al suyo. Ese domingo 11 de agosto le abrazaría sin saber que sería la última vez que sentiría sus brazos rodeando mi cuerpo. La última vez que oiría su voz y sentiría su calor. Dos días más tarde mi padre pondría fin a su sufrimiento. El martes por la mañana recibo una llamada de mi madre para ir a Alcalá a ingresar a mi padre en el hospital. Cuando llego y abro la puerta su cara lo dice todo. La misma cara que dos años atrás vería en mis amigos. Sobraban las palabras. Sobraba todo. Mi mundo se desmoronó por completo en ese mismo instante sin necesidad de que la guardia civil confirmara lo ocurrido. Seis años me costó conseguir encontrar trabajo en Granada para volver a mi tierra. Seis años para volver a tener a mis padres a un tiro de piedra. Seis años alejado de mi cultura. Seis años de estudiar y trabajar. Seis años luchando. Seis años. Mi padre era quien más soñaba con verme volver a mi tierra y por suerte pudo verme conseguir ese logro. Él se fue sabiendo que había hecho un buen trabajo, que ya no lo necesitaba. Llegué a tiempo para acompañarle en sus últimos días. Un martes 13 vino y un martes 13 se fue. Siempre me sentiré afortunado de haber podido compartir parte de mi vida contigo, de aprender de ti. Aunque ya no estés, te tengo presente cada día, en cada decisión. Tu sacrificio no habrá sido en vano, te prometo que mi vida no se para aquí. Te fuiste para protegernos tomando una decisión difícil que yo no supe tomar. Tuviste fuerzas para actuar cuando yo no tenía fuerzas para decidir. Espero que desde el cielo nos veas a mamá y a mí y te alegres de ver que todo salió bien, que hemos salido adelante. Mamá aún te llora y yo aún sigo cabreado contigo. Pero nos salvaste a los dos con tu decisión. Actuaste de muro de contención para evitar que dónde hoy hay un nicho, hubiera tres. Ojalá algún día ser tan fuerte como tú. Así que eso fue lo que destrozó mis planes. Los fuegos eran el 15 de agosto, pero después del funeral pocas ganas me quedaban de ver a nadie. Días más tarde tenía organizado un viaje con mis amigos de Barcelona el cual dudé si ir pero al final fui. El viaje me sirvió para despejarme aunque cada noche necesitaba estar a solas para llorar. Aún a día de hoy muchas noches necesito llorar su muerte. De vez en cuando sueño con él, que hablamos todo lo que no pudimos hablar y aunque sé que es un sueño quiero seguir ahí, en el sueño, y al despertarme las lágrimas vuelven a brotar porque es como perderlo otra vez. Por suerte Ana pudo acercarse a darme un abrazo antes del viaje. Tampoco sabía que ese abrazo sería el último. Ana sigue aún viva, pero desde que mi padre falleció nada volvió a ser igual. Meses más tarde por teléfono empezaríamos formalmente una relación que duraría apenas unos días. El valor que me faltó en agosto lo tuve en septiembre. Ella no quería compromiso, quería diversión. Yo ya había planeado una vida con ella pero ella solo planeaba su próxima noche. Podrían ser unos meses divertidos pero no podría formar una familia con ella, así que con todo el dolor del mundo tuve que cortar por lo sano ya que ella no se atrevía. Agosto y septiembre fueron meses de tragedias. A finales de agosto otro amigo mío, Guiem, falleció en un accidente de barco. La voz de Pau por teléfono era como la de Bernat años atrás. Cambia tanto el tono al expresar el dolor de una pérdida. Es como hablar con una persona ausente, perdida. En septiembre subí a Barcelona a consolar, o al menos acompañar a los compañeros de Guiem en esos momentos difíciles. Guiem era un chico muy alegre, su partida se notó mucho en el grupo. Esa felicidad se ha evaporado por completo para esos chicos igual que se evaporó para nosotros en 2022. Fue a la vuelta cuando descubrí que mi historia de amor llegaba también a su fin. El fin de semana que subí a Barcelona era la feria de mi pueblo y ella aprovechó para recordar viejos momentos. Se volvió a encontrar con su ex y afloraron sentimientos. Sentimientos prohibidos porque él no quiere nada con ella, pero sentimientos que no puedo ignorar. Al principio pensé que no era tan malo y que podía vivir con ello. Pero después llegué a la conclusión de que no quería ser el segundo plato. Tras tanta tragedia, ya pocas fuerzas me quedaban para nada. Pero la vida sigue y hay que seguir con ella. Volví al trabajo y empecé un máster de matemáticas puras. Era mi momento de demostrarme que yo también podía hacer deep learning teórico sin importar que no me cogieran en el laboratorio de Mihaela. Mi TFM lo hice de topología algebraica aplicada a la interpretabilidad y explicabilidad de modelos de deep learning. No descubrí ningún resultado pero aprendí bastante. El objetivo del máster realmente era decidir si quería seguir con el doctorado. Tenía mis dudas y un máster era perfecto para reflexionar durante un año si quería esa vida. Con 18 años uno se embarca en una carrera sin saber el sacrificio que le espera, no quería embarcarme en un doctorado a ciegas también. Antes de comprometerme de nuevo a ese nivel, necesitaba probar una pequeña dosis del sufrimiento que me esperaría. Así pues, tras un año estudiando y trabajando a tiempo completo, llegué a la conclusión de que hacer un doctorado mientras trabajaba no era viable. Esa vida no es para mí. Mihaela hizo el doctorado mientras trabajaba y nos advertía que no recordaba nada de su juventud. Por lo que si quería hacer el doctorado tendría que dejar el trabajo y eso iba en contra de mis planes iniciales. No es que el máster haya sido tiempo perdido porque me ha servido para aclarar mis ideas, algo muy valioso en estos tiempos. A veces necesitas experimentar, solo pensando no puedes tomar según qué decisiones importantes. Antes de empezar el máster me veía muy confiado de que podría trabajar y hacer el doctorado a la vez, pero después del máster, me ha quedado claro que no. Tras los exámenes del primer cuatrimestre me pasé dos semanas enfermo. Vómitos, diárreas, fiebres… El esfuerzo es sobrehumano y tiene un coste que no me apetece pagar para que encima nadie lo valore. Se podría decir por tanto que mi carrera como investigador público se da por terminada, al menos por ahora. Con el máster no me quedaría mucho tiempo libre por lo que mi vida social se vería reducida a mínimos. Aún así todavía quedaba en 2024 otra noche para el recuerdo. Sería la noche de Halloween. Surrealista como las demás. Ahí fue cuando descubrí que existen mujeres despreciables que es mejor evitar. Su conducta era más propia de una depredadora sexual que de una mujer borracha. Mis amigos estaban encantados de poder hacer con ella lo que quisieran pero yo no tolero que el desorden se meta en nuestras vidas. Hay ciertos límites morales que creo deben respetarse y ella los superó con creces. Encerrarte en una habitación para intentar liarte con un hombre que tiene novia desde hace seis años es de no tener vergüenza. Más aún cuando él no quería e intentaba escapar de ella. Cruzó límites que no deberían cruzarse. Si dejas que el desorden entre en tu vida al final todo termina desmoronándose. El más mínimo ápice de desorden debe ser erradicado a la mayor brevedad posible. Es muy difícil tener estabilidad y seguridad, y hay que proteger esta tranquilidad a toda costa. Dejé bastante claro que esa mujer no debería volver a estar cerca nuestra, que no la invitaran a nada más. Por exagerada que pareciera mi reacción con respecto a esta chica yo considero que está justificada. Mejor prevenir que curar. El último evento reseñable de mi vida este año sucedería en noviembre. Por estas fechas se celebraba la cuarta edición de la datathon fme. Un año antes ya escribí en otro post el apego que le tengo a este tipo de eventos. Son para mí un refugio para los estudiantes que no nos dedicamos solo a emborracharnos y que nos gusta aprender. Asistí a esta edición de la datathon para ya decir adiós a mi etapa universitaria y pasar página. Esa sería la última vez que ayudaría a organizar un evento así. Han sido muchos años que he disfrutado muchísimo pero ya se empieza a notar la desconexión con los alumnos. Cuatro años es mucha diferencia a estas edades y eso hace que ya no me sienta tan identificado con ellos. Supongo que es hora de traspasar el relevo a la nueva generación, que demostró estar a la altura. En este mes de noviembre pasaría mi última noche durmiendo en el suelo de la facultad que me ha visto madurar. Vería por última vez a los alumnos apilar vasos de plástico haciendo una torre, tocando el piano, comiendo yogur nocturno o bebiendo monster para activarse. En mi portátil llevo pegado un sticker en recuerdo de estos eventos que tanta vida me han dado. También me entristece pensar que las nuevas generaciones nunca sabrán lo que era una auténtica hackaton. Ya todo se hace con la ayuda de ChatGPT. Lejos quedan los intentos de leerte la documentación de un framework durante horas para finalmente usar un bot de telegram. Ahora el frontend es automático. Esto hace que los proyectos sean más impresionantes que años anteriores, con saltos de calidad notables. Nosotros no éramos tan pros. Se nota como la tecnología avanza y empodera a las nuevas generaciones. Es un orgullo ver de lo que son capaces hoy en día con las nuevas tecnologías, pero también me da nostalgia de ver que nada será cómo antes. Estamos llegando al final. Un final digno para un año que marcó el trascurso de mi vida. Un final que me cruzó con dos viejas amistades y cumplió un viejo deseo. Con la llegada del final de año, en navidad, el dolor de la pérdida de mi padre era demasiado grande tanto para mi madre como para mí. Celebrar las navidades en casa no sería agradable. Por eso no lo hicimos. Tomamos la disparatada idea de ir a Barcelona por Navidad, rompiendo así con una larga tradición. Mis padres siempre solían invitarme a comer a algún buen restaurante cuando venían a Barcelona. Hemos estado en el Salamanca, en el Siete Portes, en el Abac, en Can Saia y en otros múltiples locales. Sin embargo, nos quedaba uno por visitar: el Botafumeiro. Hacía algunos años yo había visto a Ibai comer allí y me entraron ganas de ir. Convencí a mis padres para ir pero obvié el detalle de los precios así que tan rápido como entramos nos salimos. Sin embargo, ahora corren otros tiempos y nos podemos permitir celebrar la Navidad ahí. Aprovechando el viaje quedé también con mi grupo de amigos de la universidad para cenar. Lo curioso sería que mientras esperaba para la cena me toparía con Javier, un viejo compañero a quien hacía tiempo que no veía. Y después de cenar, volviendo a casa me tropezaría con Aleix, otro viejo compañero. No había nadie más en la calle a esas horas, pero ahí estaba él. Desde nuestro incidente en primero de carrera nos habíamos distanciado, pero el universo quiso que nos encontráramos justo ahí justo en ese momento. Resulta que él estaba en el mismo proceso que yo al comenzar el año. Tomando decisiones difíciles que marcarían sus próximos cinco años. Un encuentro bastante poético, fue como mirarme a mí un año atrás. Como has podido comprobar mi 2024 fue una montaña rusa, plagado de éxitos y tragedias. Un año que ha forjado quién soy ahora, cómo pienso y qué quiero en la vida. 2024 fue un año decisivo que marcó el rumbo de los años posteriores, tanto laboral como personalmente. Que estuvo plagado de aventuras y emociones fuertes que difícilmente podré olvidar. Sin duda, un digno capítulo más que añadir en el libro de mi vida. "
    }, {
    "id": 8,
    "url": "https://granadata.art/vibe-coding/",
    "title": "Vibe-coding, an informed guide.",
    "body": "2025/07/27 - At the beginning of this year 2025, Andrej Karpathy coined the term: “vibe-coding”. And since then we have seen the AI discourse become even less serious over time. The typical FOMO discourse (Fear Of Missing Out) continues to say that if you are not using AI you are missing out, that AI will replace us all, that what previously took years now takes hours and so on. I am a big fan of AI and want it to succeed. My side is definitely with e/acc, but I also want to throw a bit of clarity to all this madness. AI is progress and cannot be stopped, but we must think critically and analyse exactly what is the point in which we found ourselves right now. Vibe-coding is being monetised which means the PRs (Public Relations) are everywhere trying to convince us to buy their products. Replit is one of those tools that sells itself as a way to build apps just with natural language. It is quite impressive and I have tested and created one webpage entirely with it. Nevertheless, Replit is also responsible for deleting another company’s codebase. As I said I want to bring some light to the discourse and this post is going to be an informed analysis on how to do vibe-coding properly, what are its risks and when is it worth it. What the data shows: Let’s start with what we already know. AI is being here for a while and studies are starting to appear. The first study that got to the news was about Copilot. They compared the time it took developers to complete tasks with and without Copilot. They measured improvements in the range of 26% and 73%. This study was optimistic since it was coming from Microsoft and it was about one of its products which may raise some eyebrows. But even this optimistic value (73%) is not “what took months takes days”. That is one of the premises that should be banned from the current discourse. Another independent study from Stanford found that improvement to be 14%, with some productivity metrics going up to a 22. 2% improvement, which is a more conservative value. That study started to show that the improvement depended highly on your level of expertise, which is something quite reasonable. Before somebody says that those articles are from 2023, those results have been replicated in 2024 (~50%) and 2025 (~25%). This clearly means that there is some gain, but there is high variability. The next question is when does this gain happen so that we don’t use AI when it is pointless. One study advancing our knowledge in that matter was recently published by METR. This study showed a 19% decrease in productivity in one very specific case: experienced developers of complex projects that knew perfectly all the details of their projects. It also found that developers wrongly believed they were being more productive, something I think is caused by the current discourse. We have been told for a few years that AI is making us more productive so we feel like that even when that is not the case. In the Copilot study developers estimated their gains properly, they felt more productive and were more productive. But here is the opposite. The subjective perception is challenged by the objective measurements. In a curious case of Stigler’s Law, this was not the first study in that direction even though it was the first to reach the public discourse. The 2024 DORA report reached the conclusion that AI is good for “code quality, documentation, and review processes”. However, it was detrimental for “software delivery performance” (aka teams are slower). Those two reports are all I could find so far using Google and Perplexity that provide negative results, but they provide a clear enough picture into what are the correct and incorrect ways to use AI for software development. What can we do now?: In a few years I will come to this post and the advice I give is going to be obsolete, but in the current state of affairs what I will describe below are probably good practices when using AI. The main issue with our current systems is the long context. The METR study pointed to complex systems as a place where AI is falling behind, why? AI can perfectly follow orders when you tell it to write a function and has good enough reasoning capabilities for most of the day to day problems, what is happening? The answer is long context rot. Current systems are technically capable of managing millions of tokens which is something my professors at university thought impossible. That context is enough to read entire books and answer any question about any page of the book. Not even a human can do that. Therefore, again, what is happening? What is happening is that the performance is decaying as Lecun predicted, he was initially wrong and correctly criticised but in the end, and for completely unrelated reasons, he seems to be right, once again. Complex systems require a massive amount of knowledge before introducing a new change. Otherwise another feature may stop working, just like in the meme but for real. Long context have been a problem for a while. Before long context models we had RAG systems that are still being developed today. With the arrival of the agentic models a new protocol was created: MCP, which enabled the usage of tools and gave the model access to different contexts. This technology is still in development and will solve the complexity issue in the future but for now, we are stuck with the complexity issue. Now that we know the main limitation of current systems, what could be a good workflow to improve productivity? We have a technology that is good in some places and bad in others. The correct approach is neither to discard nor to glorify it. The correct approach is to design a workflow to mitigate risks and increase productivity. This part is a bit technical, so if you just want to know when the usage of this technology is financially justified skip it and go to the next one. The workflow I propose is based on my current experience. I have tried many tools, almost all the LLMs that are in the market, I have worked in production systems, big codebases with no documentation, and also have my own personal projects. Apart from my experience, I also have the habit of discussing software development methodologies with my coworkers. The main concern of senior developers that I have met when using this technology is maintanibility and scalability. Another constraint is that a senior developer must know what every single line of code does, otherwise you just have a functional product that nobody can fix when it breaks. So, how can we reconcile the senior developer job with that of the machine? The workflow in itself is quite simple, but the demon is in the details. The workflow that has worked most for me is the following. The first time you arrive at a new codebase you know nothing. It does not matter how many years of experience you have, you start from zero. Therefore, before you are productive you need to get familiar with the codebase. Here, AI is your best ally. Instead of looking at the code and thinking hard what is happening you can ask the bot. The quintessential example is regex. Suppose you find this regex: r'&lt;a[^&gt;]*index\s*=\s*[ \']?(\d+)[ \']?[^&gt;]*&gt;(?=. *?&lt;p[^&gt;]*index\s*=\s*[ \']?\1[ \']?[^&gt;]*&gt;)(. *?)&lt;/a&gt;'A simple question to ChatGPT tells you that it is looking for html a tags with a child p tag that has the same index. Some engineer may have needed to find duplicated html indices to fix some bug and did not have time to document the hacky solution. A few seconds of LLM inference and you can continue understanding the code. The next step after you have familiarized yourself with the codebase is to identify patterns. LLMs are matching patterns most of the time, it is their best quality. In day to day work there are many boring tasks that could be easily automatized. One example is generating boilerplate code. LLMs are faster than us at typing. When I learnt to type, to obtain the certificate you needed to type 300 characters per minute, or around 60 words per minute (wps). Some pages show the average is 40 wps with developers probably in the 70-80 wps side. Evan Chen, which was famous for typing fast is around 150 wps. But that is peanuts compared to LLMs, ChatGPT API throughput is 55 tokens per seconds, which is 495 wps. And that is one of the slowest LLMs out there. Groq systems are way faster and some even talk about the velocity in pages per second. If you can tell the AI to code a repetitive task that is very long, you can save a lot of time simply because it is faster at typing than you. The difficult part is deciding what processes are suitable for this. Once the repetitive patterns are identified the next step is to design skeleton code. Instead of coding everything, you first code the skeleton. Create some main function that calls several other functions and then let the AI implement those other functions. For this task any Cursor-like tool is definitely the best tool, like windsurf. The key is to reduce the complexity. You know the code and you know the solution, split it into simpler tasks and when you reach the level of complexity that current LLMs can handle, you just let them do it. Let’s put an example. I recently had a project in which I had to work with OpenCV CUDA. Many of you may be familiar with OpenCV but I doubt that you know all the details of its CUDA version. For instance, cv::Mat is normally continuous but cv::cuda::GpuMat is almost never continuous. That piece of knowledge is hidden in a note in the API reference. LLMs kind of helped to find that, for instance. However, documentation ends there. How to access images pixel values? How to pass cv::cuda::GpuMat to a kernel? That is even more hidden and LLMs are of no help finding that information because it does not exist. You need to go to the code to answer those questions. OpenCV code has a lot of templates and C++ template language is known for being notoriously difficult to interpret. But, after some back and forth with LLMs I finally understood that passing a cv::cuda::GpuMat to a kernel and accessing it was as simple as putting cv::cuda::PtrStep&lt;T&gt; in the kernel signature and doing img(y, x). This is the first phase, using models to understand and familiarise with the code. Then, I designed a common structure for all the kernels, fixing grid dimensions, fixing the error checking macros and other parts that were going to be repetitive. For instance, after calling a kernel function I needed to add CUDA_CHECK(cudaDeviceSynchronize()); CUDA_CHECK(cudaGetLastError()); for error checking. That is the same everywhere. Repetitive and boring, so if I can make the AI write it for me it is going to save me time. After I designed the skeleton for all the kernels and all the kernel wrappers I finally started using AI to code. With a simple system prompt I put some limits to what the code could be and the best practices that the bot should follow. Whenever the function names were descriptive enough and the task was simple enough the code was working on the first try. Another critical aspect to make all of this work is to do test driven development. That makes vibe-coding way more effective. After you familiarised yourself with the code, you identified patterns and created the roadmap for the AI, the final step is to review the code. There are parts of the review process that can be automatised also. The DORA report that was one of the critical ones still pointed that AI was positive in this step of the development process. In CUDA code you normally check these things: memory coalescing, thread divergence, bank conflicts in shared memory, occupancy and race conditions. This is anecdotical evidence, but AI once detected a very obscure race condition I had in some code I did myself without AI help. Race conditions are the kind of needle in hay problems AI is good for. Memory coalescing is also a very LLM-friendly task because you just need to check that indices are ordered correctly. That is the kind of mechanical work that AI is good for and humans are not. Automating that leaves room to decide what the kernel should do, which is where my creativity is needed. Apart from that, before merging the code to the main branch, LLMs can perform the first review of your code before other developers get there. It can detect possible deadlocks, memory leakages or simply parts of the code that do not follow the style guidelines. Of course, you need a real human making the final review, but putting an AI in the middle can reduce the workload in the human reviewer. Is it worth it?: Finally let’s put some numbers into the financial side of the equation. We have talked about the performance benefits which range from the optimistic 73% of Microsoft to the more realistic 14% of Stanford. Now let’s talk about the costs. To know how much AI is costing we need to have an estimate of how many tokens per month does a developer consume. There is no conclusive data on this, mainly because the usage is not extended enough. I found this blogpost that estimates the consumption to be 1 million tokens per month per developer. That seems like a lot but remember that codebases are big, and the context required to answer the question is what really consumes tokens, so it could be a reasonable estimate. And what is the cost per token of the latest models? As per 27th of July of 2025 the pricing in the OpenAI API is 8$ / 1M tokens for the output of the smartest model. That amounts to a total of 96$ per developer per year. I don’t know if the price has VAT included, but we can round to 150$ to account for that and have a pessimistic estimate. Is that worth it? Well, unless you pay your developers less than 2000$ a year, then yes. Even with the 14% estimate you are better off using AI. However, you need to be careful to not use AI in unneeded places, or you could end up worse. Conclusion: Software development, when done well, is hard. Current systems have come a long way and will continue to improve. But in the current state of affairs their usage needs to be carefully monitored. There are places where it boosts productivity modestly and there are cases when it can be detrimental. Until AI is good enough to replace us all, we need to be responsible and make decisions based on the data and not on marketing claims. "
    }, {
    "id": 9,
    "url": "https://granadata.art/gan-optimality-revisited/",
    "title": "GAN optimality proof revisited",
    "body": "2025/07/20 - Three years have passed since I published two posts related to the original formulation of the Generative Adversarial Networks (GANs). Three very crazy years in which the state of the art (SOTA) of generative models has surpassed any kind of expectation. Specially in video models that have now passed Tik-Tok quality by of large, they are still below TV quality and, of course, below cinema quality, but it is a lot more than what I predicted back in the days. The new SOTA models are all based on a different formulation from GANs, they use diffusion models. I am not going to write about diffusion models since that topic is already well covered in this post together with a continuation for video diffusion models. And for those of you who want to have deeper mathematical intuition there is this wonderful paper with hundreds of formulas and derivations that gets as deep as you can get into the foundations of diffusion models. However, this post is again about GANs, more concretely, about the optimality proof. Three years ago I was not convinced by Goodfellow’s proof of this theorem: The global minimum of $C(G) = \max_{D}V(G,D)$ is reached if and only if $p_{data}\equiv p_g$. Also, that minimum value is $-\log(4)$. Just to recap, this theorem is saying that the solution of the min-max problem is reached when we have a perfect generator. And the step that bothered me was in an intermediate proposition: If $G$ is fixed, the optimal $D$ is $$ D^*(\textbf{x}) = \frac{p_{data}(\textbf{x})}{p_{data}(\textbf{x}) + p_{g}(\textbf{x})} $$This proposition computed the optimal discriminator given the generator. And in the limit it results in a stupid discriminator who always predicts 50-50 probability of being real and fake. I already showed my doubts with respect to one of the steps regarding the proof of this proposition. In fact, I wrote a post back in 2022 asking if anyone knew of a way to overcome what seemed to be a minor error. Surprisingly, after three years somebody has taken the time to provide a more convincing proof of that tiny step and also highlighted an omission in the Goodfellow paper. So, without further ado, let’s see what I was missing and what Goodfellow was missing. First, let’s remember what was the proof. It involves two steps: a change of variable and a maximization problem. The change of variable is a bit technical and you can give a look to an sketch of the details and the theorems involved in my previous post. It reduced the problem to finding the maximum of this integral $$ \int_{\textbf{x}} p_{data}(\textbf{x})\log(D(\textbf{x})) + p_g(\textbf{x})\log(1-D(\textbf{x})) d\textbf{x} $$The original argument to maximize this integral read, quote: “For any $(a,b) \in \mathbb{R} - {(0,0)}$, the function $y \rightarrow a \log (y) + b \log (1-y)$ achieves its maximum in $[0,1]$ at $\frac{a}{a+b}$. ” That argument for me was insufficiently explained, but it is correct (for the most part). The argument is basically constructing a function that at each point is the maximum; therefore, when integrating, the result is the maximum possible. I initially understood the argument as simply optimizing the integrand which seemed incorrect to me because $(a,b)$ here are not constant. But later on, Graham Pulford pointed out that this is calculus of variations. So here it goes what I think is a more rigorous rewrite of that step. We define the lagrangian to be $$ \mathcal{L}(\textbf{x}, D, D') = p_{data}(\textbf{x})\log(D(\textbf{x})) + p_g(\textbf{x})\log(1-D(\textbf{x})) $$Thus, the optimum (if it exists) must satisfy the Euler-Lagrange equations: $$ \frac{\partial \mathcal{L}}{\partial D} - \frac{d}{d\textbf{x}}\frac{\partial \mathcal{L}}{\partial D'} = 0 $$It looks like I am treating multivariable as single variable but I am just using this notation for simplicity: $\frac{d}{d\textbf{x}} = \sum_i \frac{\partial}{\partial x_i}$. The Euler-Lagrange equations is only one equation in fact because there is no derivative in the lagrangian. Therefore we just need to solve for $\frac{\partial \mathcal{L}}{\partial D} = 0$. With some patience that leads to the desired result: $$ \begin{align*}&amp;\frac{\partial \mathcal{L}}{\partial D} = \frac{p_{data}(\textbf{x})}{D} - \frac{p_g(\textbf{x})}{1-D} = 0\\\Rightarrow&amp; \frac{p_{data}(\textbf{x})}{p_g(\textbf{x})} = \frac{D}{1-D} = \frac{1}{1-D}-1\\\Rightarrow&amp; \frac{p_{data}(\textbf{x}) + p_g(\textbf{x})}{p_g(\textbf{x})} = \frac{1}{1-D}\\\Rightarrow&amp; D = \frac{p_{data}(\textbf{x})}{p_{data}(\textbf{x}) + p_g(\textbf{x})}\end{align*}$$But there is a catch. This argument only works for sufficiently smooth lagrangians which is why intuitive arguments should be made rigorous. Taking the optimum at each point and integrating only returns the functional optimum if the functions are not pathologic, which in this context means twice differentiable. To solve the Euler-Lagrange you only need the integrand to be once differentiable, but to prove it is a maximum you need to look into the second derivative and see it is negative, which is, in fact, the case: $$ \frac{\partial^2 \mathcal{L}}{\partial D^2} = -\left(\frac{p_{data}(\textbf{x})}{D} + \frac{p_g(\textbf{x})}{1-D}\right) &lt; 0 $$And what happens if you do not have a sufficiently smooth integrand? In that case you no longer have such a simple way to find the optimal discriminator. But that surely only happens in the mind of the perturbed mathematicians, right? Wrong. That assumption can be broken very easily. Graham Pulford showed that having $\dim(\textbf{z}) &lt; \dim(\textbf{x})$ is enough to break the smoothness of the lagrangian. This comes from the first step of the proof: the change of variable. When reducing the dimensionality of a probability distribution function the behaviour can be pathological, breaking the smoothness. In that article I linked, he constructed several concrete examples showing such pathological behaviour. That same author went a step further and constructed a discriminator that was optimal but was nowhere equal to the $1/2$ that Goodfellow promised. Curiously enough, that discriminator produces $1$ almost everywhere and $0$ in a set of measure zero, which means it is also a stupid discriminator that is overconfident in saying that everything is fake. As expected, answering one question creates new ones. In this case I am left with the doubt of whether or not the optimal discriminator is always independent with respect to the generator. Maybe in another three years that is also solved? Apart from the theoretical insights, all of these new results bring more light to the practical side of the GANs. Training a GAN was always an unstable process that required many tweaks to make it work. We now know the reason for that instability. The initial algorithm for training GANs was an iterative process that tuned discriminator and generator in alternate processes. The tuning method for the discriminator involved that formula that appeared in the proposition we mentioned before. That formula we now know has no guarantees of being always correct. Since the issue was with the smoothness, many tweaks for making the learning more stable did that in different ways. Either with regularization or controlling the rate of change. But, in the end, every method was just a patch to a fundamental flaw in the formulation. This may explain why everybody has moved to diffusion models. Their formulation is more robust and its training is better understood. "
    }, {
    "id": 10,
    "url": "https://granadata.art/fast_slow/",
    "title": "Pensar rápido, pensar despacio",
    "body": "2024/07/16 - Uno de enero, todo el mundo se propone cambiar algún hábito. Pocos los consiguen. En mi caso no me propuse nada pero surgió una idea que posteriormente se transformó en costumbre. Ese mismo día le pregunté a un amigo mío que me recomendara un libro para leer. Llevo años sin leer en serio y como no sabía por dónde empezar dejé que esa decisión la tomaran por mi. La recomendación fue “Momentos estelares de la humanidad” de Stefan Zweig. Tardé más de un mes en comprarlo y otro mes en leerlo. Cuando acabé llegué a la conclusión de que no era de mi estilo. No obstante, volví a probar suerte. La siguiente recomendación fue “El señor de las moscas” de William Golding. Este sí fue de mi agrado. Y a partir de ahí, surgió un hábito. Este año estoy viajando a través de los gustos literarios de mis amigos y amigas. Libro tras libro. Leer el libro favorito de alguien te revela mucho sobre su forma de ser. Y también te abre la mente a lecturas que de normal rechazarías. Requiere disciplina. De momento llevo 9 ejemplares añadidos a mi colección siendo el último del que vengo a hablar hoy. El libro titulado “Pensar rápido, pensar despacio” fue escrito por Daniel Kahneman, un psicólogo con un premio Nobel de economía. El autor reúne en esas 672 páginas toda su carrera profesional. Es una colección extensa de experimentos obtenida tras varias décadas de trabajo. Un libro de ensayo que expone una tesis de una forma muy similar a como Susan Cain expresaba la suya en “El poder de los introvertidos”. Varios experimentos me he encontrado que se hallaban en ambos libros. Sin embargo, cada autor los ve con sus propias gafas. Yo también he sacado mis propias conclusiones, pero creo que es más interesante exponer los experimentos desprovistos de connotaciones y que cada cual aprenda lo que quiera de ellos. Además de los experimentos también explicaré algunos conceptos que trata el libro y añadiré algunos detalles que me parecen relevantes. Antes de seguir, vea el siguiente vídeo a pantalla completa:           Your browser does not support the video tag.   ¿Vió al gorila? La mitad de las personas no lo ven. Cuando focalizas tu atención en una tarea, olvidas las demás. Ese era el primer experimento llamativo del libro. Casi todos los experimentos tienen un efecto similar. Inicialmente nada parece sorprender. Todo es intuitivo y obvio. Pero cuando te explican el trasfondo te das cuenta de lo sesgada que es la percepción humana, hasta el punto de no ver un gorila enorme. Posiblemente algunos de ustedes sí que vieran el gorila. Esto no es una ciencia natural, es una ciencia social. Por eso cada experimento se debe hacer con una muestra poblacional representativa. En este caso Christopher Cabris y Daniel Simons lo probaron con personas de distintas edades, culturas, educación y género. La conclusión es que el efecto es inequívocamente válido para toda la población mundial. La mitad de las personas no ven al gorila y la otra mitad sí. Parece imposible no verlo, ¿verdad? Pues igual de sorprendentes son muchos de los experimentos y resultados que expondré a continuación. Heurística de la disponibilidad: Propóngase usted dar 20 ejemplos de situaciones en las que se mostró con un carácter firme frente a la adversidad en el que demostraba su capacidad ejecutiva. ¿Ya? ¿Se siente más o menos seguro de sí mismo? En eso consistía este otro experimento. Muchas de las personas entrevistadas se sentían menos seguras si se les había pedido muchos ejemplos y en cambio, las que se les preguntó menos ejemplos se sentían más seguras. Un curioso resultado. Si doy más ejemplos de seguridad me siento menos seguro. A este efecto también se le conoce como sesgo de disponibilidad. La inseguridad viene de que los últimos ejemplos cuestan más de recordar que los primeros. Cuando nos cuesta encontrar argumentos nuestro cerebro genera dudas sobre esa postura. Este efecto se puede eliminar si a esa dificultad cognitiva se le da una explicación, como por ejemplo decirle a los participantes del estudio que les va a costar más encontrar ejemplos por el ruido de la clase de al lado. Efecto de mera exposición: Zajonc decidió publicar palabras inventadas en el periódico durante varias semanas. Después, al preguntarle a las personas qué palabras les gustaban más, las que habían visto en el periódico recibieron mayor puntuación. Todo ello a pesar de que son palabras sin significado ninguno. Igual de llamativo es este otro resultado en el que Todorov conseguía predecir el resultado de las elecciones basándose en las puntuaciones que los participantes del estudio daban a las caras de los políticos. Es remarcable porque los participantes evaluaban la competencia de la persona basándose solo en la cara y siendo estas caras de políticos de países arbitrarios sin relación alguna con los participantes. Es como si Tezanos se va a Australia y hace encuestas solo enseñando las caras de los políticos españoles a los australianos. Posiblemente acertaría más. Y en la misma línea hay otro artículo que muestra cómo los políticos guapos que aparecen más en televisión obtienen mejores resultados, simplemente por aparecer más en televisión. La mera exposición los hace recibir una mayor aprobación. Efecto halo: El autor describe el concepto de efecto halo como el hecho de asociar a una persona solo atributos positivos o solo atributos negativos. O como los autores también lo describen: “What You See Is All There Is”. Evaluamos el todo por una parte. Prueba de ello quedó reflejado en el estudio de Solomon Asch. En él se preguntaba la opinión de dos personas: 123Alan: inteligente-diligente-impulsivo-crítico-testarudo-envidiosoBen: envidioso-testarudo-crítico-impulsivo-diligente-inteligenteSeguramente te hayas percatado de que son la misma descripción. Pero recibieron valoraciones dispares, obteniendo Alan puntuaciones más favorables que Ben. Los adjetivos del principio terminan acaparando la atención y cogen más peso que los del final. Ese es el efecto halo. Básicamente es formarse un prejuicio e ignorar la información que lo contradiga. También hay que notar que si se evalúan por separado o conjuntamente la visión cambia. Cuando evalúas los dos perfiles conjuntamente es más fácil darse cuenta de su equivalencia. Esta diferencia entre evaluar conjuntamente y por separado quedó plasmada por Christopher Hsee. En su experimento preguntó a varias personas que evaluaran dos vajillas. Una tenía 24 unidades en perfecto estado y otra tenía 40 unidades con 9 de ellas rotas. Cuando veían las dos conjuntamente se valuaban a un mayor precio la de 40 porque era exactamente igual que la de 24 pero con más piezas. Sin embargo, cuando se valoraban por separado la de 40 acababa recibiendo un menor precio debido al hecho de contener platos rotos. Un efecto que Hsee denominó como “menos es más”. El propio Kahneman realizó un experimento similar. En este caso menos es más hace referencia a probabilidades. Es una proposición matemática el que la probabilidad de la intersección de dos eventos es igual o menor que la de cada evento por separado. En lenguaje llano esto nos viene a decir que es más probable encontrar un gato en la calle que encontrar un gato en la calle que a la vez sea blanco. Cada propiedad adicional reduce la probabilidad de que suceda. Pero a ojos de la gente esto no siempre es así. El experimento comenzaba describiendo a una mujer:  Linda tiene treinta y un años, es soltera, franca y muy brillante. Se especializó en filosofía. De estudiante le preocupaban mucho los asuntos de discriminación y justicia social, y también participó en manifestaciones antinucleares. Y posteriormente se pedía evaluar las probabilidades de los siguiente hechos:  Linda es profesora de primaria.  Linda trabaja en una librería y recibe clases de yoga.  Linda milita en el movimiento feminista.  Linda presta asistencia social en psiquiatría.  Linda es un miembro de la Liga de Mujeres Votantes.  Linda es cajera de un banco.  Linda es corredora de seguros.  Linda es cajera y activista del movimiento feminista. De esta lista los eventos de interés son “cajera de banco” y “cajera de banco feminista”. Las personas dieron menor probabilidad a cajera de banco. Sin embargo, toda cajera de banco feminista es a su vez cajera. Luego la probabilidad es estrictamente menor. Menos es más. Regresión a la media: Esto no es un experimento per se sino más bien un hecho estadístico igualmente remarcable. Considérese la distribución de probabilidad (PDF) de una gaussiana: Supongamos que muestreamos a partir de dicha PDF dos veces de forma independiente. Y yo ahora le digo que la primera ha salido con un valor muy a la derecha. ¿Qué es más probable? ¿Que la segunda esté a la izquierda o a la derecha de la primera? Si observamos el siguiente gráfico se ve que claramente es más probable que esté a la izquierda. El área bajo la curva es mayor. La probabilidad de que quede en el área sombreada es superior a un medio como se ve en la función de la derecha.  Esto es el efecto de regresión a la media. Un ejemplo más palpable sería la inteligencia de los hijos. Si tus hijos son extraordinariamente avispados, es muy probable que tus nietos lo sean menos. Y al revés también. O en el caso del autor, un soldado que resolvía una prueba de forma excelente solía obtener un peor resultado en la siguiente. Y quién lo hacía muy mal, lo solía hacer mejor después. Esto sucede cuando dos variables se hallan poco correlacionadas, como en los ejemplos anteriormente expuestos. Fallos médicos: Un análisis sobre errores médicos en la UCI concluyó que “los médicos que creían estar ‘completamente seguros’ de su diagnóstico ante mortem estaban equivocados en el 40 por ciento de los casos”. De forma completamente independiente, en el libro también se relata la historia de un médico del siglo XX que siempre acertaba en sus diagnósticos de tifus pero resultó que era porque tocaba las lenguas de los pacientes sin lavarse causando la propia enfermedad. Posteriormente también se muestra como los psiquiátras evaluaban de forma diferente el riesgo de violencia de un paciente en función de si la información se le presentaba en formato frecuencial o absoluto, es decir, quien leía que hay un 10% de probabilidades de violencia se sentía menos reacio a liberar a un paciente que quien leía que 10 de cada 100 pacientes similares son violentos. En concreto, el 41% denegaron el alta al paciente en el primer caso y el 21% en el segundo caso. Efecto espectador: Este es posiblemente de los estudios más antiguos del libro. John M. Darley realizó el siguiente experimento un tanto macabro. Había diversos participantes cada uno en una salita aislada con un micrófono en su interior. Cada persona solo podía hablar cuando era su turno. En una de las intervenciones el interlocutor mencionaba su inclinación a sufrir ataques de ansiedad. Poco después, el actor que interpretaba ese papel fingía sufrir uno de esos ataques y pedía ayuda a la vez que parecía ahogarse. En medio de su sufrimiento su micrófono se corta y se da pie al siguiente participante a hablar. ¿Cuántos creéis que pidieron ayuda? De los 15 participantes, 4 pidieron ayuda inmediatamente, seis no salieron de la cabina y cinco salieron algo después de que el micrófono se cortara. Este efecto muestra que cuando hay otras personas alrededor, nos sentimos menos proclives a actuar frente a emergencias. Ahora ya sabe que una primera estimación de la probabilidad de ayuda es del 27% en casos de emergencia (4 de 15). Dado ese conocimiento, si se le presentasen otros 15 individuos y se le pidiera que identificara quién saldrá a ayudar y quién no, ¿acertaría el porcentaje? Presentado así, lo más lógico seguramente sea predecir de nuevo que solo 4 irán raudos a salvar al afectado. Pero otro estudio mostró que conocer la tasa base no afecta en las predicciones de la población. Incluso aún sabiendo que un evento tiene una determinada probabilidad, si nuestra creencia es contraria a ese hecho, las predicciones se alejarán de esa probabilidad. Algo similar sucede con la inversión en bolsa. El consejo financiero más básico es aguantar tu posición y esperar a que el mercado responda. Históricamente el precio de las acciones ha subido alrededor de un 10%. Es solo cuestión de esperar. Pero muchas personas creen tener información privilegiada del mercado que les permitirá sacar ventaja de las fluctuaciones diarias. Esta ilusión de sagacidad es lo que lleva a que los inversores individuales pierdan sistemáticamente contra las instituciones financieras. Estimándose que un 2. 2% del PIB se transfiere de inversores individiduales que realizan una compraventa frecuente hacia las instituciones financieras. De igual manera, también se ha observado que las mujeres son consistemente mejores inversoras que los hombres. Entornos de poca validez: La ilusión de sagacidad antes presentada en gran parte se debe a intentar predecir algo que es impredecible. En eso consiste un entorno de poca validez. Para poder llegar a ser un experto no es solo suficiente con dedicar 10. 000 horas de tu tiempo. También tienes que recibir un feedback consistente y estadísticamente significativo. El azar puede llegar a alterar la percepción de la habilidad. Uno de los ejemplos que se suele exponer es el de los radiólogos. Es una disciplina en la cual el feedback es tardío y en muchos casos no es concluyente al 100%. Esto lleva a que ser experto en radiología sea más difícil que serlo en otras disciplinas donde hay una respuesta inmediata y clara respecto a tus acciones. Similar a radiología es la histopatología. Durante mi colaboración con el Vall d’Hebrón era perceptible la complicación del asunto. Determinar si una célula es o no cancerígena tiene un cierto componente aleatorio. Kahneman explicaba también que determinar la valía de un soldado en pos de un examen psicológico dependía más del azar de lo que le gustaría. Afortunadamente hay soluciones. Cuando se necesita un determinado juicio de valor es posible conseguir un resultado más objetivo. El autor del libro explicaba que lo que él hizo fue separar la prueba en diversas subpruebas de menor alcance. Cada una mucho más específica que la evaluación general. A su vez, el resultado de cada entrevista se asignaba a un profesional distinto el cual solo daba una puntuación subjetiva de su percepción de ese aspecto. Posteriormente, todas las puntuaciones se combinaban en un único resultado. De esta forma se consigue domar un poco más la intuición. Este sistema de evaluación por tokens también lo desplegó un antiguo profesor mío de literatura para evaluar debates. Fernando Fedriani hizo su tesis doctoral en un sistema de evaluación del debate. De nuevo, mediante la creación de tokens se consigue que el evaluador reduzca sus sesgos a unos atributos concretos. Al promediar los resultados los efectos azarosos quedan reducidos, resultando en un método más objetivo. En el ámbito médico también es posible dotar a los profesionales de herramientas para conseguir mejores diagnósticos. Sin embargo, como describe el autor del libro, los especialistas son reticentes a aceptar esos cambios. A pesar del cuerpo de evidencia creciente, el statu quo tiene bastante peso. Pre mortem: Una técnica exitosa para mitigar los efectos de los sesgos humanos es lo que el autor denomina como el pre mortem. Es un ejercicio simple. Considere este escenario. Es un ejecutivo de una empresa que está planeando un proyecto a largo plazo. Ha hecho la investigación pertinente y sabe los riesgos y posibles beneficios. Sin embargo, es posible que haya sobrestimado la posibildad de éxito. Así que la solución es la siguiente. Imagínese que ha pasado un año y el proyecto ha fracasado estrepitosamente, ¿por qué? Intente dar una explicación de qué ha salido mal. Así es como Kahneman descubrió que los ejecutivos conseguían estimar con mayor precisión los verdaderos riesgos así como los plazos necesarios para llevar a cabo sus proyectos. Ley Weber-Fechner: Esta ley psicofísica es una formulación matemática de la percepción humana a los estímulos. Fechner descubrió que la percepción de un estímulo escala acorde al logaritmo de la magnitud del mismo. De manera parecida sucede en la percepción de la utilidad de dinero. De forma aproximada Kahneman nos muestra el siguiente gráfico: Y nos explica que para quien es rico 200€ valen menos que para quien es pobre. Nada remarcable. De igual manera perder 200€ te duele más si solo tienes 200€ que si tienes 200. 000€. Y parece evidente también que perder duele con más fuerza que ganar. Son hipótesis sencillas y bastante aceptables. Pero las conclusiones que se deducen de ellas veremos que no lo son tanto. Primero a través de dos preguntas y luego más rigurosamente con el teorema de Rabin. Empecemos por la primera pregunta, tiene que que escoger entre dos opciones:  25% probabilidad de ganar 240€ y 75% probabilidad de perder 760€.  25% probabilidad de ganar 250€ y 75% probabilidad de perder 750€. La segunda opción es claramente mejor y así lo decidieron todos los participantes del estudio. Ahora se presentan otras dos elecciones diferentes. Primero elija entre:  100% probabilidad de ganar 240€.  25% probabilidad de ganar 1000€ y 75% probabilidad de no ganar nada. Y luego elija entre:  100% probabilidad de perder 750€.  75% probabilidad de perder 1000€ y 25% probabilidad de no perder nada. Si es como la mayoría de participantes del artículo del apéndice B del libro seguramente tenga aversión al riesgo en las ganancias apostando por la cantidad segura e inclinación al riesgo en la pérdidas apostando por la posibilidad de no perder nada. De igual manera en la primera pregunta habrá escogido la opción dos sin dudarlo. Pues ahora viene la contradicción. Si escogió la opción segura de ganar 240€ y la incierta de no perder nada, eso es equivalente a la opción 1 de la primera pregunta que claramente rechazó. Un mismo problema enmarcado de dos formas distintas lleva a decisiones opuestas. Kahneman trabajó durante mucho tiempo con un compañero llamado Amos. Amos comprobó que este efecto marco se podía usar para manipular las decisiones de los expertos en salud pública. En una conferencia sobre lo que denominaban enfermedad asiática a la mitad de los asistentes se les presentó una versión que ilustraba una posible solución junto con las posible vidas salvadas. A la otra mitad les mostró el programa pero les explicó cuántas personas morirían. En esencia es lo mismo, si de 1000 personas 100 fallecen, eso implica que 900 sobreviven. No cambia en absoluto la eficacia del tratamiento el exponerlo en términos de fallecimientos que de salvaciones. Pero los funcionarios que asistieron sí que tomaron decisiones diferentes en función de la versión escuchada. Otro caso peculiar es sobre las deducciones fiscales por hijos. Thomas C. Schelling explicaba en su libro Choices and Consequences uno de estos efectos marco. En él presentaba dos escenario que en principio son equivalentes pero no lo parecen. El primer escenario consistía en una posible ley sobre deducciones fiscales por hijos. La pregunta era como sigue:  ¿Debe la deducción fiscal por hijo ser mayor para el rico que para el pobre? Sus alumnos contestaron que no. Era injusto dar mayor ventaja al rico. Ahora veamos el segundo escenario. Supongamos que no existe ninguna deducción fiscal por tener hijos. Pero que en cambio existe una penalización para quien no los tiene. Dicha penalización vendría dada por un impuesto adicional para quien no tenga hijos. Ahora la pregunta es:  ¿Debe el pobre sin hijos pagar el mismo impuesto que el rico sin hijos? De nuevo respondieron que no. Quien tiene más paga más. Es injusto hacer pagar al pobre igual que al rico. Ahora bien, esta situación Schelling explicaba que era contradictoria. Si el que no tiene hijos paga más, es lo mismo que decir que el que tiene hijos paga menos. Esto a su vez nos está diciendo que si queremos que el rico sin hijos pague más impuestos, esto equivale a querer que el rico con hijos tenga una mayor deducción fiscal. Reformular el problema hace que la respuesta cambie por completo. Enmarcar una propuesta en términos de pérdidas o de ganancias altera la conducta de quien toma la decisión. De igual modo tomar varias decisiones una detrás de otra o una sola que englobe todos los escenarios de golpe también altera el resultado. En el libro se menciona un ejemplo de este artículo del propio Kahneman. En él se preguntaba por cuánto se estaba a dispuesto a pagar para salvar delfines respecto a cuánto se estaba dispuesto a pagar para ayudar trabajadores del campo que padecen cáncer de piel. En el libro nos cuentan que se está más dispuesto a dar más dinero a los delfines, aunque la diferencia no es muy elevada (12. 57\$ vs 12. 18\$, con elefantes sí que es algo mayor la diferencia). El autor explica que si se presentasen a la vez estas cuestiones nos daríamos cuenta de que en un caso estamos ayudando a una persona y en otro no, por lo que la cifra se revertiría, aunque no aporta evidencia de ello. Sin embargo, de lo que sí aporta evidencia es de las sanciones de las agencias reguladoras estadounidenses. Una multa en materia de seguridad laboral está acotada en 7000 dólares mientras que la Ley de Conservación de Aves Silvestres contempla multas de hasta 25000 dólares. Dentro de cada agencia las sanciones son coherentes entre sí, pero al no haber una puesta en común, los resultados terminan siendo muy dispares. Si hubiese un marco regulatorio común, estos resultados no se observarían. Teorema de calibración de Mathew Rabin: Este teorema es una continuación de las contradicciones del apartado anterior pero formalizado matemáticamente. Es una prueba de que si rechazas este juego:  50% probabilidad de perder 100€ y 50% probabilidad de ganar 110€. También deberías rechazar este otro:  50% probabilidad de perder 1000€ y 50% probabilidad de ganar 2. 000. 000€. Lo que resulta contradictorio. El teorema se enmarca dentro de la teoría de la utilidad esperada donde las decisiones se toman en base a la esperanza de utilidad y no en función del dinero esperado. Se define una función $U(w)$ que devuelve el valor de la utilidad sobre una cantidad de dinero $w$. Y un maximizador como ellos lo llaman es aquel que se basa en esa función para decidir. Por ejemplo, rechazar la primera apuesta descrita anteriormente se traduce en $0. 5 \cdot U(w-100) + 0. 5 \cdot U(w+110) &lt; U(w)$. Y aceptar la segunda es equivalente a $0. 5 \cdot U(w-1000) + 0. 5 \cdot U(w+2. 000. 000) &gt; U(w)$. Pues la implicación de que aceptar una conlleva aceptar la otra y vice versa no es otra cosa que una descripción de la función de utilidad. En el apartado de la ley de Weber se mostraba que la utilidad del dinero es una función cóncava. Esto nos viene a decir que si trazas una recta sobre dos puntos en la curva, esta quedará por debajo de la propia función. Y esa es la hipótesis final que faltaba para rematar el teorema que dice como sigue: Supongamos que para toda $w$ la función $U(w)$ es estrictamente creciente y cóncava. Supongamos también que existen $w_1$ y $w_2$ con $w_1 &gt; w_2$ y que existen $g$ y $l$ con $g &gt; l &gt; 0$ tales que para todo $w \in [w_1, w_2]$$$0. 5 \cdot U(w-l) + 0. 5 \cdot U(w+g) &lt; U(w)$$entonces para todo $w \in [w_1, w_2]$ y para todo $x&gt;0$ se cumplen estas dos condiciones, estando la primera sujeta a que $g \le 2\cdot l$:\begin{align}U(w) - U(w - x) \geq&amp; \begin{cases} 2 \left( \sum_{i=1}^{k^*(x)} \left( \frac{g}{l} \right)^{i-1} r(w) \right) &amp; \text{si } w - w_1 + 2l \geq x \geq 2l, \\2 \left[ \left( \sum_{i=1}^{k^*(w - w_1 + 2l)} \left( \frac{g}{l} \right)^{i-1} r(w) \right) \right. \\\left. + \left[ x - (w - w_1 + l) \right] \left( \frac{g}{l} \right)^{k^*(w - w_1 + 2l)} r(w) \right] &amp; \text{si } x \geq w - w_1 + 2l;\end{cases}\\U(w + x) - U(w) \leq &amp;\begin{cases} \sum_{i=0}^{k^{**}(x)} \left( \frac{l}{g} \right)^i r(w) &amp; \text{si } x \leq w_2 - w, \\\sum_{i=0}^{k^{**}(w_2)} \left( \frac{l}{g} \right)^i r(w) + \left[ x - w_2 \right] \left( \frac{l}{g} \right)^{k^{**}(w_2)} r(w) &amp; \text{si } x \geq w_2 - w,\end{cases}\end{align}donde $k^*(x) = \lfloor x / 2l \rfloor$, $k^{**}(x) = \lfloor x / g + 1 \rfloor$ y $r(w) = U(w) - U(w-l)$Quien quiera ver la demostración original puede dirigirse al artículo original. Pero no está muy pulida. Por ejemplo, la hipótesis de $0. 5 \cdot U(w-l) + 0. 5 \cdot U(w+g) &lt; U(w)$ es innecesaria en la demostración. La propiedad descrita del teorema es válida para toda función cóncava. Una demostración algo más pulida con un análisis más riguroso de las hipótesis se halla en este artículo más moderno. De este teorema tan enrevesado se deduce un colorario que es el que se ha usado para justificar los escenarios del principio: Supongamos que para toda $w$ la función $U(w)$ es estrictamente creciente y cóncava. Supongamos también que existen $w_1$ y $w_2$ con $w_1 &gt; w_2$ y que existen $g$ y $l$ con $g &gt; l &gt; 0$ tales que para todo $w \in [w_1, w_2]$$$0. 5 \cdot U(w-l) + 0. 5 \cdot U(w+g) &lt; U(w)$$Entonces para todo entero positivo $k$ y para todo $m &lt; m(k)$ se cumple también$$0. 5 \cdot U(w- 2\cdot k \cdot l) + 0. 5 \cdot U(w+m \cdot g) &lt; U(w)$$siendo este multiplicador $m(k)$ tal que así:\begin{equation}m(k) \equiv \begin{cases}\frac{\ln \left[ 1 - \left( 1 - \frac{l}{g} \right) 2 \sum_{i=1}^{k} \left( \frac{g}{l} \right)^i \right]}{ \ln \frac{l}{g}} - 1 &amp; \text{if } 1 - \left( 1 - \frac{l}{g} \right) 2 \sum_{i=1}^{k} \left( \frac{g}{l} \right)^i &gt; 0, \\\infty &amp; \text{if } 1 - \left( 1 - \frac{l}{g} \right) 2 \sum_{i=1}^{k} \left( \frac{g}{l} \right)^i \leq 0. \end{cases}\end{equation}Aquí es donde realmente sí que se usa la hipótesis de que $0. 5 \cdot U(w-l) + 0. 5 \cdot U(w+g) &lt; U(w)$. Cabe decir que en la demostración de este corolario se asume que $[w_1, w_2] = \mathbb{R}$, es decir, que de las dos ramas de la función por partes del teorema solo se usa la rama de arriba la cual simplifica los cálculos. Traducido esto significa que la negativa a aceptar o rechazar los juegos propuestos al principio es irrelevante de la fortuna inicial. El propio Rabin reconoce este hecho y dice que incluso si el comportamiento del jugador varía en función de la fortuna inicial, se pueden llegar a conclusiones similares pero requiere de más operaciones algebraicas todavía y el resultado es algo menos fuerte. Para terminar de ejemplificar este corolario veámos qué sucedía en el primer juego. Rechazarlo significaba que $0. 5 \cdot U(w-100) + 0. 5 \cdot U(w+110) &lt; U(w)$, por lo tanto con este corolario si tomamos $k=5$ nos queda que $1 - \left( 1 - \frac{l}{g} \right) 2 \sum_{i=1}^{k} \left( \frac{g}{l} \right)^i \approx -0. 22 &lt; 0$ por tanto $m$ puede ser cualquier valor quedando que $0. 5 \cdot U(w-1000) + 0. 5 \cdot U(w+2\cdot 10^6) &lt; U(w)$, o lo que es lo mismo, que rechazaríamos jugar aunque el premio fuera de dos millones de euros. Valores decisorios: La asimetría entre la búsqueda del riesgo en las pérdidas y la aversión al riesgo en las ganancias se extiende un poco más en un patrón de cuatro opciones: En un intento por explicar esta situación y además resolver la problemática del teorema anterior, Amos y Kahneman hacen una modificación a la función de utilidad añadiendo una función de ponderación de probabilidades. Esta función ha ido evolucionando con el tiempo. El concepto es sencillo, si tienes dos opciones eliges la que tiene mayor valor. Pero calcular ese valor es complicado. En el modelo más básico los agentes racionales simplemente eligen la opción que repercute más ganancias y menos pérdidas, es decir, siguiendo la notación anterior eliges la opción donde $w$ es mayor ($V=w$). Tras analizar que los eventos tienen probabilidades se mide el valor esperado de la ganancia y no la ganancia per se, o sea, $$V=\mathbb{E}[w] = Pr(A_1) \cdot w_1 + \dots + Pr(A_n) \cdot w_n$$Cuando se consigue entender que el valor monetario no corresponde con el valor real se añade una función de utilidad que es la $U(w)$ y el valor real pasa a ser $$V=\mathbb{E}[U(w)] = Pr(A_1) \cdot U(w_1) + \dots + Pr(A_n) \cdot U(w_n)$$Y finalmente, para parchear las incongruencias que resultan del comportamiento humano se añaden ponderaciones a las probabilidades obteniendo la formulación de la teoría de perspectivas: $$V=\pi(Pr(A_1)) \cdot U(w_1) + \dots + \pi(Pr(A_n)) \cdot U(w_n)$$Es difícil hacer más flexible el marco teórico. Y en concreto cabe preguntarse si $\pi(x) = x$ o no, porque de ser así es una complicación innecesaria. Eso es lo que hicieron Amos y Kahneman en su artículo sobre esta teoría. Para estimar las ponderaciones preguntaron a los participantes sobre qué decisiones tomarían. Siendo las decisiones como esta:  50% de probabilidad de ganar 1000€ y 50% probabilidad de no ganar nada.  100% de probabilidad de ganar 450€. Variando las cuantías monetarias y las probabilidades de este tipo de preguntas se puede estimar la función de ponderación $\pi(\cdot)$ de los participantes. El resultado lo presenta en la tabla 4 del capítulo 29.  Quedando palpado el hecho de que $\pi(x) \neq x$. En la traducción al español llaman a esta función de ponderación el valor decisorio. Traducción que no considero muy acertada dada la connotación de la propia formulación. Es cierto que es un valor decisorio porque el experimento utiliza el valor a partir del cual prefieres la opción segura a la opción azarosa. Sin embargo, si se mira desde los ojos del marco teórico esa tabla tiene más sentido si se habla de ponderaciones. En cualquier caso, lo que el estudio de estos valores muestra es que las personas no ponderan las probabilidades de forma lineal. Un incremento del $2\%$ de la probabilidad tiene mucho más peso si es del $98\%$ al $100\%$ que si es del $61\%$ al $63\%$. Efecto anclaEste efecto queda ilustrado y medido con el siguiente experimento. En él se les preguntó a los participantes una de estas dos cuestiones:  ¿Es la altura de la secuoya más alta mayor o menor de 1. 200 pies? ¿Cuál es su estimación de la altura de la secuoya más alta?El valor inicial que se da en la pregunta es lo que denominan ancla. Los valores de las preguntas previas eran 844 y 282 respectivamente. Como se puede observar, la presencia del ancla mueve la media en dirección del valor inicial mencionado en la pregunta. Esto es el efecto ancla, que hace que las estimaciones estén condicionadas por el primer valor que se escucha. Y el índice de anclaje es el ratio entre la diferencia de las medias y la diferencia de la media sin ancla y el ancla. O sea, en el caso anterior sería $(844 - 282) / (1200 - 282) = 0. 55$, un cincuenta y cinco por ciento. Este efecto afecta a todos los mortales. Los expertos tasadores inmobiliarios y los jueces no son excepciones. En otro estudio se vio que el índice de anclaje de los tasadores era del 41%. Sus tasaciones se veían afectadas por cifras que los investigadores les habían dado momentos antes de visitar las viviendas. El grupo de control que no tenía conocimientos inmobiliarios sufría un anclaje del 48%, ligeramente superior. Y más sorprendente es el índice de anclaje de los jueces. Un estudio con jueces alemanes mostró que su índice de anclaje era del 50%. En ese experimento a los jueces se les hacía tirar un dado antes de evaluar las condenas. Los dados estaban trucados para que salieran 3 o 9. El resultado fue que los jueces a los que le salía un 9 tendían a dar condenas más altas que aquellos que el dado les marcaba un 3. Este no es el único condicionante mencionado en el libro que afecta a los jueces. En otro capítulo Kahneman nos cuenta como los jueces israelíes tendían a negar la condicional a los reclusos que las solicitaban si el momento de evaluar el informe se daba próximo a la hora de comer. En ese experimento los jueces no sabían que les estaban estudiando. Ley de los pequeños números: Ese es el nombre que Kahneman le dio a la ignorancia del tamaño muestral a la hora de sacar conclusiones. En muestras pequeñas es posible encontrar resultados sorprendentes debido a la varianza del proceso. Para obtener una conclusión certera uno debe asegurarse de tener una muestra grande para que no quede lugar a dudas. En ese capítulo me llamaron la atención dos ejemplos de esta ley. El primero es el hecho de que los condados con menor tasa de cáncer renal son rurales. La vida en el pueblo es más sana se podría concluir. Pero también es un hecho que los condados con mayor tasa de cáncer renal también son rurales. Debido a que estos condados tienen menor población, la tasa de cáncer renal sufre una mayor varianza. Esto hace que sean los pueblos pequeños los que destaquen con estadísticas muy grandes y también muy pequeñas por puro azar. El segundo ejemplo es más llamativo porque costó 1700 millones de dólares a la fundación Gates. Querían averiguar qué colegios eran mejores y llegaron a la conclusión de que los más pequeños. Lo que los llevó a promover la creación de escuelas más pequeñas. Lo más probable es que sus resultados se debieran precisamente a ley de los pequeños números. Efecto de dotación: Este efecto se ilustra con el ejemplo de un artículo de Alan B. Krueger. En él se habla de cómo los encuestados no estarían dispuestos a pagar 3000\$ por una entrada de la super bowl porque es muy cara pero a su vez no venderían una entrada por menos de 3000\$. El efecto de dotación es el término usado para referirse a la diferencia entre los precios de compra y venta de los usuarios cuando se trata de bienes de uso vs bienes de intercambio. En las transacciones habituales se vende un objeto que costó 20€ por esa misma cantidad. Pero cuando el poseedor del producto le asocia un valor emocional, el precio de venta y el de compra difieren. El alcance de este efecto y las formas de hacerlo desaparecer aún están por estudiar comenta Kahneman. La explicación que da Kahneman de este efecto de dotación se basa en la asimetría de las ganancias y las pérdidas. Perder 1€ duele más de la alegría que da ganarlo, como se veían en las gráficas previas de la función de utilidad. Esto hace que se genere esa asimetría de los precios de compra y venta. Esta misma aversión a la pérdida se estudió en el entorno matrimonial quedando registrado que el éxito a largo plazo depende más de evitar lo negativo que de perseguir lo positivo. Felicidad: Hacia el final del libro se trata sobre varios resultados referentes al estudio de la felicidad. La metodología tiene sus puntos débiles, y considero que esos puntos débiles quedan bastante bien plasmados en este post reciente de Maxim Lott. Aún así son resultados interesantes los que se presentan en el libro. El más remarcable es que la felicidad crece con los ingresos pero se estanca en 75000 dólares al año. Hay múltiples fuentes de este dato, en el libro se basan en este artículo de 2010 realizado por el propio Kahneman. Cuando veo este resultado siempre pienso que me gustaría que hubiese resultados similares en España. El índice de bienestar Gallup-Healthways que es el que se mide en ese estudio solo involucra estadounidenses. Eso hace que sea muy difícil interpretar el resultado. Porque 75000 dólares en Estados Unidos y en 2010 no tienen un equivalente sencillo de calcular en la España de 2024. Otro hallazgo curioso es que el sufrimiento se distribuye de forma poco uniforme. En el artículo titulado “La mayor parte de la gente es feliz” nos cuentan que en general la gente no experimenta episodios desagradables salvo una minoría que sufre por todos los demás. El resultado se remonta a 1996, mucho antes de las redes sociales y la globalización, por lo que es posible que ya no sea tan aplicable. Y el último resultado que presento en esta entrada del blog es la gráfica de satisfacción en la vida antes y después del matrimonio Dejo al lector o lectora que la interprete como quiera. Conclusión: El objetivo del autor con este libro es proporcionar nuevo vocabulario a sus lectores para sus charletas frente a la maquinilla del café, al mismo tiempo que intenta informar a la sociedad de los sesgos cognitivos que todos padecemos. Es la obra de toda una vida dedicada a la investigación pero desprovista de formalismos y en un lenguaje bastante accesible. Evidentemente el autor tiene sus opiniones y su visión es claramente parcial, algo que él mismo reconoce en el libro cuando habla de una de sus rivalidades con sus adversarios intelectuales. Pero los experimentos son los que son y los datos son datos, no opiniones. Cada cual ha de formarse su propia opinión en base a esos datos con calma y sin prisas. Para mí al principio mi reacción a todos estos resultados no era otra que pensar que el ser humano es imbécil y cabezota. Tras reflexionar debidamente finalmente he concluido que la conducta humana es mucho más compleja de lo que parece. Nuestras experiencias nos forman intuiciones del mundo que habitamos que con frecuencia no son completamente acertadas. Nuestra capacidad ejecutiva se ve afectada por factores externos que raras veces controlamos y tener conocimiento de ello no resuelve el problema. Las apariencias engañan y es muy difícil ser plenamente objetivo y racional. No obstante, cada día se sabe más y se descubren nuevas herramientas para amortiguar el impacto de los sesgos cognitivos. La sociedad evoluciona lentamente, pero lo hace en la dirección correcta. Extra: Este experimento no es del libro, lo cual es imposible porque es de hace pocos años, pero me llamó mucho la atención. Yo lo conocí por este tweet. En él se dice a los participantes que giren una ruleta en su cabeza que tienen un $20\%$ de probabilidad de salir amarillo y un $80\%$ de salir rojo. Después se hace una encuesta preguntando a la gente que de qué color ha salido la ruleta al girar en su cabeza. El resultado de la encuesta es que alrededor del $80\%$ dijeron que salía rojo y alrededor del $20\%$ dijeron que salía amarillo. Otras personas han reproducido el mismo resultado con otros porcentajes y otras redes sociales y los porcentajes de la encuesta suelen estar en un margen del $5\%$ respecto a las probabilidades imaginarias. Curioso, ¿verdad? "
    }, {
    "id": 11,
    "url": "https://granadata.art/Python/",
    "title": "How to build a python library",
    "body": "2024/03/24 - When I was halfway through my bachelor’s thesis I decided to package everything into a python package to facilitate easier usage for the next student. However, instead of just following a simple and nice tutorial I decided to read and study the relevant PEPs and the official setuptools documentation. That was quite time consuming but also very worth it. I will always recommend getting as deep as you can in every topic you are interested. This post is a summary of the fundamental takeaways. Packaging in python has changed over the last years, so what you will find here is probably not used in some of the packages you already know. The post is structured as follows: first, the summary. Then, some extra details. And finally, some anecdotes. Packaging your codeThe process is very simple once you know how to do it. You just need to put every code file inside a folder, add a pyproject. toml file, run some commands and you are done. For example, my library was called tumourkit, so I put everything I got so far into a folder called tumourkit. Now, the most basic pyproject. toml can be this: 123456789101112131415[build-system]requires = [ setuptools&gt;=61. 0 ]build-backend =  setuptools. build_meta [project]name =  tumourkit authors = [ { name =  Jose Pérez Cano , email =  joseperez2000@hotmail. es  },]description =  A SDK for tumour study requires-python =  &gt;=3. 8 dynamic = [ version ,  dependencies ][tool. setuptools. dynamic]dependencies = { file =  requirements. txt  }Here, the file structure should be 12345root-folder├── tumourkit│  ├── . . . ├── pyproject. toml├── requirements. txtWith that, you just need to install some packages python -m pip install twine build wheel and run two commands: python -m build and python -m twine upload dist/*. With that you are done. You will be probably thinking that this is very simple, and you are right. Packaging code can get quite complex, but for the most part it is just this. There are some extra additions you can consider. Like adding a readme, exposing some command line programs, tagging the package or including a license. Many of the previous changes can be done by simply adding some lines to the project section: 1234567891011121314[project]. . . readme =  README. md license = { file =  LICENSE  }classifiers = [   Development Status :: 2 - Pre-Alpha ,   Environment :: Console ,   Intended Audience :: Healthcare Industry ,   Programming Language :: Python :: 3. 8 ,   Programming Language :: Python :: 3. 9 ,   Programming Language :: Python :: 3. 10 ,   Typing :: Stubs Only ,  . . . ]There are many classifiers, a full list can be found here. And the file structure should now include the extra files: 1234567root-folder├── tumourkit│  ├── . . . ├── pyproject. toml├── requirements. txt├── README. md├── LICENSEFor the commands it is also very simple. Suppose you have a function called main under the file tumourkit/example. py and you want to call that command my_example. Then you could add these lines to the toml: 12[project. scripts]my_example =  tumourkit. example:main So now, when the library is installed running my_example in the terminal is equivalent to running that function. You don’t need to have the classical if __name__=='__main__', in fact it will get ignored. The choosing of the license could be a topic for another day. For now, I will just mention that my library ended up being AGPL-3. 0. Even though this is enough for a package to be distributed, many of the packages you know have far more files than just the four I mentioned. That is because packages also have documentation, testing, building, excluded / included non-code files and many more. For every extra thing you add to your package, you will need an extra file that contains information about that. In my case I only added one more file which is . readthedocs. yaml. This file is read by the read the docs servers and contains the information on how to build the documentation. The file was just copied from their page. This is something to be done incrementally. Whenever something is required you look up the relevant documentation and include it. Following the philosophy of doing things incrementally, it is also a good practice to keep a CHANGELOG. md and follow semantic versioning. The rabbit holeAll of the above is now very well explained here. But when I decided to do it, it was a period of transition. There were some online tutorials that still referred to the old-fashioned setuptools way of doing things. Until only very recently, setuptools was the only way of doing things. You needed a setup. py file that was run to build the egg files that where then installed or uploaded. But the python foundation decided it was time to democratize it. PEP 517 specified how to create new build backends so that others could emerge. We now have poetry, flit, hatchling and others. Also, to simplify the configuration file, in PEP 518 the pyproject. toml was introduced. This was written in 2016 and 2017. But it took a few years until backends started to appear. Between 2018 and 2019 setuptools added support for pyproject. toml in version 40 and you no longer needed to add the setup. py file even if you use setuptools. I did my thesis in 2023. It would seem as if 4 years were enough for the world to adapt to those changes. But it wasn’t. I think in most cases packages are built once and used forever, but never fully maintained. The cost of being up to date with those changes is quite high with no reward at all. I dit it this way because it was my first time, so I thought I better be doing it the updated way. But I can understand that you don’t want to change the way everything is configured now that it is working. Python is always evolving. In the past months the PEP 703 showed up proposing a way to remove the GIL, which is probably one of the most representative aspects of the language, together with the garbage collector. And some very characteristic features like pip, were not in the beginning. In the beginning there was distutils and easy_install. It was in 2013 with PEP 453 that pip was proposed as the default. However, community often rejects wide adoption as with the type hints. They were proposed in PEP 484 in 2014 but as of today, I can only find them in very mature libraries. Amateurs just use python without types. And that is fine. There is no need to be knowledgeable about everything. Sometimes the basics are more than enough. That is something I really like about computer science. You can do a lot knowing so little, but if you want to know more, there is always more. AnecdoteI was hired by the university when I was doing my bachelor’s thesis to develop an algorithm that could solve a problem. That means you obtain money in exchange for some freedom on what you can and cannot do. In my case that meant to provide guidance to others in the project and that I wasn’t the one taking the decisions about priorities. Packaging code is time consuming, and therefore I needed to ask my “boss” about whether or not I should spend time on that. However, I just wanted to do it, so I dit it. But since I needed to justify myself I did the following. On a friday night I emailed my tutors asking for permission. I knew they were busy so they wouldn’t respond until next monday. During saturday I spent more than 10 hours reading and making changes to the code so that it could be a package. There were around 5 thousand lines of code, but luckily I could refactor everything in one day. When sunday arrived, I just emailed again saying the absence of response was an approval and that I have just done everything that was needed. They couldn’t say no, after all, the work is already done. Normally, you won’t get blamed for doing more, only for doing less. If you want to build something, just do it. "
    }, {
    "id": 12,
    "url": "https://granadata.art/hack_life/",
    "title": "The Hack Life",
    "body": "2023/11/12 - Believe or not today I got up at 4am to take a train Madrid -&gt; Barcelona to work for more than 24 hours straight for free. Why? Melancholy. Melancholy is such a strong emotion and it’s not given enough credit. We love to dwell in past memories and try to revive the happy ones. Today, I got the opportunity to feel a fading emotion: Youth. There are certain lifestyles and choices that get lost when we get older, more mature. Coding at 2am is one of them. It’s 2am again, and instead of coding this time I will be writing about my hacking adventures. Okay, that was a quite poetic and obscure introduction. Quite contrary to the very specific intros that we are used to see in this blog. What is the hack life I mention in the title? Let me introduce you to the concept of a hackaton. Hundreds of students in the same place coding a solution to a challenge in a weekend. That is the official definition. Unofficially, a hackaton is a place to meet awesome friends, life-long friends, a place to learn from more experienced people about technical topics and receive advice about life. There are funny activities and deep lore. Like the ducks in the HackUPC. It was in my first hackaton that I met my crew. At that time we didn’t know each other but created a team and participated together. Now we are besties. How do we go from there to writing a post at 2am in a faculty where I no longer study? Stay with me, it is a long story.  In total I have assisted to 6 hackatons / datathons. The HackUPC 2019, 2021, 2023, MamutHack, CFIS Datathon and Hacknights. After that, I wanted to be in the other side of the event, to be an organizer. I helped organize the second datathon FME on 2022 and today was the third datathon FME, which I am also helping organize. This is probably the last time I will be at either side of a datathon / hackaton. Only time will tell, but it definitely feels like so. I am in a position where I know personally many of the people that participate, being this either their first time or second time. I also know other organizers and I even know personally many sponsors, both in companies and universities. A datathon is the people that composes it. When time passes, and people goes, the datathon dies. Figuratively. For me. For others it is a discovery. (As a side note, when I was midway writing this paragraph the whole electricity of the faculty went off, more on that later). Where was I? Ah, yes, the cycle of the hackaton. You participate, you organize, you sponsor and you continue with your life. It is like that summer romance that you will never forget but you will also never continue with. That’s life, continuous change. No matter how hard you try to repeat some memorable memory, it will never be the same. You are not the same, can’t experience it the same way. You only have one chance to enjoy it. Back to the hackaton stuff. Living a hackaton, and specially a HackUPC comes with lots of anecdotes. You may end up shooting nerfs to random scandinavian people. Or find a ruber duck below a random chair. The first time I participated I was sucked deep into the challenges. Full focus in coding and giving the best solution. Spoiler: It was not. As a first year student you have no experience nor knowdledge. And this type of challenges would normally require months of full-time work to fully develop. You are only supposed to build a quick MVP. But the first (and second) time, it is about learning. The real challenge is to learn to code and code at the same time. Do you want a webapp? Great! Learn Javascript, HTML and CSS all at once from scratch and then apply the knowdledge. Do you want to build a vision classifier? Fantastic! Look for a github repo with a pretrained classifier, learn how to tweak the solution to your needs and do it. It is a way of learning by building. By hacking. If something does not give the perfect solution, instead of doing a whole course to fix everything you just learn or create any trick that does the job. The power in it comes from doing it during 36 hours straight. It is a very condensed way of studying. I will probably never be as focused as I was in those first hackatons.  Once you get older you start valueing other aspects of the events. More personal. Less technical. And that is when I knew I preferred to deliver food and give advice rather than to code. When you are a volunteer, everybody asks you. You get to know many more people. And you get to see wonderful ideas. When I was participating it was all about me, about my solution and my algorithm. Which is fine. However, when you start hearing many team’s ideas you learn a lot. In this datathon I was surprised several times with the Mango challenge. Mango brought a problem where you were given a set of good-looking outfits and where asked to analyse it and recommend 10 new outfits. Said like that is looks quite interesting. But if you know a bit about machine learning you know this problem is ill-posed. You only have positive labels. It is the most imbalanced dataset possible. My first thought was to simply create ugly outfits and label them as negative. But this approach is highly subjective which will make the model subjective. Teams came up with way better methods. One suggested to build a classifier with pairs of garments. Every outfit is composed of several garments. You can construct positive pairs by looking at existing combinations and consider every combination that is not in the dataset as negative. This is a natural way to recover the balance in the dataset. It is not perfect, but is a very interesting idea. Other team tried to build a markov model and construct the outfit iteratively. To learn the probabilities they just assumed independence between attributes and computed ratios. Simple, yet possible useful. Maybe these approaches get you nowhere, but the ideas are amazing. The creativity is not something you can learn. Everybody is creative in its own way. Being a mentor provides you with the opportunity to discover new ideas from others.  In the second datathon FME, the first I helped organize I discovered this new world. And I met lots of mentorees (I can’t find an appropiate enough word for this type of relationship). Meeting with them again, seeing how they have evolved it’s what fills me. That and coming to see my crew again. All the work that needs to be done, moving tables, giving food, solving doubts. It is all paid in a different way. As an unforgettable experience. This is the reason when this datathon was announced I immediately bought tickets to come here from Madrid. And what a datathon. There is yet one day to come and it is already my most memorable datathon. Last year some students built a tower of plastic glasses as big as a person and enclosed a person in it. Well, this datathon also had some architecture classes. This time with pizza boxes.  It was a bit chaotic and full of rustle up. There are 100 more students and we tried the same logistic strategy which didn’t work as well. We learned a lot in the process and manage to get everyone fed which is a major statement. I also guided many people with ideas on how to proceed with the challenges. From my perspective, I am only giving ideas. I know perfectly that implementing such ideas is the real deal. And sometimes they look at me and say: “Yeah, putting the data in wide format, then one-hot encoding it and apply a PCA sounds fine, but how the hell do I do that?”. Others are more advanced technically, but less knowdledgeable in project management. Surely you can apply a very complex network, but have you tried a linear model as your baseline? There are many ways to arrive at a solution. Some are way harder than others. Advancing in the right direction can save you time and headaches. Sadly, you only learn that with experience. No matter what I tell them they will make the same mistakes as I made. And that is the beauty of life. At least I can warn them about the difficulty of the process. Remember the light incident I told you about before? The organizers also have our challenges. Buying 7kg of yogurt at the last moment because we miscalculated how much was needed was one of them. And another challenge was dealing with plug shortages. Being so much people, all with computers, requires electricity and plugs. The building was not prepared for so much load. First, we needed to buy more adaptors to cover the demand. And in the middle of the night, at the most unexpected moment, the net got overloaded. Part of the job.  So that’s the hack life. A very short life. It became a part of me when I started university and it is leaving me now that I am leaving university. When I get older I will look back and remember with melancholy those years. With the nostalgia of a time that cannot be recovered, but that was spent in the best way possible. "
    }, {
    "id": 13,
    "url": "https://granadata.art/crowdsourcing/",
    "title": "Crowdsourcing methods for cancer detection",
    "body": "2023/11/09 - I recently survived the reviewing process of a scientific article called: “Annotation protocol and crowdsourcing multiple instance learning classification of skin histological images: The CR-AI4SkIN dataset” and today I want to explain what all those fancy words mean. The article is currently accessible through this link although Elsevier will paywall it in some weeks. Ironically enough, this is my first published article despite it being the second article I wrote. The peer review process is so unpredictable that some articles take 18 months to review while others take 2 months. At the end of this post I will also talk in more detail about how broken the peer review system is, with examples of the things we were asked to do to publish this article. The problemThe problem at stake here is skin cancer classification. One of the most used tests to analyse the seriousness of a tumour is a biopsy. It is a technique that consists of extracting a piece of the affected organ and digitalizing the image for a physician to look at. The magnification of the microscope allows the cells to be seen in the image and so the pathologist can recognise cancerous structures and tell if they are malignant or benign. An example of such image which are called Whole Slide Images (WSI) in the case of lung is below: The tiny little red square that you see there corresponds to this zoomed in image: As you can see those images are massive, and analysing them requires time and effort. Not only that, each tissue is different, the above was for lung, but the skin looks more like this: And worse than that, there are several different ways to stain the cells. This is the cheapest staining which is called Hematoxylin and Eoxin, but there are others like the immunohistochemistry that creates a better distinction between tumoural and non-tumoural cells but are more expensive. The problem we tackled in the article was to detect if a given WSI corresponded to malignant tissue or not. This type of problem can also be described in the context of Multiple Instance Learning (MIL). In a MIL setting you have to classify a bag of elements into a given class where each individual element has its own class that influences the overall class of the bag. For instance, imagine you want to give a score of how good a basketball team is. If you have Stephen Curry or Lebron James in the team the overall score goes up. The difference in the MIL setting is that you don’t know how good each player is, you just know how the team performed in the last matches. And you try to infer based on that. If you know the team scored 30 three-point shots then you may assume there is one hyperscorer and if in another match the team does not score one single three-point shot then you may be inclined to think that hyperscorer is not playing. This is the type of latent information that we try to discover in a MIL setting. By analysing bag of images we try to find some patterns in the individual images that help analyse other bags of images, without ever knowing anything specific about those images, just knowing about the whole bag.  To be more specific, what we want is to split the WSI into smaller images and be able to classify the WSI only by classifying all the zoomed in patches and then aggregating the result. If we detect one patch to be malignant, then the WSI is quite probable malignant, no matter whether the other images are malignant or not. By looking at many malignant WSI it is possible to extract patterns about the smaller patches and learn to classify those as well. One reasonable question is why? Why is useful to split the image in little pieces? Why not classify the WSI as a whole? The reason is because it is not technically possible at the moment. Training a convolutional neural network (or worse, a transformer) may require TBs of GPU RAM which is impossible right now. And even if that were possible, the amount of data to train such models will also need to be huge, which is not at the moment. The dataset we published in the article has less than 300 patients. Current AI models require billions of images for working at 512x512 resolution. Imagine what is needed for 100000x100000 resolution. But the problem does not finish there. A MIL classification problem is something already quite studied, that is not worthy of an article. The problem is far more complicated. In a MIL setting you ignore the individual labels (they are considered latent variables) and try to infer them using Ground Truth (GT) bag-level labels. Well, what if we don’t even have a Ground Truth? That’s right, labelling medical images requires doctors, but what if we could avoid that little annoyance? What if we could use non-expert annotators instead? Instead of training with expert labels, the problem we faced was to train with annotations that were made by students. Students are cheaper and are more prone to collaborate in research. If we could somehow learn when each of the students is making a mistake we could correctly obtain the GT by asking several students. That is called crowdsourcing.  To sum up, the problem we face is to classify huge medical images were we only have access to noisy labels during training. The solution The first step to reduce the dimensionality of the problem is to train a Region of Interest (ROI) prediction algorithm. The WSIs have a lot of white pixels that can be discarded. To extract such ROIs the experts annotated some regions and then a network trained on that subset of WSI was used to predict the ROI of the rest of the dataset. After that the WSIs were splitted into 512x512 images at 10x magnification. As a reference, the maximum level of magnification is 40x, so we also reduced the dimensionality here by working with more pixelated patches. That is this part of the diagram: The next step to reduce even more the dimensionality was to train a convolutional neural network in an unsupervised manner to obtain feature vectors. This way we could reduce 512x512 images into vectors of dimension 256. Even more than that, we aggregated the feature vectors of each image inside a WSI. So we went from between 100 and 1000 images to just one single vector of 256 numbers. That unsupervised method is called SimCLR. It consists of a very special loss called normalized temperature-scaled cross-entropy loss (NT-Xent). Mathematically it can be expressed like this: $$ \begin{equation}  \ell_{i,j} = - \log \frac{\exp(\text{sim}(\bf{h}_i,\bf{h}_j) / \tau)}{\sum_{k=1}^{2S}\mathbb{1}_{k\neq i}\exp(\text{sim}(\bf{h}_i,\bf{h}_k) / \tau)} \end{equation}$$The $\bf{h}$ are the feature vectors. Unlike other common losses, this loss is not computed independently for each item in the batch and then aggregated. This loss takes into account every element in the batch and is computed for each positive pair and then aggregated. A positive pair is a pair of images that should be considered similar, and a negative pair one that should be considered different. For instance, an image and its reflection or rotation are considered similar while two images of two different patients are considered dissimilar. This way by minimizing this loss one is maximizing the similarity of positive pairs (numerator) at the same time that it is minimizing the similarity between negative pairs (denominator). Where the similarity is measured using cosine similarity. The result is that the network is training to have a separable latent space. Such property is very useful for training classifiers on that latent space. The rest of the parameters in that formula are not quite relevant. The $\tau$ is the temperature but is always fixed at $0. 5$ and the $S$ is the batch size which is limited by the RAM you have, so there is not much room for improvement there. Surprisingly, we found that a batch size of 256 gives better results than one of 512. This is surprising because in the original SimCLR article they stated that anything below 512 was useless. Well, it is not. We have covered the top right of the diagram: With that we overcome the limitations of the MIL setting and reduce the problem to a strongly-supervised one. The next step is to solve the crowdsourcing problem, which is the last part of the diagram: To solve it we can model the problem as a bayesian network. In a bayesian network you have observable variables and latent variables. All the variables influence each other in some way, either as a prior distribution or a dependency relation or in many other ways. The key here is the Bayes Theorem. It lets us go forward and backward. If by knowing the distribution of A we can know the distribution of B, then the reverse also holds. If we know B we can get A. This means that if the output depends on some latent variable we can infer some information about that unknown variable by observing the output. In our case, the bayesian network is visualized in this diagram: Here $\bf{x}_i$ are the input images and $\bf{y}_b^a$ are the non-expert annotations. Those are the only two variables that are observable. And everything that is in between must be inferred in order to obtain a model that predicts labels from images. There are two parts to distinguish in this diagram. The crowdsourcing part and the Gaussian Process part. One models how each annotator behaves while the other adds extra latent variables to make the model more robust and scalable. The easiest to understand is the crowdsourcing part. There are true labels $\bf{z}_b$ for each bag $b$ and each annotator $a$ has a confusion matrix $\bf{R}^a$ that describes the probability that they make a mistake. We don’t know such probabilities neither the true labels, but we aim for discovering them based on the data. Such thing is possible whenever the majority of the annotators are better than random and not adversarial annotators. But even if there is only one adversarial annotator, that is, someone that says exactly the opposite of the truth, it is possible to identify it and correct their labels. The Gaussian Process part is adding an extra latent variable called $\bf{f}_b$ that is more or like a hidden layer in a neural network. It is supposed to capture the relations between new samples and all the training samples so that it can provide a prediction based on similarities computed with some kernel function. Since computing the covariance matrix with respect to all the training data has cubic cost, there is yet another latent variable called inducing points or $\bf{u}_m$ that is supposed to summarise all the content of the training data into just a reduced set of points. There is a lot of theory behind this way of modelling, if any of you is interested in all the maths you can refer to the original LIGO paper that designed this model. Or if you want to know about gaussian processes you can refer to the original article of the scalable version of the GPs. To summarise in a simple phrase the whole method, we are reducing dimensionality by first extracting a ROI, then applying a pretrained network to obtain latent variables, and after that we are applying a crowdsourcing method to classify those extracted features learning from noisy labels. Believe or not, this is not an original method, it is just “a mechanical combination of existing methods”. Or that is what a reviewer said. CuriositiesThe problem and the solution are explained with more detail in the paper. I would like this blog post to contain information that is not published, to reveal other more enjoyable facts that are not told anywhere. During this project we found many results that were not only counterintuitive but sometimes outright against the theory. Since there is the possibility that those facts were due to some implementation detail and not due to the method itself, we didn’t publish them, or didn’t highlight them too much. The first interesting fact is that the initialization of the parameters of the GP affected the result in a very dramatic way. Going from 1 to 2 make the model improve quite drastically. That is surprising because the model must converge to the same value no matter how you initialise it. And it is even more surprising that such a little change can affect so dramatically the output. Contrary to neural networks, GPs have much less parameters. And changing the initialization of just two of them can change the result. This is something that does not happen on other problems or is very difficult to replicate, but it happened to us. The second interesting fact, and this one was the matter of many discussions, is that the crowdsourcing model is sometimes better that a model trained with the expert labels. That is, using non-expert labels can bring better results than using expert labels. At first this caused some commotion. The other authors thought that this implied the experts were not so expert after all. And we even thought about not publishing the article, because such a result could invalidate the test labels. However, the experts are indeed experts and this fact has nothing to do with that. The reason why a model trained on non-experts labels outperforms a model trained on expert labels is not because the labels are worse, but because the model is worse. In machine learning there is one thing called the bias-variance tradeoff where you can obtain better results in a regression problem by reducing the variance even if your model is systematically worse (high bias). In regression one can reduce variance by using ensembles. Without improving the quality of the members of the ensemble, the overall result improves if you perform the mean. In crowdsourcing something similar happens, but this time it is classification and not regression, and instead of bias and variance we have precision and recall. What we found is that the model trained on experts has a higher recall. And that has a lot of sense. Doctors have a tendency to diagnose an illness even if not present because the alternative is worse. It is better to do more tests than to say one is healthy wrongly. For that reason, training with expert labels tends to have that tendency too. The magical part of using crowdsourcing is that it increases precision out of thin air. By averaging the annotations of several people a similar effect as with an ensemble is achieved. This way, even though all the individual non-expert annotators have lower precision than the experts, when combining them they achieve higher precision. So, the conclusion is that doctors are very cautious and to obtain a more precise answer one should ask several doctors. The third fact is a consideration about the research process itself. When we started this project we were quite sure that the method was going to give better than random results easily. Wrong! This database was absurdly difficult to work with. We had to perform a very exhaustive hyperparameter search and even when doing so it was not enough the first time. Initially we were not going to use SimCLR. We tried to use the crowdsourcing method alone. But the method was designed for low dimensions and so we had to try a way to simplify things. And even after finding SimCLR it was not enough because it provided feature vectors for images, not for WSIs. In the end, we made some very arbitrary decisions like simply averaging the vectors instead of using some attention mechanism. And after all that was done, we still had to perform an exhaustive search over the hyperparameters of the model. When we began the project we thought we could iterate at least three times, and we barely manage to iterate once. That is why the model only tackles the binary case even though the database is multiclass. And that is why our model is not end to end. Those two ideas were supposed to be done after achieving a simple baseline. But achieving such simple baseline ended up consuming all the time. The last amazing fact is that the validation set does not correlate with the test set. Truly inspiring isn’t it? The metrics we obtained in the validation set were around 60% or so while the metrics from train and test were around 70 and 80%. And the best performing model in the validation was rarely the best performing model on the test set. That is why the metrics we report on the article are so low sometimes. Not because the model itself is bad in the test, but because the best set of hyperparameters in the validation set is not the best set in the test. Since you cannot use the test set for hyperparameter tuning, the result ended up being worse than expected. If I were to start the project again, I would choose the split to have similar images on all of the splits while not mixing the patients. The current split makes it impossible to infer useful information. It is even better to do the tuning on the training than on the validation set. This is a reason not to deploy any model trained on this database, since you cannot be sure it won’t drift on new patients. The reviewing processWe are used to hear many scientis complain about the peer review system and I now know why. All the reviewers wanted was for us to cite their articles. They criticised that we didn’t mention this or that article. And after looking at all the proposals we found that they all had the same author in common, what a coincidence! This is a recurring theme, in the other article I have the same happened. It is not something strange. Even the editor in chief advises you not to give up to the citation abuse in their mails. Yet not every request is about citing them, some are about “pinta y colorea”. Little changes about how the figures are made or the colours we use. For instance, if you look at the bayesian network above you will see it is blue. That was a request from the reviewers. Initially it was black and white like in the thumbnail of the post but that change was proposed “To enhance the originality andclarity of the method’s approach”. Worse than that is when the reviewers simply ask you to improve your english. The article has been supervised by more than five people who have been working on this for decades and have several hundreds of articles published and the reviewer still thinks the english is improvable. I sometimes think the reviewers didn’t even read the article and criticised based on the title and little more. ConclusionResearch has its ups and downs, but I like it. The happiness that fills your heart when your method is finally improving and giving good results is very different than any other emotion. It is different than selling a product to a client or developing a new feature in an app. It feels as if you are discovering a new world. Although when the time passes you look back and think “What a useless piece of … did I do there?”. That is progress though, improving means what you did is now obsolete. Many people is afraid of change. I am not one of them. I embrace it. Whenever I have the possibility of learning something new I try. Even if that means learning that all your efforts in the past year are now a commodity. That is compensated by the amazing new thing you just discovered. I hope I will always be discovering new and better technology and science for that is what improves the quality of life of everybody. "
    }, {
    "id": 14,
    "url": "https://granadata.art/gpt4-audiobook/",
    "title": "How to make a full audiobook with AI tools.",
    "body": "2023/10/15 - A few days ago I got access to DALLE·3 and thought, “what if I made a book for children with it?” It may seem a bold idea, but as you will see, it played out decently and that’s the reason for this tutorial. Since there is not yet a full consensus on the copyright of this material, I won’t be posting the full audiobook just in case. But if you follow this tutorial you may end up with a similar copy in your hands. The process is quite simple:  Ask GPT-4 to structure a book around a given topic, which in my case was “animals in Africa”.  Create the figures using DALLE·3.  Give texture to letters using Adobe text effects.  Compose letters and images in powerpoint.  Generate the audio using Tortoise.  Edit video and audio using Davinci Resolve. The objective is to reduce the human in the process as much as possible, although some careful editing and reviewing will always be neccesary. Let’s dive into the details. GPT-4One would think that a lot of prompt engineering is needed to make a full book, even if it is for children, right? Well, no. The prompt I used was simply this:  hey, i want to create a child book for my nephew, he loves animal from africa, like giraffes and lions and pumas. Would you like to help me build a narrative together with the proper animations? Yeah, that’s it! Even with caligraphic errors GPT-4 understood the task and gave me this in response:  Narrative::  Title: “The Great African Adventure”  1. Introduction:In a land far away, where the golden savannah stretches as far as the eye can see, three unlikely friends embark on an unforgettable journey. Meet Gigi the Giraffe, Leo the Lion, and Pippa the Puma.  2. The Lost Waterhole:One day, Gigi notices that their favorite waterhole is starting to dry up. The three friends decide to search for a new one, ensuring they have water during the hot summer days.  3. The Mysterious Map:While resting under an acacia tree, Pippa finds a tattered map blowing in the wind. It hints at a hidden waterhole, surrounded by lush greenery and cool shade. They decide to follow it.  4. Challenges Along the Way:As they journey, they face various challenges. They cross rapid rivers, climb steep hills, and navigate through dense forests. Each friend uses their unique abilities to help the group. Gigi uses her long neck to scout ahead, Leo roars to scare away potential threats, and Pippa’s agility helps them find shortcuts.  5. The Hidden Oasis:After days of searching, they stumble upon a hidden oasis, just as the map had shown. It’s more beautiful than they could have imagined, with fresh, clear water and plenty of food.  6. Conclusion:The trio realizes that their journey was about more than just finding water. It was about friendship, teamwork, and the adventures they shared. They decide to keep the map as a reminder of their great African adventure.  Now, let’s create some illustrations to go with our story. I’ll start by creating images for the introduction and the first two events. Shall we? Not bad, but not a book either, this is more like a summary. So, given that summary, I then asked GPT-4 to create the full book, chapter by chapter. This was my prompt for the introduction:  Okay, let’s be more sistematic. We have 6 chapters. Let’s start with the introduction. Write me a simple story that would fit in 5 pages with one sentence in each page. Also, for each page an illustration is needed. The characters must be the same as before: Gigi the Giraffe, Leo the Lion and Pippa the Puma. And the response:  Introduction: Meeting the Trio:  Page 1:Sentence: “In the heart of the African savannah, three friends stood tall and proud. ”Illustration: A vast savannah landscape with Gigi the Giraffe, Leo the Lion, and Pippa the Puma standing together, the sun setting in the background.  Page 2:Sentence: “Gigi, with her long neck, loved to watch the clouds and dream. ”Illustration: Gigi gazing upwards at the sky, a dreamy expression on her face, as whimsical cloud shapes float above.  Page 3:Sentence: “Leo, with his majestic mane, loved to roar and ensure everyone was safe. ”Illustration: Leo standing on a small mound, letting out a powerful roar, with birds taking flight in the background.  Page 4:Sentence: “Pippa, always curious, loved to explore hidden paths and trails. ”Illustration: Pippa sneaking through some tall grass, her tail flicking with excitement, as she discovers a hidden trail.  Page 5:Sentence: “Together, they were ready for any adventure the savannah had in store. ”Illustration: The three of them walking side by side, with the vast savannah ahead, signaling the beginning of their journey.  Let’s start by creating the illustrations for these pages. Shall we? And this is the process, you tell GPT-4 how many pages you want per chapter and it expands the summary to fit into that number of pages. Even better, it suggests some illustrations, let’s see how to make them a reality. DALLE·3A few months ago, to achieve the illustrations one would have to take the prompt suggested by GPT-4 and take it to Midjourney, or Stable Diffusion to generate a valid image. The process required a lot of trial and error and a bit of prompt engineering. Now, however, DALLE·3 is embedded inside GPT-4, so you just have to say “yes” and it will generate the illustrations, like this:                                 Since it is limited to generating four at a time you need to generate them in batches, which is a minor problem since you can just say “continue” and it will do the rest. As you can notice, even though I did not specify any specific style or artist, these images do not seem to be copyright-free. Some of them resemble too much to Disney style or to other cartoons. Technically, OpenAI gives you ownership of this images to do whatever you want, commercial or not. But it is not clear to me that you are really allowed to do whatever you want. The problem is, even if I wanted to credit the authors of this material, or ask for permission to cite, or even request a commercial license, I don’t know who is the author I should be asking permission for. Anyway, the process is as simple as I described here and it is more or less automatic. In many cases the algorithm does not generate what I ask it for, but by repeating the question several times you can arrive to a satisfying image almost always. Next step, generating covers for every chapter. This is a bit more challenging since generating letters in images is a quite difficult task, and it fails horribly most of the time. For instance, generating the words “The Hidden Oasis” seems an impossible task, here are some examples of the failure cases:                                                 With a lot of trial and error I could manage to get cover for all of the chapters, but re-generating the illustrations to include extra text on them seemed and impossible task. For that reason I decided to edit the text in Adobe Firefly. Adobe text effectsI could have just used plain colors for the text, but it seemed too boring, so I decided to try this AI tool. By using a prompt like “moon” you could add texture to your letters based on it, although the result is not quite there yet in my opinion: For every prompt, no matter what I told to the tool it always gave me dark images. I surrendered and decided to apply a gain to the result if the background was also dark. Nonetheless, I used this tool for all the images because I found it funny, look, there is another example: This one is a bit more special, because I didn’t use any original prompt, I just used the whole sentece provided by GPT-4 “The oasis was a paradise, with crystal-clear water and an abundance of food. ”. This way everything is automatic, you use the sentence as prompt to give texture to itself. The biggest limitation is that the tool only allows a few characters at a time, which made the process quite long and exhausting. The most consuming part was realizing that the text was not even useful because the color didn’t match the scene. This is probably the worst part of the book and the part that has the most room for improvement in terms of AI tools. TortoiseGiven the catastrophic failure that the letters were, we need to fix it somehow. A possible solution for an unreadable text is to create an audiobook. Can’t you read the text? No problem, here is somebody reading it for you. Well, more like something. Because we are going to use another AI tool to generate the voices automatically. This tool is not deployed on any pages like the previous tools. If you want to use it you will need to install it in your machine or in colab. In their github repository you have instructions on how to do it. You will need an NVIDIA GPU with CUDA capabilities or a mac with Apple Silicon or a lot of time, you choose. In my case I have a Mac M1 Max and the generation of all the 33 audios took one and a half hour. As an example, the following command generated the audio that is below: 1PYTORCH_ENABLE_MPS_FALLBACK=1 python tortoise/do_tts. py --text  \[I am really excited,\] Let's follow the birds; they'll lead us to water.   --voice train_dreams --preset fast --output_path book/chapter2/audio4/ --candidates 1 --seed 210501;    Your browser does not support the audio element. The quality is decent. Its main limitation is the generation of emotions. As you can see in the example above, it is possible to trick the algorithm into expressing emotions, but there is much room for improvement and you need to specify which emotion you want. You could probably use GPT-4 to automatically identify which emotions to give, but for this side project I didn’t want to overcomplicate things. Davinci ResolveFinally, let’s put everything together and generate the audiobook. We have images, text and audio. So, we can create a video. In my case I decided to compose images and text in powerpoint and then use a predefined animation to pass between pages. Then, I put the presentation in automatic mode giving each slide 8 seconds and recorded the screen. After that, I had to manually synchronize audio and video by cutting and moving segments. Here is the timeline before editing: And here is after editing: Hit render and you are done. Final thoughtsThe whole process took me 3 days, but I was experimenting the most part. If this was fully automated it could be done in 1 day. Imagine, a editorial could put a hole office of, let’s say, 20 employees and create 400 books in a month. Even more, if the process gets even more automatic, a child could read a different book each day of its whole life, or at least until this kind of books bore them. Nobody can deny this technology is going to have a huge impact in ours lives. Either positive or negative is still to be seen. For me, this can have a good impact since it will democratize knowledge even more. Not only that, people will be able to create masterpieces, books, films and many more. You will no longer need a huge budget to do this kind of things. Indie creators could compete with huge studios or editorials. It will all depend on what the legislation will say. If future (or present) politicians decide against this technology, that democratization will never arrive. Big fishes will remain big, and monopolies will win. As much as I believe this technology is capable of wonderful creations, I also believe that politicians are capable of awful laws. Time will say. "
    }, {
    "id": 15,
    "url": "https://granadata.art/WebApp-Docker-Django/",
    "title": "Deploying AWS webapp tutorial",
    "body": "2023/07/12 - Following the philosophy of my blog, this will be a very specific post. You can find many resources on the internet about how to deploy a web app. I will even be referencing many of those here. But the main difference with those posts is that mine is going to be straight to the point. The following is a tutorial on how to create a web app from scratch. The backend will be on Django and the database will be Postgresql. The app will be running on AWS and to deploy it there we will create a Docker image. Last but not least, I’ll explain how to buy a domain and link the domain to the AWS ip address. Let’s get ours hands dirty! Also, don’t worry about the code, it is all available here. Django and PostgresqlLet’s start by creating a minimal python environment with just Django. You can do it either via python or conda. For reproducibility, please use python3. 10 and Django 4. 2. 2. Open a terminal and run the following: 123python3. 10 -m venv . venvsource . venv/bin/activate # For Windows use: . \. venv\Script\activatepip install django==4. 2. 2 psycopg2-binaryFor the Conda installation: 123conda create --name . venv python=3. 10conda activate . venvpip install django==4. 2. 2 psycopg2-binaryPostgresql server setup: The next step is to create a prosgresql server. To install postgresql go to the official page. Once installed, you need to start running the server on your system: 123mkdir /usr/local/var/postgres # Create folder if it does not existinitdb -D /usr/local/var/postgres # Initialize database clusterpg_ctl -D /usr/local/var/postgres start # Start serverThis will start the server and save everything into the /usr/local/var/postgres folder. For Window users, replace pg_ctl and initdb with the path to the pg_ctl. exe and initdb. exe binaries, which may be something similar to  C:\Program Files\PostgreSQL\14\bin\pg_ctl. exe  and use any data directory you want. Once the server is running we need to create a database, for that, you need to run postgres in a terminal and execute the relevant SQL code: 12345psql postgres # Start SQL shellpostgres=# CREATE DATABASE mydatabase;postgres=# CREATE USER myuser WITH PASSWORD 'mypassword';postgres=# GRANT ALL PRIVILEGES ON DATABASE mydatabase TO myuser;postgres=# exit;The commands are self-explanatory, just replace mydatabase, myuser and mypassword with what you deem appropiate. Now you have the database ready to use locally, you can connect to it using any database management system you want, I recommend Dbeaver. The connection is through localhost and port 5432. Later on we will see how to automate this process but for now, this is how it is done locally. Django webapp setup: Given the database, we need a web on top. With the python environment activated, run the following using any name you want: 1django-admin startproject myprojectnameThis will create the basic skeleton for a Django project. We now have to configure the database and create a simple app to store data. Go to settings. py and locate the INSTALLED_APPS variable. Append 'django. db. backends. postgresql', to the list, it should be like this: 123456789INSTALLED_APPS = [  'django. contrib. admin',  'django. contrib. auth',  'django. contrib. contenttypes',  'django. contrib. sessions',  'django. contrib. messages',  'django. contrib. staticfiles',  'django. db. backends. postgresql',]Also locate the DATABASES variable and modify it to contain the information necessary to connect to the posgresql database: 12345678910DATABASES = {  'default': {    'ENGINE': 'django. db. backends. postgresql',    'NAME': 'mydatabase',    'USER': 'myuser',    'PASSWORD': 'mypassword',    'HOST': 'localhost',    'PORT': '5432',  }}Since storing passwords in plain text is normally not a good idea, I recommend you use environment variables for that: 12345678910DATABASES = {  'default': {    'ENGINE': 'django. db. backends. postgresql',    'NAME': 'mydatabase',    'USER': 'myuser',    'PASSWORD': os. environ. get('DB_PASSWORD', ''), # Don't forget to import os    'HOST': 'localhost',    'PORT': '5432',  }}You can now provide the password through environment variables: 123export DB_PASSWORD='mypassword' # Unixset DB_PASSWORD  mypassword  # CMD Windows$env:DB_PASSWORD= mypassword  # PowerShell WindowsFinally, let’s create a simple page. Start by typing: 1python manage. py startapp simpleappNow, modify settings. py to include it by adding it to the INSTALLED_APPS variable: 12345678910INSTALLED_APPS = [  'django. contrib. admin',  'django. contrib. auth',  'django. contrib. contenttypes',  'django. contrib. sessions',  'django. contrib. messages',  'django. contrib. staticfiles',  'django. db. backends. postgresql',  'simpleapp',]After that, we will add a very simple model and view to handle data. Our model will only contain names of users. Go to simpleapp/models. py and add this: 12345class User(models. Model):  name = models. CharField(max_length=50)  def __str__(self):    return self. nameThen, create simpleapp/forms. py and add this: 1234567from django import formsfrom . models import Userclass UserForm(forms. ModelForm):  class Meta:    model = User    fields = ['name']Next, modify simpleapp/views. py to include the following: 123456789from django. views. generic. edit import CreateViewfrom . models import Userfrom . forms import UserFormclass UserCreateView(CreateView):  model = User  form_class = UserForm  template_name = 'user_form. html'  success_url = '/create_userYou also need to create the template under a templates folder, create simpleapp/templates/user_form. html and insert this: 12345&lt;form method= post &gt;  {% csrf_token %}  {{ form. as_p }}  &lt;button type= submit &gt;Create&lt;/button&gt;&lt;/form&gt;Last, we need to link the urls for everything to work properly. Create the file simpleapp/urls. py and write: 123456from django. urls import pathfrom . views import UserCreateViewurlpatterns = [  path('create_user/', UserCreateView. as_view(), name='create_user'),]Go to the main myprojectname/urls. py and edit it to be like this: 12345678from django. contrib import adminfrom django. urls import pathfrom django. urls import includeurlpatterns = [  path('admin/', admin. site. urls),  path('simpleapp/', include('simpleapp. urls')),]Now you are good to go. We can finally start adding rows to the database. For doing so, apply migrations and start the webapp: 123python manage. py makemigrationspython manage. py migratepython manage. py runserverOpen a browser, go to http://127. 0. 0. 1:8000/simpleapp/create_user/ and you will be able to input users’ names. If it is your first time using Django, this is a whole lot, I know. This is a simple example using Django’s class-based views. Things can get very, very complex. The aim of this tutorial is to set up a minimal working webapp. For more information on Django, you can go to their official documentation. Okay, close everything and let’s start our Docker journey. To stop the webapp run ctrl+C and to stop the posgresql server run: 1pg_ctl -D /usr/local/var/postgres stopDockerSetting everything from scratch is time consuming but if you only need to do it once, it is affordable. The problem comes when you want to migrate to other machines or you want to scale. Having to go through all the process above everytime is annoying. As I mentioned before, it would be nice to automate it. That’s when Docker comes into play. It is a way to pack everything up so that it can run on your machine, the cloud or a microwave, if it has Docker installed, of course. A Docker is basically made of a few configuration files that are used to construct an image that does whatever you want, in our case handle data through a web app. Having introduced the concept, let’s build a Docker for our web app. This section is mostly inspired by this other tutorial. Hand over there if you feel curious. Also, I recommend you giving a look at the official Docker beginner tutorial for more information on how to set up Docker and the basics of it. To start, create a Dockerfile inside of the Django project directory. Specify the following information on it: 123456FROM python:3. 10. 2-slim-bullseyeWORKDIR /codeCOPY . . RUN pip install -r requirements. txtYou also need to create a requirements. txt with this: django==4. 2. 2psycopg2-binaryYou could simply install it with pip, but when the project grows you will be thankfull to have it all in a requirements. txt file. So, that’s the container of the webapp. Simple, right? However, we still need to connect it to a posgres database. For that we need to use docker compose to run another container with the database and connect them. For that, create a docker-compose. yml file with the following: version:  3 services: web:  build: .   command: sh start. sh  environment:   - DB_PASSWORD  volumes:   - . :/code  ports:   - 8000:8000  depends_on:   db:    condition: service_healthy db:  image: postgres:14  restart: always  ports:   - 5432:5432  environment:   POSTGRES_DB: mydatabase   POSTGRES_USER: myuser   POSTGRES_PASSWORD: ${DB_PASSWORD}  volumes:   - . /postgres_data:/var/lib/postgresql/data/  healthcheck:   test: pg_isready -U myuser -d mydatabase   interval: 1s   timeout: 10s   retries: 10   start_period: 30sYou will need to substitute myuser and mydatabase with what you like most. To explain a bit what is happening here, we are running a postgres container, then performing healthy checks to be sure the database is running and after that we launch the webapp container. You could provide the password also there, but for security reasons is better to provide it through an environment variable, just like before. The database is stored in the folder postgres_data locally, so that whenever you kill the container you don’t lose the data. The port 5432 is forwarded locally so you can connect to the database from your machine when the container is running and see the data. Wait! We have not finished yet. We need to create the start. sh and modify the myprojectname/settings. py file. The DATABASES variable should look like this: 12345678910DATABASES = {  'default': {    'ENGINE': 'django. db. backends. postgresql',    'NAME': 'mydatabase',    'USER': 'myuser',    'PASSWORD': os. environ. get('DB_PASSWORD', ''),    'HOST': 'db',    'PORT': '5432',  }}The only change is on HOST. It is now set up to db which is the name of the posgres container. And the start. sh script is the following: 123python manage. py makemigrationspython manage. py migratepython manage. py runserver 0. 0. 0. 0:8000It is creating all the migrations needed to set up all the models from Django into postgres. Finally, just run the container: 1docker compose upTo stop it, do ctrl+C and then docker compose rm. That’s it, that’s all you need to do to restart your webapp from anywhere. You can now take your code to any machine and you won’t need to set up posgres, python and django from scratch. Just install docker and run docker compose up. Also, it is a good practice to include a . dockerignore, just like . gitignore. For this simple app I have postgres_data/Dockerfiledocker-compose. yml*/__pycache__/*/*/__pycache__/*/*/*/__pycache__/This way I don’t load any unnecessary files to the container, making it faster. So now that we have everything packed up in our bag, let’s travel. Let’s deploy the web to AWS for others to use it. AWSAmazon Web Services are a way to deploy your code into servers that you don’t need to manage. This way instead of the cost of setting up a whole server, you just pay for the hours used. Nevertheless, you won’t save yourself the cost of configuring everything. Even though configuring AWS may be simpler than configuring a server, it is still an important investment of time. For that, I will provide here the bare minimum to make our webapp work on AWS. You will still need to read the AWS docs extensively, there are many tutorials online, but Amazon keeps changing the interface every so often. The only web that is for sure updated is the AWS official docs page. Account and IAM roles: Before we can start configuring the server, we need to configure an account. For that you will need access keys. You can create access keys for your root account but it is not recommended. AWS recommends that you create role with less permissions than your root account (specially without billing permissions) and to use those access keys. In the past this was made using the Identity and Access Management (IAM) app. Now, it is being migrated to IAM Identity Center. Both methods still work as of this writing but I will explain the second one which is more updated. The following is a reduced version of this tutorial. Go to the AWS console. There look for the IAM Identity Center. Once on the IAM Identity Center you will need to create an user, create a permission set and link both. In the section User click to Add User and fill the neccesary information. Then, on permission sets, click on Create permission set and create the predefined role AdministratorAccess. After that, go to AWS accounts, select the account under root and click Assign users or groups. Select your created user, click next, select the role, click next, review it and click submit. Finally, to activate that user, you must open the mail you provided and register that user with some password. Before you continue, go to Dashboard and save your AWS access portal URL, that is the URL you need to use to log in with that user. Now, click that URL and sign in. Once you are logged in you should see your user and two links at the right: one for Management console and one for Command line or programatic access. Click the latter and you will see your access keys. The next step is to install and configure the AWS CLI. Go here and follow the steps for the installation. Once installed execute aws configure and provide the access key and secret access key obtained previously. It will also ask for a region, I will be using us-east-1. If you choose a different one you may encounter problems later on because the free tiers differ across regions. And for the output format choose json. You are now (almost) ready to start launching instances. EC2 Instances: Having created our account it is time to create an instance where to deploy our webapp. If your page gets too large you may be interested in storing the database in S3 buckets, but for now I will store code and data in the same instance. You can find the docs for EC2 here. As before, I will summarize it to just use what we need. First, create the instance. For that go to the EC2 console. Under Instances section, click Launch instances. There give it a name, select the OS and arch (I recommend Ubuntu and x86-64), select the instance type (I will be using t2. micro cuase it is free), select a key-pair or create since you probably don’t have one and leave everything as default. Once you launched it you can now access your machine through ssh. In the instances section, you can click on your created instance and then click on Connect and it will give you instructions on how to connect. The next steps are to install Docker, copy your webapp to the instance, change the firewall of the instance to allow http and postgres traffic and finally deploy the app. Installing Docker (again): If you have chosen Ubuntu as your OS you can follow the instructions here. You just basically need to execute the following commands after accessing the machine: 1234567891011sudo apt-get update -ysudo apt-get install ca-certificates curl gnupg -ysudo install -m 0755 -d /etc/apt/keyringscurl -fsSL https://download. docker. com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker. gpgsudo chmod a+r /etc/apt/keyrings/docker. gpgecho \  deb [arch= $(dpkg --print-architecture)  signed-by=/etc/apt/keyrings/docker. gpg] https://download. docker. com/linux/ubuntu \  $(. /etc/os-release &amp;&amp; echo  $VERSION_CODENAME )  stable  | \ sudo tee /etc/apt/sources. list. d/docker. list &gt; /dev/nullsudo apt-get update -ysudo apt-get install docker-ce docker-ce-cli containerd. io docker-buildx-plugin docker-compose-plugin -yAnd if you want to check that everything is working just do 1sudo docker run hello-worldUpload your code to EC2: You can copy your entire directory recursively into you EC2 instance with scp: 1scp -r -i YOUR_KEY . /* ubuntu@YOUR_EC2_ADDRESS:. YOUR_KEY is the key pair previously created and YOUR_EC2_ADDRESS may look something like ec2-30-29-46-221. compute-1. amazonaws. com. It is the same address that you use to ssh into your machine. Changing the firewall: In AWS, the instances have some security rules that control the inbound and outbound traffic of your app. By default all the ports are closed except for 22 which is the ssh port. We will need to open the port 80 and 5432 since they are the http and postgres ports. If you have an SSL certificate you could open the port 443 for https, but we will just use those two for now. Let’s go back to the EC2 console and go to Security Groups. Click on Create security group. Give it a name and add two inbound rules. You can just select HTTP and Postgresql in the dropdown menu and it will set the port for you automatically. Then, on source, select Anywhere IPv4 and click Create security group. Now go back to your created instance and click Actions &gt; Security &gt; Change security groups. There simply add the newly created security group and you are free to go. Deploy the app: In order to deploy our app we need to make one change to our docker-compose. yml. Initially we were redirecting port 8000 into 8000, we are now going to redirect it to 80 which the http port. The line to change will end up like this ports: - 80:8000Finally, ssh into your machine with docker installed and execute 1sudo DB_PASSWORD=. . . docker compose up -dRemember that you have to specify the password of the database as a environment variable. Okay, the app is running but, how can we access it? Well, we cannot. We still need to make some changes. Stop the container and let’s finish this: 1sudo DB_PASSWORD=. . . docker compose downThe first thing to know is what is the IP that we can use to access this page. In the AWS console, when you enter your instance it displays somewhere “Public IPv4 address”. That is the IP of your app. However, if you were to enter there, Django will not let you in. That is because you need to allow that host. For that, go the setting. py of your app and add it: 1ALLOWED_HOSTS = ['YOUR_IP']Also, even after changing this, when you access your ip you don’t see the page. That is because the base url is not pointing anywhere, but we can fix that. Create a view that only has the redirection: 1234from django. shortcuts import redirectdef redirect_to_create_user(request):  return redirect('/simpleapp/create_user')Then, in your main urls. py add path('', redirect_to_create_user), it will end up like this: 12345678910from django. contrib import adminfrom django. urls import pathfrom django. urls import includefrom . views import redirect_to_create_userurlpatterns = [  path('', redirect_to_create_user),  path('admin/', admin. site. urls),  path('simpleapp/', include('simpleapp. urls')),]Now, copy again all the files into your machine and deploy the webapp: 1sudo DB_PASSWORD=. . . docker compose up -dAccessing the DataBase: Let’s see how we can access the server database locally. Open you favourite DB program (mine is DBeaver) and create a new connection. This time you will have to provide an URL instead of localhost. Everything else is the same as when you did it locally. The port is 5432, the user is what you gave it, and the database name is what you name it. If you have configured properly the EC2 security group you could now access your database locally. Web DomainNice, we have our fantastic webapp up and running, but wait, are you going to share to your friends the page 50. 283. 48. 100? Obviously not, you need a fancy domain like myawesomepage. com or something that describes your project. To achieve that you need to first buy a domain and then link that domain to your IP. Domains that are not on high demand typically cost around 10$ to 20$. You can buy them on Namecheap. Once you have it you need to do several things on the AWS side. You will need to fix the IP so that it doesn’t change, otherwise the DNS redirection will get broken over time. After that you need to create nameservers and then route your domain to that IP. Let’s go step by step. To fix the IP go to the section Elastic IP in the left bar of the EC2 menu. Create such Elastic IP and then, in actions, associate it to your instance. Once you have done that, you will need to create the hosted zone. For that, search in AWS the Route 53 service. Once there, click on Create hosted zone. Insert your domain and create it. Before we continue, two more records need to be created. Create one with Type A and your previous Elastic IP under the value section, everything else as default. Repeat now but add ‘www’ in subdomain so that your page can be accessed either by its domain or adding www at the beginning. Once you have done that, go to your domain on Namecheap and click on manage. Select custom DNS and enter the four nameservers that were created previously. If you didn’t understand something, you can check the tutorials I followed both for the AWS and Namecheap part. DNS redirection may take up to 48 hours. There is one last thing to modify, remember that you need to include the IP in ALLOWED_HOSTS? Well, you also need to include your domain there. Change that and you will have your marvelous webpage running. ConclusionCongratulations! You have managed to reach to the end of this tutorial. If you followed the steps carefully you now know how to create your own web apps. The first time you do it is quite tiresome, but once you know how to do it you can get your millionaire idea up and running at the moment. Let’s recap. First, you need to create your Django app. Then, you create a Docker to launch it easily. After that, you create an AWS instance and deploy your app there. Finally, you link your domain to the instance IP. Once you are done you can enjoy your creation! "
    }, {
    "id": 16,
    "url": "https://granadata.art/HMM-padel-part3/",
    "title": "Modelling a padel match with Hidden Markov Models (Part 3)",
    "body": "2022/10/16 - In part I and part II of this series I have talked about what is a Hidden Markov Model, why it is useful for modelling a padel match, and how to design it properly. Today the topic is about testing the model. Testing a machine learning model is the basis for deploying it. Here I will be explaining how to test in python and guide you through our HMM example. PyTest: Testing in python can be done with several libraries. One of the most popular ones is pytest. There are many good tutorials out there about how to use it, like this one. And if you have a project on your hands and have specific needs, you can always go and have a look at their documentation. For this post I will pass very briefly on the funcionalities that we need for testing the HMM. In pytest you have files that contain the word ‘test’ on it. Those files contain functions that also have the string ‘test’ in their names. And those functions must have code that yields True of False depending on whether they pass the test. To run the tests you just execute pytest on the command line and the library does the rest of the job for you. This is the big picture, let’s define now what are going to be our tests. Our tests are going to have a sequence of observations as input and we are going to check for the result of the match. That is the final goal of the HMM. The difference is that here we are going to deal with simple scenarios where we know for sure the result. That way we can check that the HMM is at least well implemented. We cannot check if the design is going to work in a real-world scenario, but we can check that it works in ideal scenarios. If your model breaks in an environment where you control the input, then something is clearly wrong in the code. However, if it breaks in production it may be that the model hypothesis about the world are wrong. That’s why you need these tests, to detect bugs prior to analysing the correctness of the model itself. Once the tests are designed, it’s time to implement them. For that, pytest comes with two main features that make the process easier. Parameterization: In pytest you can parameterize the input. When testing a function, several input-output pairs are used to ensure that the function produces the desired result. Using a different file for each input-output pair would be an inconvenience. Instead, you can specify several input-output pairs for a given test. Consider the following test where we are interested in knowing if the HMM correctly detects an ace: 123456def test_ace(sequence, indexers, hmm, hidden_states):  indexer_hidden, indexer_obs = indexers  sequences = [[indexer_obs[obs] for obs in sequence]]  decoded_seq = hmm. decode(sequences)  decoded_seq = [hidden_states[idx] for idx in decoded_seq[0]]  assert('Point-server' in decoded_seq)The parameter sequence is a list of observations that corresponds to an ace. Without parameterization we would have to specify a different test function for each sequence that represents an ace. With parameterization we can reuse the function. In pytest that features is implemented with decorators. For those of you who don’t know python enough, a decorator is basically a function of functions. Here it takes the test function as input and creates a parameterized test function as output. Syntactically it is coded like this: 123456789@pytest. mark. parametrize( sequence , [  ### Ace examples  ])def test_ace(sequence, indexers, hmm, hidden_states):  indexer_hidden, indexer_obs = indexers  sequences = [[indexer_obs[obs] for obs in sequence]]  decoded_seq = hmm. decode(sequences)  decoded_seq = [hidden_states[idx] for idx in decoded_seq[0]]  assert('Point-server' in decoded_seq)You only have to add the decorator on top of the function. If you recall the problem at hands, which sequences could possibly represent an ace? Let’s see some examples: 12345678910111213141516171819### ace in first service ###['player-hit', *['flying'] * 10, 'bounce-ground-receiver', *['flying'] * 10,   'bounce-ground-receiver', *['flying'] * 100, 'end'],  # Bounce on wall['player-hit', *['flying'] * 10, 'bounce-ground-receiver', *['flying'] * 10,  'bounce-wall-outer', *['flying'] * 10, 'end'],   # Bounce on wall twice['player-hit', *['flying'] * 10, 'bounce-ground-receiver', *['flying'] * 10,  'bounce-wall-outer', *['flying'] * 10, 'bounce-wall-outer', *['flying'] * 10, 'end'],### ace in second service ###  # Bounce on wall['player-hit', *['flying'] * 10, 'bounce-ground-server', *['flying'] * 10,   'player-hit', *['flying'] * 10, 'bounce-ground-receiver', *['flying'] * 10,   'bounce-wall-outer', *['flying'] * 10, 'end'],  # Bounce on wall twice['player-hit', *['flying'] * 10, 'bounce-ground-server', *['flying'] * 10,   'player-hit', *['flying'] * 10, 'bounce-ground-receiver', *['flying'] * 10,   'bounce-wall-outer', *['flying'] * 10, 'bounce-wall-outer', *['flying'] * 10, 'end']Basically, whenever there are two consecutive bounces on the other side after the first player has hit the ball we have an ace. When an ace happens the server wins, therefore we have to check that the last hidden state correspond to the server winning. If that doesn’t happens, then our HMM isn’t working. Passing this test doesn’t prove that the HMM works, but by adding more and more tests we at least know that our model is robust to all those cases. Fixtures: Have you wondered how do you pass parameters to a test? In the previous section I talk about parameterizing with a decorator. But what about the rest of the parameters? Not every parameter is part of the input. There are parameters that are part of the function itself. For instance, the parameter hmm. How does pytest know where to look for that object? At the beginning I said that you just have to run pytest in the command line. You don’t specify parameters to the internal test functions directly. Instead, you use fixtures. A fixture is another decorator provided by pytest. In this case, you decorate a function that returns the parameter you want to use later on. Let’s look at an example by specifying the fixture for the hmm object. Suppose that you have a function in your code (not your test, your real code) that initialized the hmm and returns it. Then, you would convert that function to a fixture this way: 123@pytest. fixture()def hmm():  return read_hmm()That’s everything you have to do in order for every other function to know where to look for the hmm object. The same would be needed for the indexers and hidden_states that in my case are just dictionaries to convert from strings of states to the internal identifiers that the HMM uses. Noisy tests: To end this post I’ll show you some concrete tests I designed for my HMM. They are a bit different than the rest of the tests. When evaluating the HMM I said that we give ideal scenarios as input to the tests. But it is possible to give noisy scenarios too, if you control the noise. There are no rules for writing tests, they just serve to check that your code does what you want it to do. And if I want my HMM to be robust to noise, I can test for it. When I talk about noise in this problem it would be missing some observation or having repeated observations for the same hidden state. We designed our model so that it would work even on those cases. So if I provide a series of noisy observations, it must predict correctly the result. For example, in the following code there is the test for two cases where the server fails on the second service. However, the first hit is repeated and some bounces are repeated too. This series of observations doesn’t correspond to an ideal one, but the model should correctly predict the result. 1234567891011121314151617@pytest. mark. parametrize( sequence , [  ### Fail in second service ###  # Bounce in and then on inner wall  ['player-hit','flying', 'flying', 'player-hit', *['flying'] * 13,   'bounce-ground-server', 'flying', 'bounce-ground-server', *['flying'] * 9,   'player-hit', *['flying'] * 10, 'bounce-ground-receiver', *['flying'] * 4,   'bounce-wall-inner', *['flying'] * 10, 'end'],   # Bounce out  ['player-hit', *['flying'] * 15, 'bounce-ground-server', *['flying'] * 10,   'player-hit', 'player-hit', *['flying'] * 12, 'bounce-ground-server', *['flying'] * 10, 'end']])def test_fail_noise(sequence, indexers, hmm, hidden_states):  indexer_hidden, indexer_obs = indexers  sequences = [[indexer_obs[obs] for obs in sequence]]  decoded_seq = hmm. decode(sequences)  decoded_seq = [hidden_states[idx] for idx in decoded_seq[0]]  assert('Point-receiver' in decoded_seq)You can also have tests that don’t pass on purpose. I call this one ‘impossible_noise_test’: 1234567891011121314151617    Test designed to make the model fail    @pytest. mark. parametrize( sequence , [  [*['flying'] * 3, 'bounce-ground-server', *['flying'] * 3,  'bounce-ground-server', *['flying'] * 3,'player-hit', # Bounces in the ground badly detected as player-hit  *['flying'] * 3, 'bounce-ground-server', # Badly detected bounce  'bounce-ground-receiver', *['flying'] * 5, 'player-hit', *['flying'] * 4, # Well detected serve  'bounce-ground-server', 'bounce-ground-server', 'bounce-ground-server', # Same bounce   *['flying'] * 5, 'player-hit', # Well detected response  *['player-hit', *['flying'] * 5]*10, # Normal rally (now the ball is for receiver)   'bounce-ground-receiver', 'end'] # It goes to net and out])def test_impossible_noise(sequence, indexers, hmm, hidden_states):  indexer_hidden, indexer_obs = indexers  sequences = [[indexer_obs[obs] for obs in sequence]]  decoded_seq = hmm. decode(sequences)  decoded_seq = [hidden_states[idx] for idx in decoded_seq[0]]  assert( Point-server  in decoded_seq)This type of test allows you to know the limits of the model. There must be a limit, and if you don’t manage to create a test that fails, that is also and indicator that something is wrong. Maybe you are not creative enough for the cases, or you have some bug that makes all the test pass (that has happened to me). So it is also a good idea to have a test that fails, just in case. Conclusion: With this post the Hidden Markov Model for Padel Modelling series comes to an end. We have learned everything to deploy a HMM. From the theory behind the model, till testing the model extensively. Those are the steps to put any machine learning model into production. Learning the basics, designing the model and testing the model. The only thing left is to create a pipeline and integrate it into the final product, but that is a topic for another day. I hope you liked the process as much as I did. See you on my next series. "
    }, {
    "id": 17,
    "url": "https://granadata.art/HMM-padel-part2/",
    "title": "Modelling a padel match with Hidden Markov Models (Part 2)",
    "body": "2022/09/16 - In my last post I explained the usefulness of Hidden Markov Models for predicting the outcome of a padel match with only a few observations. There I also showed how easy it was to implement everything in Python, but I left the most important part: the HMM itself. Today we are going to learn how to design a HMM to predict the result of a point. This is going to be an iterative process. The final model, as you will see, is a monstruosity. But step by step we are goint to build it succesfully. “Rome wasn’t made in a day”. Inspiration and design: Before diving into the details, let me explain how I tackled this problem. I think the creative process is worth mentioning. If you just want the details, you can skip this section. Before starting to code or thinking on my own for the solution of a problem I always research for similar problems that are already solved so that I can get some inspiration. In this case, it turned out that somebody had already designed a HMM for tenis. In this article, they present a HMM for following the state of a tennis match. The observations in this article were poorly defined, but the hidden states wer very clear. I could get an idea of what I had to do by just looking at this image.  This graph is just depicting visually the rules of tenis. In our case we just have to show the rules of padel with a similar graph. If you look carefully you will see that the white boxes represent hidden states with an obvious observation. For padel there will be more of them because apart from bouncing on the ground, the ball can bounce on the walls, which makes the graph more complex but the idea is the same. Another helpful diagram on the same article represented the same graph but organised in several subgraphs.  What’s interesting here is that those four subgraphs will be the same for padel. The high-level view of the process (top) is exactly the same. That’s part of the job which is already done. We just have to change the internal representation between those four subgraphs and maintain the interconnections. What about designing graphs? What is the software I used for that? You may say. Well, it is actually a pen and a lot of papers. No matter how well developed graph visualizations software are, drawing a simple graph by hand will always be faster than coding it. Of course, when the project keeps growing and the graphs becomes massive you will need those software tools which I will mention later. But at the beginning, just take a pen and start drawing. In the other sections I will present diagrams made with a computer because they are visually more pleasant and you will understand them better. Nevertheless, here is one of the graphs I painted by hand, in case you are curious.  The hidden states’ graph: For a HMM we need the transition and emission matrices. The transition matrix is going to be the adjacency matrix of the transitions graph. That graph is simply a representation of the rules of the game. I am going to distinguish two main parts in that rules. The rules for the serve and the rules for the normal game. The reason for creating two distinct graphs is because the effect of the ball going out or touching the net is different at the beginning. Serve: What happens when a player is on their serve? It can go in, it can go out or it can touch the net. And if it touches the net, it can then go in or out. Let’s ignore when it goes in without touching the net for the moment. How would you represent the 1st serve? Like this? Did you think about the init state? Remember, this model is a realistic one. In practice you don’t know when is the point starting. Therefore, you need a special state for waiting until you have enough evidence that the match has begun. If you look closely, you will see that there is a self-loop on the init state. That is how we represent a waiting state in a HMM, by a self-loop. Now, let’s pass to the 2nd-serve. How would you design it? Exactly the same as the first service. The simpler, the better. We are ignoring when the ball goes in and the game continues. We are just focusing on when the ball goes out or to the net. The rest of the details will be added later on. For now let’s just focus on what we have. We have a graph with several nodes, each representing a state. What do we need? Consistent labels across the whole graph. One of the limitations of the HMM is that you have to fulfill the Markov property. Which means that the state representing going out after the 1st-serve is different than the state of going out after the 2nd-serve. So they need different names. In my case, I just added a suffix number when that happened. That way, going out in the first service is ‘out1’ and after the second is ‘out2’. Another state that repeats a lot across the graph is the ‘in’ state. For that one adding a suffix number is not enough, so I added a more descriptive suffix. For instance, going in after touching the net in the first serve is ‘in-net1’. This is a decision that could have been made of many ways, but I decided to make it like this. Okay, let’s now talk about what happens when the ball actually goes in and the game continues. To keep it simple, let’s focus on what happens before any player hits the ball. And let’s call this the ace model. As the name states it, one of the things than can happen is an ace. What characterizes an ace? The fact that the ball touches the ground again before any player hitting it. Try to draw the scheme for the ace model. Keep in mind that before touching again the ground it can hit the walls. And also bare in mind that there are two types of walls. What are the connections among those states? Which combinations are valid and which not? Here is my solution for that problem, omitting the connections to the states Point-server and Point-receiver representing the end of the game.  Did you thought of the ‘time-out’ state? Again, this is a real model so it has to deal with real problems. And one of them is that you miss the observation that characterizes the end of the game. If that happens you can only know the game has finished by time. That’s why you need a state to represent the end of the game by time. Later on when defining the emissions it will be made more clear why this state is needed. Most of the extra states are created so that when an observation is wrong, there is still a path in the graph to the end. Otherwise, the model will give an error and doesn’t return anything. Observe also that there are two ‘in’ states. Can you imagine why? The state ‘in1’ is when the ball goes in on the first serve, and ‘in2’ on the second. Since we have to maintain the Markov property, those two states are different although they have the same emissions and are identical to us. Before going to the next block, which is when a player hits the ball and the game actually starts, let me explain how I made this pictures and how you can code graphs on Python. With the library networkx you can do many thing on graphs, it has implemented almost every algorithm that exists related to graphs. In this project we only use it to define the graphs. The syntax is pretty straighforward, you define a DiGraph which stands for directed graph, and add nodes and edges with the functions add_nodes_from and add_edges_from. After that, you can save the model in gml format and open it with Gephi. That’s it. Here is the code for the three models presented above. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import networkx as nxfolder_path = 'graphs/'    First serve model    first = nx. DiGraph()first. add_nodes_from([  ( init , { hidden : True}),  ( 1st-serve , { hidden : True}),  ( net1 , { hidden : True}),  ( out1 , { hidden : True}),  ( in-net1 , { hidden : True}),])first. add_edges_from([  ( init ,  1st-serve ), ( init ,  init ),  ( 1st-serve , net1 ), ( 1st-serve , out1 ),  ( net1 , in-net1 ), ( net1 , out1 ),  ( in-net1 , 1st-serve )])nx. write_gml(first, folder_path + 'first. gml')    Second serve model    second = nx. DiGraph()second. add_nodes_from([  ( 2nd-serve , { hidden : True}),  ( net2 , { hidden : True}),  ( out2 , { hidden : True}),  ( in-net2 , { hidden : True}),])second. add_edges_from([  ( 2nd-serve , net2 ), ( 2nd-serve , out2 ),  ( net2 , in-net2 ), ( net2 , out2 ),  ( in-net2 , 2nd-serve )])nx. write_gml(second, folder_path + 'second. gml')    Ace model    ace = nx. DiGraph()ace. add_nodes_from([  ( in1 , { hidden : True}),  ( in2 , { hidden : True}),  ( time-out , { hidden : True}),  ( ground , { hidden : True}),  ( wall-outer1 , { hidden : True}),  ( wall-outer2 , { hidden : True}),  ( wall-inner1 , { hidden : True}),  ( wall-inner2 , { hidden : True}),])ace. add_edges_from([  ( in1 ,  time-out ), ( in1 ,  ground ), ( in1 ,  wall-outer1 ), ( in1 ,  wall-inner1 ),  ( in2 ,  time-out ), ( in2 ,  ground ), ( in2 ,  wall-outer1 ), ( in2 ,  wall-inner2 ),  ( wall-outer1 ,  time-out ), ( wall-outer1 ,  ground ), ( wall-outer1 ,  wall-outer2 ),  ( wall-outer2 ,  time-out ), ( wall-outer2 ,  ground ),])nx. write_gml(ace, folder_path +  ace. gml )Gephi has some handy features that make posible visualize big graphs. Concretely, you can use the Force Atlas distribution to reorder the nodes by simulating forces proportional to the number of edges they have. It has many parameters you can try, but I normally just click on execute and wait a few seconds for convergence.  Then, on the previsualization tab, you can create the diagrams I showed you. It has many options, like curved edges. I don’t use that feature for this post because for complex graphs it can be messy. But for some graphs I think is prettier with curvy edges. Other things to adapt are the font size and the size of the arrows. With a bit of practice you can create nice figures quite fast.  Rally: The rally model is a bit more complex than the ones presented above. There are two ways to design it based on which observations you have. If you only have an observation for bouncing on the ground, anywhere, then the rally model only has one ‘HIT’ state and the rest is similar to the ace model. However, in practice you can have more information than that. Suppose you have an image of a game and you know the location of the ball together with the fact that it is a bounce in the ground. You could, potentially, distinguish if the ball is on the server side or on the receiver side. You just need to segment the court and see if the ball is on the upper side or not. This is not trivial, but is possible to achieve. For that reason, we are going to have two ‘HIT’ states: one for the server side and one for the receiver. The rest is just identical to the ace model, with one exception. Instead of ending the game or returning to the beginning, the states point to the other ‘HIT’ states. Similar to what would happen in a match. Your head is going from one side to the other. Here the hidden state is moving from one model to the other. Finally, the diagram.  If you understood the ace model, you just need to focus on the arrows that cross from left to right and vice versa. The rest is just the standard mechanics of a padel game. One more thing to notice is that there are many arrows missing. Concretely, the arrow between the models and the arrows that point to the absorbing states, that is, those which end the game. In the next section, I will try to describe the connections between the models and will show you the (almost) full picture of the HMM transition graph. Also, below is the code for this model. 1234567891011121314151617181920212223242526272829303132333435363738394041424344    Rally model    rally = nx. DiGraph()rally. add_nodes_from([  ( HIT1 , { hidden : True}),  ( net-HIT1 , { hidden : True}),  ( in-HIT1 , { hidden : True}),  ( out-HIT1 , { hidden : True}),  ( time-out-HIT1 , { hidden : True}),  ( ground-HIT1 , { hidden : True}),  ( wall-inner-HIT1 , { hidden : True}),  ( wall-outer1-HIT1 , { hidden : True}),  ( wall-outer2-HIT1 , { hidden : True}),    ( HIT2 , { hidden : True}),  ( net-HIT2 , { hidden : True}),  ( in-HIT2 , { hidden : True}),  ( out-HIT2 , { hidden : True}),  ( time-out-HIT2 , { hidden : True}),  ( ground-HIT2 , { hidden : True}),  ( wall-inner-HIT2 , { hidden : True}),  ( wall-outer1-HIT2 , { hidden : True}),  ( wall-outer2-HIT2 , { hidden : True}),])rally. add_edges_from([  ( HIT1 ,  out-HIT1 ), ( HIT1 ,  in-HIT1 ), ( HIT1 ,  net-HIT1 ),  ( in-HIT1 ,  HIT2 ), ( wall-inner-HIT1 ,  HIT2 ), ( wall-outer1-HIT1 ,  HIT2 ), ( wall-outer2-HIT1 ,  HIT2 ),  ( net-HIT1 ,  in-HIT1 ), ( net-HIT1 ,  out-HIT1 ),  ( in-HIT1 ,  time-out-HIT1 ), ( in-HIT1 ,  ground-HIT1 ), ( in-HIT1 ,  wall-inner-HIT1 ), ( in-HIT1 ,  wall-outer1-HIT1 ),   ( wall-inner-HIT1 ,  time-out-HIT1 ), ( wall-inner-HIT1 ,  ground-HIT1 ), ( wall-inner-HIT1 ,  wall-outer2-HIT1 ),  ( wall-outer1-HIT1 ,  time-out-HIT1 ), ( wall-outer1-HIT1 ,  ground-HIT1 ), ( wall-outer1-HIT1 ,  wall-outer2-HIT1 ),  ( wall-outer2-HIT1 ,  time-out-HIT1 ), ( wall-outer2-HIT1 ,  ground-HIT1 ),  ( HIT1 ,  HIT2 ), ( HIT2 ,  HIT1 ),    ( HIT2 ,  out-HIT2 ), ( HIT2 ,  in-HIT2 ), ( HIT2 ,  net-HIT2 ),  ( in-HIT2 ,  HIT1 ), ( wall-inner-HIT2 ,  HIT1 ), ( wall-outer1-HIT2 ,  HIT1 ), ( wall-outer2-HIT2 ,  HIT1 ),  ( net-HIT2 ,  in-HIT2 ), ( net-HIT2 ,  out-HIT2 ),  ( in-HIT2 ,  time-out-HIT2 ), ( in-HIT2 ,  ground-HIT2 ), ( in-HIT2 ,  wall-inner-HIT2 ), ( in-HIT2 ,  wall-outer1-HIT2 ),   ( wall-inner-HIT2 ,  time-out-HIT2 ), ( wall-inner-HIT2 ,  ground-HIT2 ), ( wall-inner-HIT2 ,  wall-outer2-HIT2 ),  ( wall-outer1-HIT2 ,  time-out-HIT2 ), ( wall-outer1-HIT2 ,  ground-HIT2 ), ( wall-outer1-HIT2 ,  wall-outer2-HIT2 ),  ( wall-outer2-HIT2 ,  time-out-HIT2 ), ( wall-outer2-HIT2 ,  ground-HIT2 )])nx. write_gml(rally, folder_path +  rally. gml )Interconnections: Let’s go model by model, edge by edge, starting by the 1st-serve. It has two connections, one to the ‘in1’ state of the ace model and one to the 2nd-serve. The latter is from the ‘out1’ state, if the ball goes out, you have a second service, that’s the rules. The 2nd-serve model is similar, one connection to the ‘in2’ state and one to the absorbing state ‘Point-receiver’. If you fail your second serve, you lose the point. Simple, concise and clear. Let’s continue with the ace model. The ‘ground’ and ‘time-out’ states point to ‘Point-server’. This is the representation of an ace. If the ball hits the ground twice, it’s an ace and the point goes to the server side. The rest of the connections are to the ‘HIT1’ state, which means that the receiver has hit the ball and the game continues. And that’s it. The only remaining connections are between the rally model and the absorbing states, just like in the ace model. If the ball bounces twice, or if it goes out of the court after bouncing in, the last player wins. If it goes out, the last player loses. For those of you that know how to play padel, this should be no surprise. There are many cases to deal with because the ball can hit the walls and the net. But more or less it is summarized like that. Here’s the full picture.  The code for this part is different. This time we have to join the four different models. To do so we are going to use the function union_all that creates a new graph with all the nodes and edges from before but without any connections between the subgraphs. To add those connections we use the add_edge function and for the absorbing states the add_nodes_from. This is the result. 1234567891011121314151617181920212223242526272829    Graph union plus connection edges    union = nx. union_all((first, second, ace, rally))union. add_edge( out1 ,  2nd-serve )union. add_edge( 1st-serve ,  in1 )union. add_edge( 2nd-serve ,  in2 )union. add_edge( wall-inner1 ,  2nd-serve )    Absorbing states    union. add_nodes_from([  ( Point-receiver , { hidden : True}),  ( Point-server , { hidden : True})])union. add_edge( wall-inner2 ,  Point-receiver )union. add_edge( out2 ,  Point-receiver )union. add_edge( time-out ,  Point-server )union. add_edge( ground ,  Point-server )union. add_edge( out-HIT1 ,  Point-server )union. add_edge( time-out-HIT1 ,  Point-receiver )union. add_edge( ground-HIT1 ,  Point-receiver )union. add_edge( out-HIT2 ,  Point-receiver )union. add_edge( time-out-HIT2 ,  Point-server )union. add_edge( ground-HIT2 ,  Point-server )    Connection between ace and rally    union. add_edge( in1 ,  HIT1 )union. add_edge( in2 ,  HIT1 )union. add_edge( wall-outer1 ,  HIT1 )union. add_edge( wall-outer2 ,  HIT1 )nx. write_gml(union, folder_path +  union. gml )Delays: Previously I said this was almost the full picture. The reason for that is that the model here does not take into account that the ball is flying in between bounces. In an ideal model this would be irrelevant, with just the bounces we can predict the outcome of the game. But in practice that is not true. Do you remember the reason for using a ‘time-out’ state? Here is similar. Imagine you detect the same bounce twice by error. If you only look at bounces you will assume that the game has finished. However, if you detect twice the same bounce, in time they will be very close. In contrast with what will happen if those two bounces are both real. Therefore, if you somehow take into account the time between observations you can solve those kind of errors. The way to do that is by adding ‘flying’ states. In between any two states you include a ‘flying’ state and in the emissions you consider ‘flying’ as a plausible observation. This way you have a way of measuring time. The more ‘flying’ observations you have, the more time that has passed between states. The key here is that the ‘flying’ state has a self-loop. Similar to the ‘init’ state. You don’t know how much time is going to occur between states. For that reason you add a self-loop to stay in that state until there is evidence enough that you are not flying anymore. For this part there is no diagram. As you may have guessed, adding one state for every edge is going to make the model huge and very complicated to deal with. At this step, I mostly work with the code. Adding all the edges by hand is a nightmare. For that reason, I let python do that for me. Before showing you the code, there is one more hypothesis to deal with. I said that the reason for the ‘flying’ state is to correct duplicate observations. But to do that it is needed to add more than one ‘flying’ state per edge. Why? Because the first ‘flying’ states are going to be corrective states without self-loop. It is only the last one that is a waiting state. Those corrective states can emit the same observations as the first state. While the waiting state can only emit the ‘flying’ observation. The reason for this is mostly empirical. Using only one ‘flying’ state yielded unsatisfactory results. In my experiments I ended up using five ‘flying’ states: four correctives, and one waiting. Finally, the code for that. 123456789101112131415    Add flying states    current_edges = list(union. edges)fly_err_len = 5for (u,v) in current_edges:  if u ==  init :    continue  union. add_nodes_from([( flying- +u+'-'+str(k), { hidden : True}) for k in range(fly_err_len+1)])  union. remove_edge(u, v)  union. add_edge(u,  flying- +u+ -0 )  union. add_edge(u,  flying- +u+ - +str(fly_err_len))  for k in range(fly_err_len):    union. add_edge( flying- +u+'-'+str(k),  flying- +u+'-'+str(k+1))    union. add_edge( flying- +u+'-'+str(k),  flying- +u+'-'+str(fly_err_len))  union. add_edge( flying- +u+'-'+str(fly_err_len),  flying- +u+'-'+str(fly_err_len))  union. add_edge( flying- +u+'-'+str(fly_err_len), v)And the last part of the code is to convert the ‘Point-server’ and ‘Point-receiver’ states into waiting states. The reason for this is numerical. When creating the transition and emission matrices the values need to be normalized. If you don’t add this connections you have rows with all zeros that give errors and it is easier to solve them like this. Those edges are added after creating the ‘flying’ states because those edges don’t need any ‘flying’ states attached to them, they are simply a trick for the computation. 123    Self-loops for absorbing states    union. add_edge( Point-receiver ,  Point-receiver )union. add_edge( Point-server ,  Point-server )The observations’ graph: First, let’s define the observations. 123456789101112    Possible observations    obs = nx. DiGraph()obs. add_nodes_from([  ( player-hit , { hidden : False}),  ( bounce-ground-receiver , { hidden : False}),  ( bounce-ground-server , { hidden : False}),  ( bounce-net , { hidden : False}),  ( bounce-wall-inner , { hidden : False}),  ( bounce-wall-outer , { hidden : False}),  ( flying , { hidden : False}),  ( end , { hidden : False})])This is all we can observe, at least automatically with a camera. We can observe the ball bouncing anywhere: in the walls, in the net, or in the ground. And we can observe any player hitting the ball. Notice that we don’t have to distinguish which player hits the ball, that job is done by distinguishing where is the ball bouncing. The reason for that is that it is quite difficult in practice to detect when a player hits the ball and which player it is due to projection. With just one observation representing all the player is enough to solve the problem. Keep in mind that this is for real cases, and the model has to reflect the limitations of the detections. There is one special observation called ‘end’. The HMM presented here can only deal with isolated points. The ‘end’ observation is only emitted by the absorbing states. It is a way of forcing the HMM to find a solution. When dealing with more than one point it is needed to detect when the point has finished. As with the ‘flying’ states, I didn’t add all the emissions by hand. There are a lot of nodes in the transition graph. And the emission graph is a bipartite graph with transitions on one side and emissions on the other. That is a lot of edges. But in the end, is no more than a regex problem. The states have descriptive names. Any state with ‘ground’ in their name is going to emit either ‘bounce-ground-receiver’ or ‘bounce-ground-server’. And the ‘flying’ states emit the ‘flying’ observation. There is no fancy ideas here, just nasty work. I leave here the code for you. There are better ways to code this for sure, but this works and it’s mine, so I like it. There are two more things to mention. The flying probability and the missing probability. As I said before there are corrective states. Those corrective states can emit the same observation as the state they are attached to it, but with a smaller probability. Otherwise, if the probability isn’t lower, we are not taking into account the fact that the more separated two observations are, the more likely they are to be correct. Thus, the corrective ‘flying’ states have some probability of emit ‘flying’ and some probability of emiting other things. The missing probability is similar but is for the other states. If instead of a duplicate you miss an observation, the graph still needs to find a path to the end. For that reason every state has some little probability of emitting ‘flying’. And there are many other little changes that were added in the process of creating this matrix. The justification behind most of the strange things you will see in the code is empirical. You start with a simple model and find a case where it doesn’t work, change the model and repeat. After several iterations you arrive at this. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576    Create bipartite graph representing emissions    G = union. copy()G. clear_edges()U = nx. union(G, obs)flying_prob = 0. 9err_miss_prob = 0. 001eps = 1e-12for u in G. nodes():  if  Point  in u:     #U. add_weighted_edges_from([(u,  flying , 1)])    U. add_weighted_edges_from([(u,  end , 1)])    continue  if  flying  in u and  init  not in u:    U. add_weighted_edges_from([(u,  flying , flying_prob)])    U. add_weighted_edges_from([(u,  bounce-ground-server , eps)])    U. add_weighted_edges_from([(u,  bounce-ground-receiver , eps)])    if fly_err_len &gt; 0 and fly_err_len == int(u. split('-')[-1]):      continue  elif u not in [ net1 ,  net2 ,  wall-inner1 ,  wall-inner2 ]:    U. add_weighted_edges_from([(u,  flying , err_miss_prob)])  if  init  in u:    U. add_weighted_edges_from([(u,  bounce-ground-receiver , (1-flying_prob)/5)])    U. add_weighted_edges_from([(u,  bounce-ground-server , (1-flying_prob)/5)])    U. add_weighted_edges_from([(u,  bounce-wall-inner , (1-flying_prob)/5)])    U. add_weighted_edges_from([(u,  bounce-wall-outer , (1-flying_prob)/5)])    U. add_weighted_edges_from([(u,  bounce-net , (1-flying_prob)/5)])     # U. add_weighted_edges_from([(u,  player-hit , eps)])     U. add_weighted_edges_from([(u,  flying , flying_prob)])   elif  serve  in u or u ==  HIT1  or u ==  HIT2  or  flying-HIT1-  in u or  flying-HIT2-  in u:    U. add_weighted_edges_from([(u,  player-hit , 1-flying_prob)])    U. add_weighted_edges_from([(u,  bounce-ground-receiver , eps)])    U. add_weighted_edges_from([(u,  bounce-ground-server , eps)])    U. add_weighted_edges_from([(u,  bounce-wall-inner , eps)])    U. add_weighted_edges_from([(u,  bounce-wall-outer , eps)])  elif  time  in u:    U. add_weighted_edges_from([(u,  flying , 1)])  elif  wall  in u: # this must be before in and out    if  inner  in u:      U. add_weighted_edges_from([(u,  bounce-wall-inner , 1-flying_prob)])    elif  outer  in u:      U. add_weighted_edges_from([(u,  bounce-wall-outer , (1-flying_prob)/2)])      if  HIT1  in u:        U. add_weighted_edges_from([(u,  bounce-ground-server , (1-flying_prob)/2)])      elif  HIT2  in u:        U. add_weighted_edges_from([(u,  bounce-ground-receiver , (1-flying_prob)/2)])    else: assert(False)    U. add_weighted_edges_from([(u,  player-hit , eps)])  elif ( in  in u or  ground  in u) and  flying-net  not in u and  flying-out  not in u:    if  in1  in u or  in2  in u or  in-HIT2  in u or  ground-HIT2  in u\      or  in-net1  in u or  in-net2  in u:      U. add_weighted_edges_from([(u,  bounce-ground-receiver , 1-flying_prob)])      if  ground-HIT2  in u:        U. add_weighted_edges_from([(u,  bounce-ground-server , eps)])    elif  in-HIT1  in u or  ground-HIT1  in u:      U. add_weighted_edges_from([(u,  bounce-ground-server , 1-flying_prob)])      if  ground-HIT1  in u:        U. add_weighted_edges_from([(u,  bounce-ground-receiver , eps)])    elif  ground  in u:      U. add_weighted_edges_from([(u,  bounce-ground-receiver , 1-flying_prob)])      U. add_weighted_edges_from([(u,  bounce-ground-server , eps)])    else: assert(False)    U. add_weighted_edges_from([(u,  player-hit , eps)])  elif  out  in u:    if  out-HIT1  in u:      U. add_weighted_edges_from([(u,  bounce-ground-receiver , (1-flying_prob) / 4)])    elif  out-HIT2  in u or  out1  in u or  out2  in u:      U. add_weighted_edges_from([(u,  bounce-ground-server , (1-flying_prob) / 4)])    U. add_weighted_edges_from([(u,  bounce-wall-inner , (1-flying_prob) / 4)])    U. add_weighted_edges_from([(u,  bounce-wall-outer , (1-flying_prob) / 4)])    U. add_weighted_edges_from([(u,  bounce-net , (1-flying_prob) / 4)])    U. add_weighted_edges_from([(u,  player-hit , eps)])  elif  net  in u:    U. add_weighted_edges_from([(u,  bounce-net , 1-flying_prob)])    #U. add_weighted_edges_from([(u,  player-hit , eps)])The matrices: Okay, we have the graphs, but what about the matrices? We need those for the hmmkay library. How do we generate them? NetworkX provides a function for generating adjacency matrices (adjacency_matrix). However, we cannot use those matrices as they are, we need to normalize them so that the rows sum up to one. Remember, they represent probabilities. After normalization, we can save the result using pandas. 12345678910111213141516import pandas as pd    Emission matrix    V1 = len(G. nodes())B = nx. adjacency_matrix(U). toarray()[:V1,V1:]err_change = 0B += err_changeB = B / B. sum(axis=1). reshape((-1,1))B_df = pd. DataFrame(B, columns=obs. nodes(), index=G. nodes())B_df. to_csv(folder_path + 'B. csv')    Transition matrix    A = nx. adjacency_matrix(union). toarray()A = A / A. sum(axis=1). reshape((-1,1))A_df = pd. DataFrame(A, columns=G. nodes(), index=G. nodes())A_df. to_csv(folder_path + 'A. csv')The err_change variable is for adding noise to the emissions so that every state can emit every observation with a little probability. In my experience it doesn’t work well, but I leave it there in case you want to experiment with it. ConclusionIn this post we have seen how to properly design a HMM for following the result of a padel match. We have learnt to use the NetworkX library to create the graphs and the Gephi program to visualize the process. In the next post of this series we will learn how to actually test whether the HMM works. Stay tuned. "
    }, {
    "id": 18,
    "url": "https://granadata.art/HMM-padel/",
    "title": "Modelling a padel match with Hidden Markov Models",
    "body": "2022/09/06 - Imagine you are at a padel match. You watch the ball go from one side to the other, hit the wall, hit the ground, the net, and after some time someone wins the point. In your head you are following the state of the match until the end to know the winner. Imagine now that you were distracted by a fly and lost concentration on the players. You don’t know what has happened when you were distracted but you managed to watch the last part of the point and so you still know who is the winner. How can we replicate this situation with a model? Which is the correct model to manage situations where you can lose track in the middle but by looking the last part you know the result? The property of only needing to know the last part to know the result is called the Markov property. So a suitable model for this task is a Hidden Markov Model. In this series I will describe how to properly design a Hidden Markov Model (HMM from now on) to keep track of the state of a padel match. I will also provide functional python code that you could play with. And as extra material I will talk about unit testing and how to use unit tests to incrementally build a model like this one. Let’s begin. First of all a rapid introduction on HMM and how to code them. A Hidden Markov Model has two main elements: hidden states and observations. The hidden states are what represent the model, in the padel case a hidden state can be first service, or ace or second service. The observations are what you can observe directly, continuing with the analogy an observation can be when the ball hits the ground. What the HMM does is to infer the hidden states based only on a sequence of observations. If you watch that the first player hits the ball and then the ball hits the ground twice on the other side, you can infer that the sequence of states is first service and then ace. A more abstract scheme is shown below.  Every arrow represents a transition either between hidden states or between a hidden state or an observation. Each transition is nothing more than a probability. For example, the probability of going from second service to first service is zero because the first service always goes first. The transitions between hidden states and observations are called emissions. One emission could be that the probability of observing the ball hit the ground after first service is $0. 5$. The transition and emission probabilities are represented as matrices. Below you can see the transitions of an example from a toy HMM.  Once you have a HMM there are three things you can do with it. Given a sequence of observations you can decode the most probable sequence of hidden states with an algorithm call Viterbi’s. You can estimate the internal probabilities of the HMM using several sequences of observations with the Baum-Welch algorithm. Or you can sample a new sequence of observations. We are only interested in the first one, decoding. The transition and emission probabilities will be designed to model the rules of a padel match. Now that we have seen the theory, let’s see how we can decode sequences in Python. As you may have guessed there are libraries that implement all the previously mentioned algorithms. My favorite one so far is hmmmkay. It is quite easy to use and is fast enough for my use cases. You can create a HMM with a few lines of code. See below. 1234567891011import pandas as pdimport numpy as npfrom hmmkay import HMMfolder_path = 'graphs/'transition_probas = pd. read_csv(folder_path + 'A. csv', index_col=0)emission_probas = pd. read_csv(folder_path + 'B. csv', index_col=0)hidden_states = emission_probas. shape[0]init_probas = np. zeros(hidden_states)init_probas[0] = 1hmm = HMM(init_probas, transition_probas, emission_probas)The important method is HMM(init_probas, transition_probas, emission_probas). Given the transition and emission matrices and given also some initial probabilities for the hidden states it returns an object that can decode any sequence. The details of how to create the matrices will be described in later posts of the series. I can advance you a bit about the process. You first begin by drawing a graph in paper with the transitions you like, you then parse that graph in paper into a graph in digital format (. gml). And finally, you use the adjacency matrices of the graph as the probability matrices. But for now, let’s assume we already have those files. How can we decode a sequence? Like this. 1decoded_seq = hmm. decode(sequences)The only concern to bear in mind is that the input and the output are numbers starting from zero. You need to parse those values to have something significant. I normally use the column names of the matrices as dictionaries to parse the sequences. 123456indexer_hidden = dict()for k, col in enumerate(transition_probas. columns):  indexer_hidden[col] = kindexer_obs = dict()for k, col in enumerate(emission_probas. columns):  indexer_obs[col] = kPutting everything together, to decode a sequence the program would look something like this code. 12345678910111213### ace in first service ###sequence = ['player-hit', *['flying'] * 10, 'bounce-ground-receiver', *['flying'] * 10,   'bounce-ground-receiver', *['flying'] * 10, 'end']sequences = [[indexer_obs[obs] for obs in sequence]]decoded_seq = hmm. decode(sequences)decoded_seq = [hidden_states[idx] for idx in decoded_seq[0]]print(decoded_seq)# Result: ['1st-serve', 'flying-1st-serve-0', 'flying-1st-serve-1', 'flying-1st-serve-2', 'flying-1st-serve-3', # 'flying-1st-serve-4', 'flying-1st-serve-5', 'flying-1st-serve-5', 'flying-1st-serve-5', 'flying-1st-serve-5', # 'flying-1st-serve-5', 'in1', 'flying-in1-0', 'flying-in1-1', 'flying-in1-2', 'flying-in1-3', 'flying-in1-4', # 'flying-in1-5', 'flying-in1-5', 'flying-in1-5', 'flying-in1-5', 'flying-in1-5', 'ground', 'flying-ground-0', # 'flying-ground-1', 'flying-ground-2', 'flying-ground-3', 'flying-ground-4', 'flying-ground-5', # 'flying-ground-5', 'flying-ground-5', 'flying-ground-5', 'flying-ground-5', 'Point-server']Don’t worry if you don’t understand all this fuzzy names, they are the names that I chose for the hidden states and observations. On my next post I will explain in detail what everything means. For now I just want you to notice that the HMM is correctly identifying the winner in this point. The sequence of observations correspond to an ace in the first service. The player hits the balls and it bounces twice in the other side. Any person watching the match will automatically identify that as an ace. Here, it gives more information than that. It recognizes the instant in which the ace is achieved. The hidden state in1 means the ball has correctly entered into the other side and the state ground means that it has bounced again in the ground. After a while of having the ball flying, the model outputs the state Point-server correctly giving the victory to the server. On my next post I will talk about the process of designing the transition and emission matrices. I will also talk about what is reasonable to be defined as observation and which hidden states are needed. And on a later post I will cover a lesson on noisy decoding. One important feature of Hidden Markov Models is that they can decode the sequence of observations even if it is wrong at some point. Like I said at the beginning you could ignore the match for some time and then you would still be able to recognize the winner. HMMs can go even further. Imagine that you are not looking the match and you are just hearing it from the radio. If the commentator makes some mistake you may hear something impossible like a player hitting twice the ball without the match finishing. HMM can decode the sequence correctly even in those situations. That is because HMMs work with probabilities. If a player hits twice the ball and the game continues, the HMM will identify that second hit as a mistake and ignore it. Of course, if the sequence is completely wrong, the result will be wrong too. But HMM are quite robust to noise in the input if you design them carefully. See you on my next post to learn how to design Hidden Markov Models. "
    }, {
    "id": 19,
    "url": "https://granadata.art/gan-convergence-proof/",
    "title": "GAN convergence proof",
    "body": "2022/09/05 - In my previous post about GANs I explained the mathematical proof of why GANs work. The theorem stated that the real data distribution was mimicked by the generator at the optimum of the cost function, although it didn’t say how to find such optimum. In this post I will specify how to find such optimum and prove that the algorithm provided works. Keep in mind that this algorithm is the original proposed by Ian Goodfellow, several improvements have been made since then. At the end I will describe some of the faults of this algorithm and various changes that have been tried since it was published. The algorithm can be described in pseudocode as follows:   % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)  \begin{algorithm}  \caption{GAN convergence algorithm}  \begin{algorithmic}  \FOR{number of training iterations}    \FOR{\$k\$ steps}      \STATE Sample minibatch of \$m\$ noise samples \( \{ \)\$z\$\( ^{(1)},\dots,\)\$z\$\(^{(m)}\} \) from noise prior \$p\_z(z)\$.       \STATE Sample minibatch of \$m\$ examples \( \{ \)\$x\$\( ^{(1)},\dots,\)\$x\$\(^{(m)}\} \) from data generating distribution \$p\_\{data\}(z)\$.       \STATE Update the discriminator by ascending its stochastic gradient:      $\nabla_{\theta_d}\frac{1}{m} \sum_{i=1}^{m}[log(D (x^{(i)})) + log(1-D(G(z^{(i)})))$      \normalsize    \ENDFOR    \STATE Sample minibatch of \$m\$ noise samples \( \{ \)\$z\$\( ^{(1)},\dots,\)\$z\$\(^{(m)}\} \) from noise prior \$p\_z(z)\$.     \STATE Update the generator by descending its stochastic gradient: $\nabla_{\theta_g}\frac{1}{m} \sum_{i=1}^{m}log(1-D(G(z^{(i)})))$  \ENDFOR  \end{algorithmic}  \end{algorithm}Let’s disect the algorithm piece by piece. Recall which function we were trying to optimize: $$V(G,D) = \mathbb{E}_{\textbf{x} \sim p_{data}}[\log(D(\textbf{x}))] + \mathbb{E}_{\textbf{z} \sim p_{\textbf{z}}}[\log(1-D(G(\textbf{z})))]$$We wanted the maximum with respect to the discriminator and then the minimum with respect to the generator: $$\min_G \max_D V(G,D)$$If you look at the pseudocode we are doing just that. We first apply gradient ascent on the discriminator and then gradient descent on the generator. You may have noticed that the formulas in the pseudocode are different from what I showed you in the previous post. That is because we are approximating the expectation using a Monte Carlo method. To compute the expected value of a variable you need to compute an integral which in this case, and many others, is intractable. For that reason the expectation is approximated by the mean: $$\mathbb{E}_{\textbf{x} \sim p}[f(\textbf{x})] \approx \frac{1}{m}\sum_{i=1}^{m} f(\textbf{x}^{(i)})$$where the $\textbf{x}^{(i)}$ are sampled from the distribution $p$. If you go to the pseudocode you will see that we sample the $\textbf{z}$ from the prior noise and the $\textbf{x}$ from the real data. You may have also noticed that there is one term missing when applying gradient descent for the generator. The reason for that is the first summand in the formula does not depend on the generator, therefore the gradient of that term is zero. As you can see the algorithm itself is quite intuitive. For maximization apply gradient ascent and for minimization you apply gradient descent. That’s it. But being simple and intuitive is not enough for an algorithm to be correct, there needs to be a proof of their correctness. In this case there needs to be a proof of convergence and optimality. And like every theorem, that proof is going to come with some hypothesis. The first hypothesis is that $D$ and $G$ have enough capacity. This means that whatever model you use for them can reach the optimum. That seems a pretty reasonable hypothesis, but in practice you never know how much complexity is enough complexity. The second hypothesis is that $D$ can reach the optimum at each step given $G$. This is basically saying that the parameter $k$ used is sufficiently large, and that the learning rate is sufficiently small so that other theorems of gradient descent convergence hold. That optimum is going to be called $D^*_G$. This hypothesis is less important than the next one, and in practice reaching the optimal discriminator at any step is a problem that I will describe later on in this post. The last hypothesis is that at each step the criterion $C(G)=\min_D(V(G,D))$ improves. Which is basically saying that gradient descent is working on the generator. Formally, the theorem can be expressed like follows If $G$ and $D$ have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given $G$, and $p_g$ is updated so as to improve the criterion$$\mathbb{E}_{\textbf{x} \sim p_{data}}[\log(D^*_G(\textbf{x}))] + \mathbb{E}_{\textbf{x} \sim p_g}[\log(1-D^*_G(\textbf{x}))]$$then $p_g$ converges to $p_{data}$. The proof consists of two main steps: proving the cost function is convex with respect to the generator and proving the gradient descent at the optimal $D$ given $G$ converges to the same as the gradient descent for the global optimal $D$. This way we don’t need to know the real optimal discriminator, just the one that is optimum for each generator. The proof for convexity is more technical and I will explain it later. Let’s go with showing we only need a suboptimal discriminator. As everything in math, we start with definitions and notation that will made the rest of the proof easier to follow. The first change is for the value function, let’s call $U(p_g,D)=V(G,D)$ to the value function, changing the dependency to the generated distribution instead of the model. This is to highlight that we are working with ideal models in the function space not in the parametric space. Let’s call $U(p_g) = \sup_DU(p_g,D)$ the value function for the optimal discriminator. Since we are working in the function space the supremum is used instead of the maximum. Now the problem is to find $\inf_{p_g}U(p_g)$. Assuming $U(p_g,D)$ is convex for every $D$ there is a theorem saying that $U(p_g)$ is also convex. Now, the key of the proof is showing that any subgradient of $U(p_g,D^*_G)$ is also a subgradient of $U(p_g)$ when $D^*_G=\text{argsup}_D U(p_g,D)$. This way gradient descent on $U(p_g,D^*_G)$ converges to the same value as gradient descent on $U(p_g)$, since that function is convex and the global optimum exists, that optimum is found. Now, the details. Mathematically, the argument of the subgradients can be expressed as follows: $$\partial U(p_g, \text{argsup}_D U(p_g,D)) \subseteq \partial \sup_D U(p_g,D)$$Being $\partial f$ the set of subgradients of $f$. The proof of that is a matter of using correctly the definitions. We have $U(p_g,D^*_G)=U(p_g)$. If $g\in \partial U(p_g,D^*_G)$, then by definition we have $U(p_g’,D^*_G) \ge U(p_g,D^*_G) + g^T (p_g’-p_g)$ for every other distribution $p_g’$, being the last term the scalar product of functions. By definition of supremum we have $U(p_g’) \ge U(p_g’,D^*_G)$. Joining everything we obtain $$U(p_g') \ge U(p_g',D^*_G) \ge U(p_g,D^*_G) + g^T (p_g'-p_g) = U(p_g) + g^T (p_g'-p_g)$$And therefore $U(p_g’)\ge U(p_g) + g^T (p_g’-p_g)$ which shows that $g\in \partial U(p_g)$. In Algorithm 1 we don’t use subgradients, and that is because if we assume the cost function is differentiable, then the only subgradient is the gradient. And we always use cost functions that are differentiable. For that reason, this part of the proof can also be expressed like this $$\nabla V(G,D^*_G) = \nabla C(G)$$which gives a way of optimizing $C(G)=\sup_D V(G,D)$ without needing to know the function itself. Quite a nice property of convex functions. Let’s now prove that the function is in fact convex. What is the definition of convex in this context? It simply means that $$U(tp_g'+(1-t)p_g,D) \ge tU(p_g',D)+(1-t)U(p_g,D)$$which, after removing at both sides the terms that doesn’t depend on $p_g$, translates to $$\mathbb{E}_{\textbf{x} \sim tp_g'+(1-t)p_g}[\log(1-D(\textbf{x}))] \ge t\mathbb{E}_{\textbf{x}\sim p_g'}[\log(1-D(\textbf{x}))] + (1-t)\mathbb{E}_{\textbf{x}\sim p_g}[\log(1-D(\textbf{x}))]$$To prove that, we are going to prove an even stronger statement, which is $$\mathbb{E}_{\textbf{x} \sim tp_g'+(1-t)p_g} \equiv t\mathbb{E}_{\textbf{x}\sim p_g'} + (1-t)\mathbb{E}_{\textbf{x}\sim p_g}$$And this is fairly easy to prove, we just need to recall the linearity of integrals $$\mathbb{E}_{\textbf{x} \sim tp_g'+(1-t)p_g} [f] =\int (tp_g'+(1-t)p_g)f = t\int p_g' f + (1-t) \int p_g f = t\mathbb{E}_{\textbf{x}\sim p_g'}[f] + (1-t)\mathbb{E}_{\textbf{x}\sim p_g}[f]$$which finally proves the convexity of $U(p_g,D)$ with respect to $p_g$. Let’s start now with the faults of this proof and why it doesn’t hold on practice. The main reason it doesn’t hold in practice is the convexity of the cost function. Whenever we change the set of all distributions $p_g$ by the set of parametrized generators $G$ the corresponding cost function becomes non-convex. This is so because we are reducing the space from an infinite convex space, to a finite space. Now $tG’+(1-t)G$ may not be any valid generator, and so the set of reproducible distributions could be non-convex. In addition, it is well-known that using deep neural networks creates non-convex costs functions. That limits the possibility of finding the global optimum by only using gradient descent. There is another problem with this proof. It requires that we find a perfect discriminator at each step, and after it we apply gradient descent on the generator. The reason why that doesn’t work is mainly numerical. A perfect discriminator is going to produce gradients close to zero. When propagating the gradient it results on the generator not training at all. In practice, many articles limit the ability of learning of the discriminator so that the gradient is non-zero. One technique for doing so is using a smaller learning rate for the discriminator. This way the discriminator is learning at a slower pace than the generator. However, there are no results either proving or disproving that a neural network is not well-suited for this task. Many researchers have achieved decent results when training GANs. I wouldn’t even be writing this posts at all if GANs were a loss of time, which they aren’t. In practice they can work very well, but bare in mind that they are difficult to train. There is no theorem guaranteeing that Algorithm 1 works always. And you may probably need to use a modification of Algorithm 1 to make it work. But all in all, you can consider real GANs as approximations of an ideal GAN that always converge. It’s better than nothing. "
    }, {
    "id": 20,
    "url": "https://granadata.art/headless-raspberry-setup/",
    "title": "Headless installation of Ubuntu on Rasbperry",
    "body": "2022/08/30 - This tutorial aims to be a fast* tutorial for installing Ubuntu Server into a Raspberry Pi and accessing it via ssh. First, download the image from here. Then, using Balena Etcher boot the image into a USB. The process is simple, just follow the steps of the program. Now, the configuration part. Access your bootable USB. In my case I accessed it via the bash command line with cd /Volumes/system-boot but you can access it in any other ways. In order to enable SSH on the first boot, you need to create a file called ssh with nothing on it. But, to be able to connect to the Raspberry you also need it to be connected to a wifi. By default, it doesn’t connect to any wifi, just to the ethernet in case it is plugged. So you need to modify the network-config file. At the end of the file you can see something like this 12345version: 2ethernets: eth0:  dhcp4: true  optional: trueAfter that there are many commented lines. You need to uncomment and modify those lines. The first you need to do is to add your wifi**. For that, after access-points include the SSID which is the name of the wifi, and the password of that wifi: 123access-points: &lt;SSID&gt;:  password:  &lt;password&gt; Be sure to maintain those commas and remove any other wifis from there. Finally, insert the SD card, start your raspberry and you are free to go. Connect to your Raspberry Pi using the command ssh ubuntu@&lt;raspberry IP&gt;***, enter the default password “ubuntu”. You will be asked to change the default password and voilà, you have a server. *For a more detailed explanation you can go to the official Ubuntu page. And for more details on how netplan and their config files work you can go to their reference. **If you have a Mac and an iPhone you can create a wifi hotspot from the iPhone’s internet. That way you can access your Raspberry anywhere. Here you have a tutorial for creating the hotspot. One more thing, use the channel 1, otherwise the Raspberry Pi won’t be able to connect to that frequency. ***To find the assigned IP yourself you can try nmaping several IPs in the range to see which one has the 22 port open. You can also follow this tutorial for MacOS or any other you find. "
    }, {
    "id": 21,
    "url": "https://granadata.art/stable_diffusion_tutorial/",
    "title": "Stable Diffusion Tutorial (Deprecated)",
    "body": "2022/08/25 - Three days ago Stable Diffusion was publicly released and today I am bringing to you an easy way of using the model without the need of having any kind of extra hardware, just your laptop and wifi connection. At the end I will also leave a script in case you do have some extra hardware and want to put that RTX 3080 to good use. In case any of you didn’t know what Stable Diffusion is, it is similar to DALLE·2. It is a diffusion model able to create images from text. For example, for the prompt “Amazing, complex, intricate and highly detailed treehouse in a snow covered bonsai tree on top of a table, steampunk, vibrant colors, vibrant, beautiful, contrast, neon highlights, Highly detailed, ray tracing, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by Beeple, Mike Winklemann, 8k” you get this amazing result.  If you want to be able to produce this astonishing images with just a few words and 20 seconds of computation continue reading. In the internet you may find many tutorials for using this model on Colab. However, the Colab notebooks sometimes don’t give you access to GPU resources and so you may take several minutes to generate one single image. To avoid that we are goint to run Stable Diffusion in Kaggle, their servers provide you with 30 weekly hours of GPU computation, which roughly translates to 5000 generated images, more than neccessary to satisfy your needs. The first step you need to do is to create a Kaggle and HuggingFace account. The Kaggle account is to have access to GPUs as I said before, and the HuggingFace account is to have access to the Stable Diffusion model. I’ll go step by step. Let’s create the HuggingFace account. Go to https://huggingface. co/.  At the top right click on Sign Up.  Follow the steps and log in with your account. Then, when you are logged in go to Settings as showed in the next image.  Now, go to the Access Tokens section.  Finally, let’s create our needed token. Click on New token.  Enter any name you like, I will use StableDiffusion for obvious reasons. You can use write or read permissions, but you only need read permissions so I advise you to leave it like that.  And you should end up with something like this. Copy the token and save it for later use in Kaggle.  Before you can use this token, you need to agree to the terms and conditions of the Stable Diffusion model. Go to the page https://huggingface. co/CompVis/stable-diffusion-v1-4, access the repository and accept the terms and conditions. If you cannot see the tick box, you just need to log in.  Same process, to create your account go to https://www. kaggle. com/ and register yourself.  Again, just follow the steps.  Once you are logged in, we can start generating images. I have created a notebook with everything explained. To go to the notebook just go to this link: https://www. kaggle. com/code/josepc/stable-diffusion-nsfw/notebook. You should see something like this. Click on the 3 dots at the top right corner. And then go to Copy &amp; edit notebook.  Ok, if it is your first time at kaggle there are a few things to explain before running the notebook. Once you are inside the notebook editor, to start the notebook you have to click the On button, but first you need to make sure the GPU is enabled and that the internet is enabled too. To do this, click the bottom right arrow.  You should see this, If you don’t see the GPU there, then click on it and set it to GPU. Leave everything else as it is and start the notebook. Once the notebook started some extra bars appear to show the GPU is running.  To run each cell, click on it and do shift+Enter. Everything is explained there, but I’ll go cell by cell again here explaining what you need to modify to make it work for you.  The first cell is just for installing libraries, run it as it is. The second one is where you actually need to enter your token. Modify the highlighted line copying there your HuggingFace token previously created. Keep in mind that the token should be enclosed by commas.  Once you hit shift+Enter, it will start downloading the model. It takes a few, you can see the progress like this. That 1. 72G bar is the biggest one, when that is finished you are almost done.  The third cell is for moving the model to the GPU, just run it normally with shift+Enter.  The fourth cell is the important one, here is where you are actually generating the images. There are two variables that you need to modify. The first one is num_images, set it to the number of images you want to generate from a given prompt, but I advise you not to use more than 4 images at once for reasons I will mention at the end. The second variable is the prompt variable, modify it to include your desired prompt. Delete the text in between commas (it is quite big) and write what you want. The result should be something like this: prompt = ['GIVEN TEXT'] * num_images, don’t change the format or it won’t work. Just write your prompt inside the commas.  Once you hit shift+Enter, a progress bar will appear. When the value reaches 51 you are done. It takes nearly 20 seconds per image.  The last two cells are for visualizing and saving the images. Just hit shift+Enter and there you have it, fabulous new images that noone has ever seen before.  Possible errors: There is the chance that you ran into an error call CUDA out of memory. When that happens, the only solution is to reset the notebook and rerun everything. If you create images in batches of 4, then it is quite difficult for that error to happen. But if for some reason you see a big red error message, just ignore it and restart your notebook. For those interested, the reason why that error happens is because there is a memory leakage into the GPU, the model creates some auxiliary tensors and then forgets of their existence. You cannot delete them because they are internal to the model, and the cache memory manager cannot delete them because they are still active, although never used. To solve it one would have to find the references of the allocated tensors and deallocate them manually, but it is easier to just restart the environment. The error looks like this, so that it doesn’t take you by surprise. If you go to the end of the large message you will see this.  NSFW version: The tutorial I showed you is for using the standard Stable Diffusion version from Hugging Face which has a safety checker to ensure you don’t generate nasty images. However, since the model is Open Source, it is possible to modify the code to remove that safety checker. In the name of liberty, I created another version of the notebook removing that safety checker. If you go to the original link of the notebook, you will see that there is a box stating Version 2 of 2.  If you click it you can see the first version which contains two extra cells to remove the safety checker. You can run that version or copy those cells into your copy of the other version. These are the cells. The first one is for loading extra libraries, and the second one is the actual code removing the checker.  This last cell is quite large, but the only important change was done at the end. Commenting out those two lines is the only thing needed.  After removing the checker you just change the original call function with the modified one.  But you don’t need to understand this, just copy and use it. I have taken the time to make it work for you. Script version: Since I have explained everything before I will just leave the script version here for those of you that have access to some server with GPUs or those of you rich enough to have GPUs in your houses. The usage is quite simple, there are only 3 flags that you need to know: the number of images to generate, the prompt and the token. The save_path can be left with the default value. The whole script is pasted below. 123456789Creates several images from a given prompt. optional arguments: -h, --help      Usage python3 StableDiffusion. py -n N --promp Text --token HuggingFaceToken [--save_path dir]]] -n N         Number of images to output.  --prompt PROMPT    Prompt to generate image.  --token TOKEN     HuggingFace token to download the model.  --save_path SAVE_PATH            Path to the folder to save the results. import osimport inspectimport warningsfrom typing import List, Optional, Unionimport torchfrom torch import autocastfrom tqdm. auto import tqdmfrom transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizerfrom diffusers. models import AutoencoderKL, UNet2DConditionModelfrom diffusers. pipeline_utils import DiffusionPipelinefrom diffusers. schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMSchedulerfrom diffusers import StableDiffusionPipelineimport argparseparser = argparse. ArgumentParser(description='Creates several images from a given prompt. ', add_help=False)parser. add_argument('-h', '--help', action='help', default=argparse. SUPPRESS,          help='Usage python3 StableDiffusion. py -n N --promp Text --token HuggingFaceToken [--save_path dir]]]')parser. add_argument('-n', type=int, default='1',          help='Number of images to output. ')parser. add_argument('--prompt', type=str,          help='Prompt to generate image. ')parser. add_argument('--token', type=str,          help='HuggingFace token to download the model. ')parser. add_argument('--save_path', type=str, default='outputs',          help='Path to the folder to save the results. ')args = parser. parse_args()pipe = StableDiffusionPipeline. from_pretrained( CompVis/stable-diffusion-v1-4 ,                        revision= fp16 ,                        torch_dtype=torch. float16,                        use_auth_token=args. token)print('Loaded model')@torch. no_grad()def NSFWcall(  self,  prompt: Union[str, List[str]],  height: Optional[int] = 512,  width: Optional[int] = 512,  num_inference_steps: Optional[int] = 50,  guidance_scale: Optional[float] = 7. 5,  eta: Optional[float] = 0. 0,  generator: Optional[torch. Generator] = None,  output_type: Optional[str] =  pil ,  **kwargs,):      Modified version to remove the NSFW filter.      if  torch_device  in kwargs:    device = kwargs. pop( torch_device )    warnings. warn(       `torch_device` is deprecated as an input argument to `__call__` and will be removed in v0. 3. 0.          Consider using `pipe. to(torch_device)` instead.      )    # Set device as before (to be removed in 0. 3. 0)    if device is None:      device =  cuda  if torch. cuda. is_available() else  cpu     self. to(device)  if isinstance(prompt, str):    batch_size = 1  elif isinstance(prompt, list):    batch_size = len(prompt)  else:    raise ValueError(f `prompt` has to be of type `str` or `list` but is {type(prompt)} )  if height % 8 != 0 or width % 8 != 0:    raise ValueError(f `height` and `width` have to be divisible by 8 but are {height} and {width}.  )  # get prompt text embeddings  text_input = self. tokenizer(    prompt,    padding= max_length ,    max_length=self. tokenizer. model_max_length,    truncation=True,    return_tensors= pt ,  )  text_embeddings = self. text_encoder(text_input. input_ids. to(self. device))[0]  # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)  # of the Imagen paper: https://arxiv. org/pdf/2205. 11487. pdf . `guidance_scale = 1`  # corresponds to doing no classifier free guidance.   do_classifier_free_guidance = guidance_scale &gt; 1. 0  # get unconditional embeddings for classifier free guidance  if do_classifier_free_guidance:    max_length = text_input. input_ids. shape[-1]    uncond_input = self. tokenizer(      [  ] * batch_size, padding= max_length , max_length=max_length, return_tensors= pt     )    uncond_embeddings = self. text_encoder(uncond_input. input_ids. to(self. device))[0]    # For classifier free guidance, we need to do two forward passes.     # Here we concatenate the unconditional and text embeddings into a single batch    # to avoid doing two forward passes    text_embeddings = torch. cat([uncond_embeddings, text_embeddings])  # get the intial random noise  latents = torch. randn(    (batch_size, self. unet. in_channels, height // 8, width // 8),    generator=generator,    device=self. device,  )  # set timesteps  accepts_offset =  offset  in set(inspect. signature(self. scheduler. set_timesteps). parameters. keys())  extra_set_kwargs = {}  if accepts_offset:    extra_set_kwargs[ offset ] = 1  self. scheduler. set_timesteps(num_inference_steps, **extra_set_kwargs)  # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas  if isinstance(self. scheduler, LMSDiscreteScheduler):    latents = latents * self. scheduler. sigmas[0]  # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature  # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.   # eta corresponds to η in DDIM paper: https://arxiv. org/abs/2010. 02502  # and should be between [0, 1]  accepts_eta =  eta  in set(inspect. signature(self. scheduler. step). parameters. keys())  extra_step_kwargs = {}  if accepts_eta:    extra_step_kwargs[ eta ] = eta  for i, t in tqdm(enumerate(self. scheduler. timesteps)):    # expand the latents if we are doing classifier free guidance    latent_model_input = torch. cat([latents] * 2) if do_classifier_free_guidance else latents    if isinstance(self. scheduler, LMSDiscreteScheduler):      sigma = self. scheduler. sigmas[i]      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0. 5)    # predict the noise residual    noise_pred = self. unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[ sample ]    # perform guidance    if do_classifier_free_guidance:      noise_pred_uncond, noise_pred_text = noise_pred. chunk(2)      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)    # compute the previous noisy sample x_t -&gt; x_t-1    if isinstance(self. scheduler, LMSDiscreteScheduler):      latents = self. scheduler. step(noise_pred, i, latents, **extra_step_kwargs)[ prev_sample ]    else:      latents = self. scheduler. step(noise_pred, t, latents, **extra_step_kwargs)[ prev_sample ]  # scale and decode the image latents with vae  latents = 1 / 0. 18215 * latents  image = self. vae. decode(latents)  image = (image / 2 + 0. 5). clamp(0, 1)  image = image. cpu(). permute(0, 2, 3, 1). numpy()  # run safety checker  #safety_cheker_input = self. feature_extractor(self. numpy_to_pil(image), return_tensors= pt ). to(self. device)  #image, has_nsfw_concept = self. safety_checker(images=image, clip_input=safety_cheker_input. pixel_values)  if output_type ==  pil :    image = self. numpy_to_pil(image)  return { sample : image,  nsfw_content_detected : False}# Change the call function to remove NSFW filterStableDiffusionPipeline. __call__ = NSFWcallprint('Removed NSFW filter')pipe = pipe. to( cuda )print('Loaded model into GPU')prompts = [args. prompt] * args. nwith autocast( cuda ):  images = pipe(prompts)[ sample ]print('Executed model')if args. save_path[-1] == '/':  args. save_path = args. save_path[:-1]if not os. path. isdir(args. save_path):  os. mkdir(args. save_path)for i, image in enumerate(images):  image. save(args. save_path+ /output +str(i)+ . png )print('Saved images') "
    }, {
    "id": 22,
    "url": "https://granadata.art/NAS-parte3/",
    "title": "Neural Architecture Search (Part 3)",
    "body": "2022/08/22 - We have seen in Part 1 and Part 2 that neural architecture search can be used to find hyperparameters and to design recurrent networks. Is there any more use to it? The answer is obviously yes (otherwise there would be no point on this post). Let’s recap, NAS is just a method where you learn something that is non-learnable by backpropagation using trial and error, aka, reinforcement learning. The key for using it in more complex settings is to have a well defined space of parameters. In the first part that space was simply the real range for each hyperparameter. In the second part the search space included every recurrent network. Since that is a pretty broad space we narrowed it down by only working with some specific recurrent networks that could be generated by a tree-like structure. In this third part the search space is going to be the set of all convolutional neural networks. And to reduce that space to something manageable we are only going to consider networks generated by residual connections. Residual connections are typically just an addition operation like this The good property of residual connections is that they prevent vanishing gradients for very deep networks. Their discovery allowed to increase the number of layers. ResNet achieved state of the art results when it was first created, and big transformer-based architecture also use residual connections. Normally the residual connection is between one layer and the next one. Can you think of a way of generalising this? Is there a ways of creating a bigger set of possible architectures? Similar to what we did for recurrent networks, we could break that restriction of just connecting to the next layer. This way we could end up with something like the architecture presented in the original NAS paper Again, the real question is how to encode that architecture, because using a picture is not quite computer-friendly. Think of the way of representing a graph, you use the adjacency matrix, or the adjacency lists. Here it is similar, for each layer you just need to know the inputs. Or in other words, the architecture is saved as adjacency lists of incoming vertices. That’s simple enough for the controller to generate. For each layer the controller has an array of previous layers and simply selects one or more indices from there representing the incoming residual connections. The rest of hyperparameters are generated afterwards. See the diagram What the anchor point box is doing is just that, sampling indices representing the incoming layers. That box may implemented as a feed forward network with several classes. It could seem that the number of classes varies and so we cannot fix the last layer to have a constant size, and that is true, but for a given layer it is always the same. Therefore, fixing the total number of layers also fixes the number of output classes for each anchor point and so we can train them. Another reasonable way to implement the anchor box is as a multinomial variable. That way you only need to learn the probabilities, maybe conditioned to something. Although that poses another problem since another way of applying backpropagation needs to be designed, probably by using some reparameterisation trick. The original implementation was done through attention. They gave some hidden state to each anchor box and used Bahdanau attention mechanism to select the most similar layers (according to that hidden state that is learnable) to use as residual connections. This solves the problem of variable sized input, you can use the same attention mechanism for every anchor point. Thus, reducing the number of total parameters while making all the connections related to each other. But simply selecting residual connections at random has one issue, have you seen it? There could be layers without connections, or layers that are not connected to anything. The solution is simple, for those layers that are not connected, you simply connect them to the last layer. And for those that doesn’t receive any input, the input image is used as the input, and not any other layer. This way, many complex architectures can emerge, not just linear ones. And there you have it, you can use reinforcement learning to explore the search space of convolutional neural networks. Here I have presented you three ways of using neural architecture search to better design neural network architectures. If you are feeling short of ideas to create networks, maybe try designing a broad search space and traing a NAS controller on a toy dataset. And don’t feel that it is an outdated technique, Google is still using it, see this article. The main disadvantage of this tecnique is that it requires a lot of computational power, but as everything else, you can use it with reduced capabilities on smaller datasets to try on your own. Maybe you find the next transformer this way, who knows? "
    }, {
    "id": 23,
    "url": "https://granadata.art/gan_optimality_proof/",
    "title": "GAN optimality proof",
    "body": "2022/08/10 - The so-called Generative Adversarial Networks have been with us since 2014, producing amazing results lately. Today I am bringing the proof of why they work. That is, a proof that states GANs are optimal in the limit when no parametric model is taken into account. But first, what is a GAN and what “amazing results” am I talking about? The above example is about pix2pix which is a conditional adversarial network which gives style to your draft. You draw some sketch, tell some style and the network does the rest of the work. But there are other types of GANs, like style GAN which is very good at generating images of some given type, like the ones below, which are all artificial.  GANs are also related to DALLE-2, the famous text-to-image model. Both are a special case of energy-based models, which is a more general framework. If you want to know more about it you can look at the Yann Lecunn lectures about it. Let’s dive into the details of the original GAN formulation. Typically, GANs are presented with the following diagram.  But I prefer to understand models in the mathematical realm. There a GAN is a min-max problem. Concretely it is this min-max problem: $$\min_G \max_D V(G,D)$$Where $D$ is the discriminator, it receives an image an outputs the probability of it being fake. $G$ is the generator, it receives some input vector $\textbf{z}$ and outputs a fake image, denoted by $\textbf{x}$. $V$ is a value function, which is defined as follows (each term will be explained in detail later): $$\mathbb{E}_{\textbf{x} \sim p_{data}}[\log(D(\textbf{x}))] + \mathbb{E}_{\textbf{z} \sim p_{\textbf{z}}}[\log(1-D(G(\textbf{z})))]$$How can we interpret those two terms? The first one is large when the discriminator is correctly identifying real images as real. The second one is large when the discriminator is correctly identifying the generator images as fake. Maximising this quantity yields a perfect discriminator. However, we want more than that, we want to fool the discriminator. That means creating a good generator. Fooling the discriminator means minimising the value function. Thus, we have the min-max problem as stated above. Intuitively the equilibrium of this problem will be reached when we have a perfect generator and the discriminator always outputs $\frac{1}{2}$ because the generated images are the same as the real ones. This is exactly what the optimality theorem from the original Ian Goodfellow’s paper proves. To understand the theorem, it is needed to give some formalism to the intuitive idea of perfect generator. What is a perfect generator? For us humans, it means that the images seem real. But mathematically, what does “real” means? It means that the probability distribution of the generated images and the probability distribution of the data are the same. Mathematically the images are random variables, whose observations are represented by a vector. If the real and fake vectors come from the same distribution, then, they can be considered to be the same. As an example, consider the probability distribution function (pdf) of images of Granada. From that pdf it is more probable to draw an image of the Alhambra than an image of the Eiffel tower. Also, random noise has zero probability on that pdf. So now, a good generator pdf should mimick that behaviour, giving high probability to images of Granada and low probability to everything else. In mathematical terms, the data pdf is denoted by $p_{data}(\textbf{x})$ and the generator pdf is $p_{g}(\textbf{x})$. So producing “real” images means that $p_{data}(\textbf{x}) = p_{g}(\textbf{x})$, $\forall \textbf{x}$. Now we can state the optimality theorem: The global minimum of $C(G) = \max_{D}V(G,D)$ is reached if and only if $p_{data}\equiv p_g$. Also, that minimum value is $-\log(4)$. Before proving it, let’s analyse the consequences of it and what it is telling us. The main takeaway is that the solution to the min-max problem gives a perfect generator, which means that we can learn to reproduce any probability distribution. For simpler distributions this doesn’t seem much, one can simply compute the histogram of a variable and use it as the estimated distribution. However, for images that is not possible. How can you compute the histogram of a set of images? There are no repeated values, we just have a bunch of different images with some similarities. So one can think of a GAN as a way of estimating the histogram of a set of images. But it is more than that, it also provides a way of drawing points (images) from that distribution. Another takeaway from the theorem is the optimal value of the value function. It may seem useless at first but it is a good indicator of whether or not the training is converging or not. Because in practice that optimal generator does not appears to us by divine revelation, an iterative method is typically used to find it. If you see that during training the model converges to a value different than $-\log(4) \approx -1. 38$ then you can be certain that you have not solved the min-max problem. So far so good but, the generator as presented above is just a deterministic function, where does the variability comes from? It is there on the formulas, you just need to give a closer look. When defining the value function the second term was computed from the distribution $p_{\textbf{z}}$, what is that? It is a prior distribution for the input of the generator, which means that the generator is a function transforming one distribution into another. Mathematically this means $\textbf{z} \sim p_{\textbf{z}} \Rightarrow G(\textbf{z}) \sim p_g$, a key fact that will be used in the proof. The inference process is now quite easy, just draw a point from $p_{\textbf{z}}$ and apply the generator to get a new image. In practice you can just put any value you want for $\textbf{z}$ since the prior is a noise prior, not anything in particular. Finally, let’s prove the theorem. The first step is to find the optimal discriminator given the generator. After that, that value is substituted into the formulas and everything is rearranged into something with an obvious lower bound. I will skip many computational details, you can just check them by hand or if they don’t seem trivial to you, email me and I will write an appendix with more details. Let’s go, first part: If $G$ is fixed, the optimal $D$ is $$ D^*(\textbf{x}) = \frac{p_{data}(\textbf{x})}{p_{data}(\textbf{x}) + p_{g}(\textbf{x})} $$ We have $V(G,D)=\mathbb{E}_{\textbf{x} \sim p_{data}}[\log(D(\textbf{x}))] + \mathbb{E}_{\textbf{z} \sim p_{\textbf{z}}}[\log(1-D(G(\textbf{z})))]$, in order to combine both integrals we need them to depend on the same variable $\textbf{x}$. To do so we are going to exploit the fact that $\textbf{z} \sim p_{\textbf{z}} \Rightarrow G(\textbf{z}) \sim p_g$. This fact, in conjunction with the Radon-Nikodym Theorem lets us conclude that $\mathbb{E}_{\textbf{z} \sim p_{\textbf{z}}}[\log(1-D(G(\textbf{z})))] = \mathbb{E}_{\textbf{x} \sim p_{g}}[\log(1-D(\textbf{x}))]$. Where $\textbf{x} = G(\textbf{z})$. Now, if we express the expectations in integral form we get the following$$ \int_{\textbf{x}} p_{data}(\textbf{x})\log(D(\textbf{x})) + p_g(\textbf{x})\log(1-D(\textbf{x})) d\textbf{x} $$The integrand is now bounded by a function that does not depend on $D$ and so that bound is the optimum. That bound is found by differentiating with respect to $D$ and equalling to zero. The integrand there is basically $a \log(D) + b\log(1-D)$, which has the maximum at $\frac{a}{a+b}$ if $a$, $b$ are constant with respect to $D$. I still have my doubts with respect to that assumption, but assuming it we get that the maximum is reached when $D=\frac{p_{data}}{p_{data}+p_g}$ which ends the proof. We now have found the optimal discrimator so we can now compute the value function for that optimal discriminator. If we call $C(G)=\max_{D}(V(D,G))$ the value of the value function for the optimal discriminator, we just want to find the minimum of $C(G)$ for any given generator. The “rearrangement” I mentioned above is the following $$ \begin{align*}\max_D V(G,D) &amp;=\mathbb{E}_{\textbf{x} \sim p_{data}}[\log(D^*(\textbf{x}))] + \mathbb{E}_{\textbf{x} \sim p_{g}}[1-\log(D^*(\textbf{x}))] \\&amp;= \mathbb{E}_{\textbf{x} \sim p_{data}}[\log(\frac{p_{data}(\textbf{x})}{p_{data}(\textbf{x}) + p_{g}(\textbf{x})})] + \mathbb{E}_{\textbf{x} \sim p_{g}}[\log(\frac{p_{g}(\textbf{x})}{p_{data}(\textbf{x}) + p_{g}(\textbf{x})}] \\&amp;= -\log(4) + KL(p_{data}||\frac{p_{data}+p_g}{2}) + KL(p_g||\frac{p_{data}+p_g}{2})\\&amp;= -\log(4) + 2 \cdot JSD(p_{data}||p_g)\end{align*}$$The acronyms means Kullback-Leibler divergence and Jensen-Shannon divergence. For a more in-depth explanation of this rearrangement you can look at this great post. We are almost finished, there is a property of the JSD that states it is a nonnegative value, being zero if and only if both distributions are equal. That property translates into $C(G) \ge -\log(4)$ with equality if and only if $p_{data} = p_g$ exactly as desired. Magic, isn’t it? Unfortunately, this theorem only states that the optimal generator exists but it doesn’t give a way of finding it. In my next post on the GANs series I will show the proof of convergence for the algorithm also proposed by Ian Goodfellow which proves that such generators can be found, at least in theory. "
    }, {
    "id": 24,
    "url": "https://granadata.art/NAS-parte2/",
    "title": "Neural Architecture Search (Part 2)",
    "body": "2022/07/13 - In the previous article we discussed how we can use reinforcement learning to design simple architectures like some types of convolutional neural networks. Today I am bringing to you the explanation on how to design more complex architectures. Before diving into how to modify the controller, let’s introduce another way of thinking about recurrent nertworks. Typically, LSTM and GRU are explained through formulas or diagrams like the one I showed in the previous article. However, in the NAS paper they introduced another way of thinking about them. They used a graph representation in which states are nodes, and the edges represent the way of merging states. For instance, and edge can mean to apply a sigmoid function, another can be summing two hidden states, and so on. Below is a simple example visualized.  Here the input is $x_t$, the hidden state from the previous step $h_{t-1}$ and the cell state $c_{t-1}$ which is used as memory. As you can see the states are combined either using multiplication or addition, and then some activation functions are applied. Now, think for a moment how can you represent this same graph in a linear way, as a sequence of operations. You have it? Well, one possible way would be [Add, Tanh, Multiply, ReLU, Multiply, sigmoid, Add, ReLU, 1, 0]. Don’t worry, there are many ways to represent the above graph sequentally, this one is just the one provided by the author of the NAS. To understand it look at the next picture.  Let’s analyze it step by step. As you can see the process is split in 5. The first three are what you see, the way in which to combine $x_t$ and $h_{t-1}$ to produce $h_{t}$. However, that description is not complete because the cells state can be injected to any tree index. The last two numbers of the sequences indicate when to inject, in this example there is a 1 and a 0, which means that the cell state is injected to the tree index 0 and that the new cell state is the value in tree index 1. And the content of the cell inject part is how you inject the cell state. Let’s recap. Tree index 0 is $a_0 = \text{tanh}(W_1 \cdot x_t + W_2 \cdot h_{t-1})$, which is located at the right part of the graph above. Tree index 1 is $a_1 = \text{ReLU}(W_3 \cdot x_t \odot W_4 \cdot h_{t-1})$ located at the left. This is the simple part. Now things get complicated, the number at the end tells which tree index to inject the cell state, in this case the 0. So we have to update $a_0$ by $a_0 \leftarrow \text{ReLU}(a_0 + c_{t-1})$. Note that there are no learnable parameters in this step. Having done that we can now compute tree index 2: $a_2 = \text{sigmoid}( a_0 \odot a_{1})$. And this is the new hidden state $h_t \leftarrow a_2$. There is just one thing left, what is the new cell state? The value at tree index 1, which is the number we haven’t used yet. So the new cell state is the value at tree index 1 previous to activation so $c_t \leftarrow W_3 \cdot x_t \odot W_4 \cdot h_{t-1}$. It is a mess at the beginning but once you understand it, is awesome. You can represent any combination by a sequence and so you can learn to generate the optimal sequence. The irony here is that we are using recurrent networks to design recurrent networks. And although the authors didn’t try it, it could be interesting to iterate that process. Use an RNN to design a better RNN, then use that new RNN to design another one and so on. My guess is that it would converge, but who knows, maybe you get an infinitely better network. Okay, we have learned a way to represent RNN, so, how does the LSTM look like with this new representation? It looks like this If you are interested you can go through the graph step by step to check that the formulas are the same. Finally, the moment we were all expecting, the new and better Recurrent cell found by the authors of the NAS, the so-called NASCell (you can find it in tensorflow with that name).  In order to find it the authors required a lot of computation. This RNN is supposed to be better at language tasks than the normal LSTM. However, since this article came before big transformers were made, this recurrent cell got forgotten after that, and didn’t get much attention. Nevertheless, it is interesting to know that there are many possible RNN, not only the LSTM and the GRU. So the next time you want to try a simple RNN instead of a big transformer, you can think of using the NASCell. If you liked this, then you are going the enjoy the last part of this series. In the next and last chapter I will be explaining another modification of the controller to include residual connections. Making the controller capable of designing architectures such as ResNet or EfficienNet. Stay tuned! "
    }, {
    "id": 25,
    "url": "https://granadata.art/NAS/",
    "title": "Neural Architecture Search",
    "body": "2022/07/12 - Today we are going to dive into an idea that some may fear, and others may praise: AI training itself. Well, in reality the idea is a bit different from an AI training itself, neural architecture search consists of using a network to design other networks in a similar way a human would do it, but automatically. The process can be described as follows.  The blue box is the network we want, it can be a Convolutional Neural Network to classify images, or a recurrent neural network to do sentiment analysis. On the other side, the red box is the network that is going to design the solution to our problem for us. However, the controller is not going to output the full code solving your problem, it is not so smart. Instead, it is going to generate the hyperparameters of your network, they can be the filter size, the number of channels of your convolutional layers, or the number of layers of your LSTM, I’ll talk in more details later which hyperparameters can be predicted. But first, how are we going to train the controller? Because training the supervised model is easy, you throw some data to it and apply gradient descent. However, the controller is not a supervised model. There is no data about the hyperparameters and there is no loss function between the values it gives and the optimal values because, of course, we don’t know the optimal values. Nevertheless, we do have a reward function: the accuracy of the child network. And so we can apply the reinforcement learning paradigm. There is still one problem, the accuracy is a reward function, but it is not differentiable, and we don’t know how it relates to the hyperparameters so it seems that we cannot compute the gradient, and therefore, we can’t apply gradient descent. The currently used solution for that problem was invented by Williams in 1992, they derived the following formula for the gradient: $$ \nabla_{\theta_c} J(\theta_c) = \sum_{t=1}^T \mathbb{E}_{P(a_{1:T};\theta_c)}[\nabla_{\theta_c}(\log(P(a_t|a_{(t-1):1};\theta_c)))\cdot R]$$That formula deserves its own post, but for now just bear in mind this is the gradient used to train the controller. The process is fairly simple, withdraw an architecture from the controller, train the architecture in a supervised manner and get a validation accuracy. Use that accuracy as reward and train the controller using the above gradient. Repeat suficiently many times and voilà, your controller has learnt to design architectures. The process is illustrated below. Several controllers are trained in parallel due to the high number of attempts the controller needs in order to achieve good performance. Remember, the controller is learning by try and error.  Now, the details. What is the controller, exactly? In the original paper it was an LSTM, however, any architecture valid for correlated data can be used, like a transformer. But, the NAS article was published the same year as the transformer paper, so they could only try the LSTM because there was no transformer at the time. More precisely, this is the scheme they present in their paper: As you can see the controller is predicting very simple parameters, the parameters of a CNN that we normally ignore and use by default. But don’t be fooled by its simplicity, it can get quite complex. Imagine we want to design a recurrent neural network, similar to the LSTM or the GRU. There are hidden states and hidden memory cells but, what are the connections? For the LSTM the connections are the ones shown below.  So think about all those arrows, what if I told you that with neural architecture search we can learn which arrows are the optimal? If you want to know how, wait for the next part in this series. "
    }, {
    "id": 26,
    "url": "https://granadata.art/the-buddhist-pace/",
    "title": "The buddhist pace",
    "body": "2022/06/26 - This is a story I read years ago from Reddit that has guided me through most parts of my life and whenever I want to engage in more that I can handle I remember it and calm myself down. The original story is lost, so I can only give to you my memory of it. The story is about several hikers in what could possibly be their longest journey. One day they are challenged by a misterious man to complete a walk. They are told that the first one to arrive at the finish line will receive a generous prize. However, they won’t know where exactly is the finish line, their only tool will be a compass specially designed to point to the end and nothing else. No length is provided, no unevenness, and of course, no map.  The hikers, confident on their experience and physical shape, started as fast as they could, some of them even running at the beginning. All except one, they called him the buddhist. He set himself a different pace, instead of going as fast as he could, he decided to go as fast as he could maintain indefinitely, which is a very different pace. He asked to himself: how long could I stay going at this velocity? If the answer was not forever, then it was too fast. But he also posed to himself another question: can I go faster? That way he set his pace, the buddhist pace, and mantained it. At the beginning the other hikers overtook the buddhist since they were almost running, like if they were in a marathon. But it was not a marathon, it was a different kind of race. With that in mind the buddhist forgot about the others and simply continued his way at the velocity he decided. The days passed and the buddhist didn’t meet the other hikers, they have taken a big advantage, but nonetheless he remained still, constant, and with the same pace always, with no rest but without accelerating himself, always the same speed. The race continued and the days passed, a week after it has started the buddhist finally met the other hikers. To his surprise, they were exhausted and walking really slow. After a week going so fast they had consumed all their batteries and were left with no energy. The buddhist looked at them, fresher than a lettuce, and continued. Now it was him the one overtaking the others. They were also astonished. The buddhist started so slow and they started so fast, how is it that he is here overtaking us and we can no longer compete with him? ‘In the long run, being constant is better than sprinting. ’ The buddhist answered. The others couldn’t help themselves but assume that the victory was for the buddhist.  What is remarkable about this story is that it is a good metaphor of life. You never know when it is going to end and you have to keep improving without losing its way. Sometimes you feel the urge to accelerate. For instance, the summer comes and you want to be in good shape so you start exercising as much as you can and begin a very restrictive diet. At first you will see benefits very fast (like the other hikers) but then you will be exhausted and find very difficult to continue the rythm. On the other hand, if you keep an exercise routine which is not so demanding, but you maintain it always, then, in some years you’ll be in perfect shape. More importantly, you will be able to maintain the shape without effort, as the buddhist maintained its pace. For me this story is a guide on my career, my philosophy. Researching can be really rewarding when it is successful, but in data science you never know when it is going to be a success. Even expert researchers have to experiment a lot to find the architecture that works. Although you may have some intuition, some compass, you ignore where the finish line is. Whenever I want to spend more hours fine-tuning a model, or coding a different architecture, or reading extra articles, I remember the buddhist and stop. More days will come. It is better to be constant than to engage in many things and then getting exhausted. As the buddhist would say: ‘set a pace, and maintain that pace’. "
    }, {
    "id": 27,
    "url": "https://granadata.art/read_time/",
    "title": "How is the reading time computed?",
    "body": "2000/05/21 - The way the reading time is computed is quite easy. It follows this formula $$ \frac{\text{Number of words in the post}}{\text{Average reading speed in words / min}} $$The average reading speed for college students is assumed to be 300 words per minute, according to this page, for normal text. If there are formulas and new concepts I use 100 words per minute. When the post is very short, the time is simply $&lt;1$ less than one minute, like this one. Also, the words are counted by copying the whole raw text with html tags into this page, and the result is rounded up so it may be bigger than expected. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});